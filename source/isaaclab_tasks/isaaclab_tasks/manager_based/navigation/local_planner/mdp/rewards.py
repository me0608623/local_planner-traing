# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""çå‹µå‡½æ•¸å®šç¾© - å°èˆªä»»å‹™çå‹µ"""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import RayCaster

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def progress_to_goal_reward(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """çå‹µï¼šæ¥è¿‘ç›®æ¨™çš„é€²åº¦

    è¨ˆç®—ç•¶å‰æ­¥èˆ‡ä¸Šä¸€æ­¥çš„ç›®æ¨™è·é›¢å·®ç•°ï¼Œçå‹µæ¥è¿‘ç›®æ¨™çš„è¡Œç‚º
    æ­£çå‹µï¼šæ©Ÿå™¨äººæ¥è¿‘ç›®æ¨™ï¼ˆè·é›¢æ¸›å°‘ï¼‰
    è² çå‹µï¼šæ©Ÿå™¨äººé é›¢ç›®æ¨™ï¼ˆè·é›¢å¢åŠ ï¼‰
    
    Returns:
        é€²åº¦çå‹µ tensorï¼Œshape (num_envs,)
        æ­£å€¼è¡¨ç¤ºæ¥è¿‘ç›®æ¨™ï¼Œè² å€¼è¡¨ç¤ºé é›¢ç›®æ¨™
    """
    # ç²å–ç›®æ¨™æŒ‡ä»¤
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]

    # ç²å–æ©Ÿå™¨äººä½ç½®
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w

    # è¨ˆç®—ç•¶å‰è·é›¢ (åªè€ƒæ…® x, yï¼Œå¿½ç•¥ z)
    current_distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1)  # (num_envs,)

    # åˆå§‹åŒ–æˆ–ç²å–ä¸Šä¸€æ­¥è·é›¢ï¼ˆå„²å­˜ç‚ºç’°å¢ƒç‹€æ…‹è®Šæ•¸ï¼‰
    # é€™å€‹è®Šæ•¸æœƒåœ¨æ‰€æœ‰ä¸¦è¡Œç’°å¢ƒé–“å…±äº«ï¼Œæ¯å€‹ç’°å¢ƒæœ‰è‡ªå·±çš„è·é›¢å€¼
    if not hasattr(env, "_prev_goal_distance"):
        # é¦–æ¬¡èª¿ç”¨ï¼šåˆå§‹åŒ–ç‚ºç•¶å‰è·é›¢ï¼Œé€²åº¦ç‚º 0
        env._prev_goal_distance = current_distance.clone()
        progress = torch.zeros_like(current_distance)
    else:
        # è¨ˆç®—é€²åº¦ = ä¸Šä¸€æ­¥è·é›¢ - ç•¶å‰è·é›¢
        # å¦‚æœæ¥è¿‘ç›®æ¨™ï¼ˆè·é›¢æ¸›å°‘ï¼‰â†’ æ­£å€¼
        # å¦‚æœé é›¢ç›®æ¨™ï¼ˆè·é›¢å¢åŠ ï¼‰â†’ è² å€¼
        progress = env._prev_goal_distance - current_distance
        
        # è™•ç†ç’°å¢ƒé‡ç½®ï¼šå¦‚æœæŸå€‹ç’°å¢ƒå‰›é‡ç½® (episode_length == 1)ï¼Œå…¶é€²åº¦æ‡‰ç‚º 0
        # å› ç‚ºç¬¬ä¸€æ­¥æ²’æœ‰ã€Œä¸Šä¸€æ­¥ã€å¯ä»¥æ¯”è¼ƒ
        reset_mask = (env.episode_length_buf == 1)
        progress = torch.where(reset_mask, torch.zeros_like(progress), progress)
        
        # æ›´æ–°ä¸Šä¸€æ­¥è·é›¢ï¼ˆæ‰€æœ‰ç’°å¢ƒéƒ½æ›´æ–°ï¼‰
        env._prev_goal_distance = current_distance.clone()

    return progress


def reached_goal_reward(env: ManagerBasedRLEnv, command_name: str, threshold: float = 0.5) -> torch.Tensor:
    """çå‹µï¼šåˆ°é”ç›®æ¨™

    ç•¶æ©Ÿå™¨äººåˆ°é”ç›®æ¨™ç¯„åœå…§æ™‚çµ¦äºˆå¤§é‡çå‹µ
    """
    # ç²å–ç›®æ¨™æŒ‡ä»¤
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]

    # ç²å–æ©Ÿå™¨äººä½ç½®
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w

    # è¨ˆç®—è·é›¢
    distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1)

    # åˆ°é”ç›®æ¨™çå‹µ
    reward = (distance < threshold).float()

    return reward


def obstacle_proximity_penalty(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    safe_distance: float = 1.0,
) -> torch.Tensor:
    """æ‡²ç½°ï¼šéæ–¼æ¥è¿‘éšœç¤™ç‰©

    ç•¶ LiDAR åµæ¸¬åˆ°éšœç¤™ç‰©åœ¨å®‰å…¨è·é›¢å…§æ™‚çµ¦äºˆæ‡²ç½°
    """
    # ç²å– LiDAR æ„Ÿæ¸¬å™¨
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data

    # ç²å–è·é›¢æ•¸æ“šï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ APIï¼‰
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        # Isaac Sim 5.0+ / Isaac Lab 2025+ï¼šéœ€æ‰‹å‹•è¨ˆç®—è·é›¢
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    elif hasattr(data, "hit_distances"):
        distances = data.hit_distances.squeeze(-1)  # Isaac Lab 2025.1
    elif hasattr(data, "distances"):
        distances = data.distances.squeeze(-1)  # Isaac Lab 2024.1
    elif hasattr(data, "ray_distances"):
        distances = data.ray_distances.squeeze(-1)  # Isaac Lab â‰¤ 2023.1
    else:
        raise AttributeError(f"RayCasterData has no recognized distance attribute")
    
    distances = distances.squeeze(-1) if distances.dim() > 2 else distances  # ç¢ºä¿ (num_envs, num_rays)

    # è¨ˆç®—æ¯å€‹ç’°å¢ƒä¸­æœ€è¿‘çš„éšœç¤™ç‰©è·é›¢
    min_distance, _ = torch.min(distances, dim=-1)

    # å¦‚æœæœ€è¿‘è·é›¢å°æ–¼å®‰å…¨è·é›¢ï¼Œçµ¦äºˆæ‡²ç½°
    penalty = torch.clamp(safe_distance - min_distance, min=0.0, max=safe_distance)

    return -penalty


def collision_penalty(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    collision_threshold: float = 0.3,
) -> torch.Tensor:
    """æ‡²ç½°ï¼šç¢°æ’

    ç•¶ LiDAR åµæ¸¬åˆ°éå¸¸è¿‘çš„éšœç¤™ç‰©æ™‚åˆ¤å®šç‚ºç¢°æ’
    """
    # ç²å– LiDAR æ„Ÿæ¸¬å™¨
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data

    # ç²å–è·é›¢æ•¸æ“šï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ APIï¼‰
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        # Isaac Sim 5.0+ / Isaac Lab 2025+ï¼šéœ€æ‰‹å‹•è¨ˆç®—è·é›¢
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    elif hasattr(data, "hit_distances"):
        distances = data.hit_distances.squeeze(-1)  # Isaac Lab 2025.1
    elif hasattr(data, "distances"):
        distances = data.distances.squeeze(-1)  # Isaac Lab 2024.1
    elif hasattr(data, "ray_distances"):
        distances = data.ray_distances.squeeze(-1)  # Isaac Lab â‰¤ 2023.1
    else:
        raise AttributeError(f"RayCasterData has no recognized distance attribute")
    
    distances = distances.squeeze(-1) if distances.dim() > 2 else distances  # ç¢ºä¿ (num_envs, num_rays)

    # æª¢æŸ¥æ˜¯å¦æœ‰å°„ç·šè·é›¢å°æ–¼é–¾å€¼
    min_distance, _ = torch.min(distances, dim=-1)
    collision = (min_distance < collision_threshold).float()

    return -collision


def action_rate_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """æ‡²ç½°ï¼šå‹•ä½œè®ŠåŒ–ç‡ï¼ˆé¼“å‹µå¹³æ»‘æ§åˆ¶ï¼‰

    è¨ˆç®—ç•¶å‰å‹•ä½œèˆ‡ä¸Šä¸€æ­¥å‹•ä½œçš„ L2 è·é›¢
    """
    # æ³¨æ„ï¼šé€™éœ€è¦ç’°å¢ƒå„²å­˜ä¸Šä¸€æ­¥çš„å‹•ä½œ
    # ç°¡åŒ–ç‰ˆæœ¬ï¼šæ‡²ç½°å¤§çš„è§’é€Ÿåº¦
    asset = env.scene[asset_cfg.name]
    ang_vel = asset.data.root_ang_vel_b

    # æ‡²ç½°å¤§çš„è§’é€Ÿåº¦ï¼ˆå¹³æ–¹ï¼‰
    penalty = torch.sum(ang_vel**2, dim=-1)

    return -penalty


def standstill_penalty(env: ManagerBasedRLEnv) -> torch.Tensor:
    """æ‡²ç½°ï¼šéœæ­¢ä¸å‹•

    æ‡²ç½°æ©Ÿå™¨äººé€Ÿåº¦éå°çš„æƒ…æ³
    """
    robot = env.scene["robot"]
    lin_vel = robot.data.root_lin_vel_b

    # è¨ˆç®—é€Ÿåº¦å¤§å°
    speed = torch.norm(lin_vel[:, :2], dim=-1)

    # å¦‚æœé€Ÿåº¦å¾ˆå°ï¼Œçµ¦äºˆæ‡²ç½°
    penalty = torch.exp(-10.0 * speed)  # é€Ÿåº¦è¶Šå°ï¼Œæ‡²ç½°è¶Šå¤§

    return -penalty


def near_goal_shaping(
    env: ManagerBasedRLEnv, 
    command_name: str,
    radius: float = 1.5
) -> torch.Tensor:
    """ã€æ–°å¢ã€‘è¿‘è·é›¢å¡‘å½¢çå‹µ
    
    è¨­è¨ˆç†å¿µï¼š
    è§£æ±º"æœ€å¾Œä¸€å…¬é‡Œ"å•é¡Œã€‚ç•¶Agentæ¥è¿‘ç›®æ¨™ï¼ˆ< radiusï¼‰ï¼Œçµ¦äºˆé¡å¤–çš„æ­£çå‹µï¼Œ
    çå‹µéš¨è‘—è·é›¢æ¸›å°‘è€Œå¢åŠ ï¼Œå¹«åŠ©Agentå­¸æœƒ"æœ€å¾Œé€¼è¿‘"ã€‚
    
    ç‚ºä»€éº¼éœ€è¦é€™å€‹ï¼Ÿ
    - progress_to_goal åªçå‹µ"æ¥è¿‘"çš„è¡Œç‚ºï¼ˆè·é›¢å·®ï¼‰
    - ä½†åœ¨æœ€å¾Œéšæ®µï¼ŒAgentå¯èƒ½åœ¨ç›®æ¨™é™„è¿‘å¾˜å¾Šï¼Œä¸çŸ¥é“å¦‚ä½•"è¡åˆº"
    - é€™å€‹å¡‘å½¢çå‹µæä¾›æ˜ç¢ºçš„æ¢¯åº¦ï¼šè¶Šè¿‘è¶Šå¥½
    
    æ•¸å­¸å…¬å¼ï¼š
    reward = max(0, (radius - distance) / radius)
    - è·é›¢ = 0ç±³ â†’ reward = 1.0ï¼ˆæœ€é«˜çå‹µï¼‰
    - è·é›¢ = radius â†’ reward = 0.0ï¼ˆé–‹å§‹é€²å…¥ç¯„åœï¼‰
    - è·é›¢ > radius â†’ reward = 0.0ï¼ˆè¶…å‡ºç¯„åœï¼‰
    
    Args:
        env: ç’°å¢ƒç‰©ä»¶
        command_name: ç›®æ¨™æŒ‡ä»¤åç¨±
        radius: å¡‘å½¢åŠå¾‘ï¼ˆç±³ï¼‰ï¼Œåœ¨æ­¤ç¯„åœå…§çµ¦äºˆçå‹µ
    
    Returns:
        shape (num_envs,) çš„å¡‘å½¢çå‹µ
        0.0 ~ 1.0ï¼Œè¶Šæ¥è¿‘ç›®æ¨™çå‹µè¶Šé«˜
    
    æ¸¬è©¦æ–¹æ³•ï¼š
    - è§€å¯Ÿ Episode_Reward/near_goal_shaping
    - å¦‚æœå¹³å‡ > 0.3ï¼Œèªªæ˜Agentç¶“å¸¸é€²å…¥1.5ç±³ç¯„åœ
    - å¦‚æœå¹³å‡ > 0.6ï¼Œèªªæ˜Agentèƒ½æ¥è¿‘åˆ°0.6ç±³ä»¥å…§
    """
    # ç²å–ç›®æ¨™æŒ‡ä»¤
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]
    
    # ç²å–æ©Ÿå™¨äººä½ç½®
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    
    # è¨ˆç®—2Dè·é›¢
    distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1)  # (num_envs,)
    
    # å¡‘å½¢çå‹µï¼šç·šæ€§éæ¸›ï¼Œå¾1.0ï¼ˆè·é›¢=0ï¼‰åˆ°0.0ï¼ˆè·é›¢=radiusï¼‰
    shaping = torch.clamp((radius - distance) / radius, min=0.0, max=1.0)
    
    return shaping


def cbf_safety_reward(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    safe_distance: float = 1.5,
    critical_distance: float = 0.5,
) -> torch.Tensor:
    """ã€PCCBF æ ¸å¿ƒã€‘æ§åˆ¶å±éšœå‡½æ•¸ï¼ˆCBFï¼‰å®‰å…¨çå‹µ
    
    è¨­è¨ˆç†å¿µï¼š
    åŸºæ–¼ PCCBF-MPC è«–æ–‡çš„ã€Œæ§åˆ¶å±éšœå‡½æ•¸ã€æ¦‚å¿µï¼Œé€™æ˜¯è«–æ–‡çš„æ ¸å¿ƒå‰µæ–°ã€‚
    CBF å®šç¾©äº†ä¸€å€‹ã€Œå®‰å…¨é›†åˆã€ï¼Œç¢ºä¿ç³»çµ±ç‹€æ…‹æ°¸é ä¸æœƒé›¢é–‹é€™å€‹é›†åˆã€‚
    åœ¨å°èˆªä¸­ï¼ŒCBF ä¿è­‰æ©Ÿå™¨äººèˆ‡éšœç¤™ç‰©çš„è·é›¢å§‹çµ‚å¤§æ–¼å®‰å…¨é–¾å€¼ã€‚
    
    è«–æ–‡ä¸­çš„ CBF æ•¸å­¸è¡¨é”ï¼š
    h(x) = ||x_robot - x_obstacle|| - d_safe â‰¥ 0
    
    æˆ‘å€‘çš„å¯¦ä½œç­–ç•¥ï¼š
    - å°‡ CBF è½‰æ›ç‚ºã€Œè»Ÿç´„æŸã€çå‹µï¼ˆå› ç‚º PPO ç„¡æ³•è™•ç†ç¡¬ç´„æŸï¼‰
    - ä½¿ç”¨ã€ŒæŒ‡æ•¸è¡°æ¸›ã€çå‹µï¼šè·é›¢è¶Šè¿‘ï¼Œæ‡²ç½°æŒ‡æ•¸å¢é•·
    - å€åˆ†ã€Œå®‰å…¨å€ã€å’Œã€Œå±éšªå€ã€ï¼Œçµ¦äºˆä¸åŒç¨‹åº¦çš„çå‹µ/æ‡²ç½°
    
    Args:
        env: ç’°å¢ƒç‰©ä»¶
        sensor_cfg: LiDAR æ„Ÿæ¸¬å™¨é…ç½®
        safe_distance: å®‰å…¨è·é›¢é–¾å€¼ï¼ˆç±³ï¼‰ï¼Œè¶…éæ­¤è·é›¢ â†’ å®Œå…¨å®‰å…¨
        critical_distance: è‡¨ç•Œè·é›¢ï¼ˆç±³ï¼‰ï¼Œä½æ–¼æ­¤è·é›¢ â†’ æ¥µåº¦å±éšª
    
    Returns:
        shape (num_envs,) çš„å®‰å…¨çå‹µ
        æ­£å€¼ â†’ å®‰å…¨è¡Œç‚ºï¼Œè² å€¼ â†’ å±éšªè¡Œç‚º
    
    æ•™å­¸ï¼šç‚ºä»€éº¼ç”¨ CBFï¼Ÿ
    - PCCBF è«–æ–‡è­‰æ˜ï¼šCBF èƒ½ä¿è­‰ã€Œå‰å‘ä¸è®Šæ€§ã€ï¼Œå³ä¸€æ—¦å®‰å…¨å°±æ°¸é å®‰å…¨
    - å‚³çµ±çå‹µï¼ˆå¦‚ obstacle_proximity_penaltyï¼‰åªã€Œæ‡²ç½°æ¥è¿‘ã€ï¼Œç„¡æ³•ä¿è­‰å®‰å…¨
    - CBF æä¾›æ•¸å­¸ä¸Šçš„å®‰å…¨ä¿è­‰ï¼Œé€™æ˜¯è«–æ–‡çš„é—œéµè²¢ç»
    - åœ¨å¼·åŒ–å­¸ç¿’ä¸­ï¼Œæˆ‘å€‘ç”¨ã€Œçå‹µ shapingã€è¿‘ä¼¼ CBF çš„æ•ˆæœ
    
    æ¸¬è©¦æ–¹æ³•ï¼š
    è¨“ç·´å¾Œï¼Œæª¢æŸ¥ Episode_Reward/cbf_safety çš„å€¼ï¼š
    - å¦‚æœå¹³å‡ > 0.5 â†’ Agent å­¸æœƒä¿æŒå®‰å…¨è·é›¢
    - å¦‚æœå¹³å‡ < -1.0 â†’ Agent ä»ç„¶ç¶“å¸¸é€²å…¥å±éšªå€
    """
    # ç²å– LiDAR æ„Ÿæ¸¬å™¨
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data

    # ç²å–è·é›¢æ•¸æ“šï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ APIï¼‰
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        # Isaac Sim 5.0+ / Isaac Lab 2025+ï¼šéœ€æ‰‹å‹•è¨ˆç®—è·é›¢
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    elif hasattr(data, "hit_distances"):
        distances = data.hit_distances.squeeze(-1)  # Isaac Lab 2025.1
    elif hasattr(data, "distances"):
        distances = data.distances.squeeze(-1)  # Isaac Lab 2024.1
    else:
        raise AttributeError(f"RayCasterData has no recognized distance attribute")
    
    distances = distances.squeeze(-1) if distances.dim() > 2 else distances  # ç¢ºä¿ (num_envs, num_rays)

    # è¨ˆç®—æœ€å°éšœç¤™ç‰©è·é›¢ï¼ˆæœ€å±éšªçš„æ–¹å‘ï¼‰
    min_distance, _ = torch.min(distances, dim=-1)  # (num_envs,)

    # ğŸ”¥ CBF æ ¸å¿ƒé‚è¼¯ï¼šå®šç¾©å®‰å…¨å‡½æ•¸ h(x)
    # h(x) = min_distance - safe_distance
    # h(x) > 0 â†’ å®‰å…¨å€ï¼ˆçµ¦æ­£çå‹µï¼‰
    # h(x) < 0 â†’ å±éšªå€ï¼ˆçµ¦è² çå‹µï¼‰
    h = min_distance - safe_distance

    # å®‰å…¨çå‹µè¨ˆç®—ï¼ˆåˆ†æ®µå‡½æ•¸ï¼‰
    # 1. æ¥µåº¦å±éšªå€ï¼ˆ< critical_distanceï¼‰ï¼šæŒ‡æ•¸ç´šæ‡²ç½°
    critical_mask = min_distance < critical_distance
    critical_penalty = torch.exp(-5.0 * (min_distance - critical_distance))  # è·é›¢è¶Šè¿‘ï¼Œæ‡²ç½°è¶Šå¤§
    
    # 2. è­¦å‘Šå€ï¼ˆcritical_distance ~ safe_distanceï¼‰ï¼šç·šæ€§æ‡²ç½°
    warning_mask = (min_distance >= critical_distance) & (min_distance < safe_distance)
    warning_penalty = (safe_distance - min_distance) / (safe_distance - critical_distance)
    
    # 3. å®‰å…¨å€ï¼ˆ> safe_distanceï¼‰ï¼šå°é¡æ­£çå‹µï¼ˆé¼“å‹µä¿æŒå®‰å…¨ï¼‰
    safe_mask = min_distance >= safe_distance
    safe_reward = torch.ones_like(min_distance) * 0.1  # å°é¡çå‹µ
    
    # çµ„åˆçå‹µ
    reward = torch.zeros_like(min_distance)
    reward = torch.where(critical_mask, -critical_penalty, reward)
    reward = torch.where(warning_mask, -warning_penalty, reward)
    reward = torch.where(safe_mask, safe_reward, reward)

    return reward


def predicted_cbf_safety_reward(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    prediction_horizon: int = 3,
    safe_distance: float = 1.5,
) -> torch.Tensor:
    """ã€PCCBF å®Œæ•´ç‰ˆã€‘åŸºæ–¼é æ¸¬çš„ CBF å®‰å…¨çå‹µ
    
    è¨­è¨ˆç†å¿µï¼š
    é€™æ˜¯ PCCBF è«–æ–‡çš„å®Œæ•´å¯¦ç¾ï¼šçµåˆã€Œé æ¸¬ã€å’Œã€ŒCBFã€ã€‚
    è«–æ–‡ä¸­çš„ã€Œå‰ç»æ™‚åŸŸåœ°åœ–ï¼ˆFTD Mapï¼‰ã€åœ¨é€™è£¡é«”ç¾ç‚ºï¼š
    è©•ä¼°æœªä¾† N æ­¥çš„å®‰å…¨æ€§ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ç•¶å‰æ™‚åˆ»ã€‚
    
    å·®ç•°å°æ¯”ï¼š
    - cbf_safety_rewardï¼šåªçœ‹ã€Œç•¶å‰ã€éšœç¤™ç‰©è·é›¢ï¼ˆå‚³çµ± CBFï¼‰
    - predicted_cbf_safety_rewardï¼šçœ‹ã€Œæœªä¾†ã€éšœç¤™ç‰©è·é›¢ï¼ˆPCCBFï¼‰
    
    ç‚ºä»€éº¼éœ€è¦é æ¸¬ç‰ˆï¼Ÿ
    - æ©Ÿå™¨äººæœ‰æ…£æ€§ï¼Œç„¡æ³•ç¬é–“åœæ­¢
    - å¦‚æœåªçœ‹ç•¶å‰ï¼Œé«˜é€Ÿæ¥è¿‘éšœç¤™ç‰©æ™‚æœƒä¾†ä¸åŠå‰è»Š
    - é æ¸¬ç‰ˆèƒ½æå‰ 3-5 æ­¥ã€Œçœ‹åˆ°ã€å±éšªï¼Œçµ¦ Agent åæ‡‰æ™‚é–“
    
    Args:
        env: ç’°å¢ƒç‰©ä»¶
        sensor_cfg: LiDAR æ„Ÿæ¸¬å™¨é…ç½®
        prediction_horizon: é æ¸¬æœªä¾†å¹¾æ­¥
        safe_distance: å®‰å…¨è·é›¢é–¾å€¼
    
    Returns:
        shape (num_envs,) çš„é æ¸¬å®‰å…¨çå‹µ
    
    æ•™å­¸ï¼šPCCBF vs CBF
    - CBFï¼ˆå‚³çµ±ï¼‰ï¼šh(x_t) â‰¥ 0ï¼ˆç•¶å‰æ™‚åˆ»å®‰å…¨ï¼‰
    - PCCBFï¼ˆè«–æ–‡ï¼‰ï¼šh(x_{t+k}) â‰¥ 0 for k=1..Nï¼ˆæœªä¾† N æ­¥éƒ½å®‰å…¨ï¼‰
    - é€™è®“æ§åˆ¶æ›´ã€Œä¿å®ˆã€ï¼Œä½†æ›´å®‰å…¨
    """
    # ç²å–ç•¶å‰ LiDAR è·é›¢
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data
    
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        current_distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    else:
        raise AttributeError("éœ€è¦ Isaac Sim 5.0+ çš„ ray_hits_w API")
    
    # ç²å–æ©Ÿå™¨äººé€Ÿåº¦ï¼ˆç”¨æ–¼é æ¸¬ï¼‰
    robot = env.scene["robot"]
    robot_vel = robot.data.root_lin_vel_b  # (num_envs, 3)
    
    # é æ¸¬æœªä¾†ä½ç½®
    dt = env.step_dt * prediction_horizon
    predicted_displacement = torch.norm(robot_vel[:, :2], dim=-1) * dt  # (num_envs,)
    
    # è¨ˆç®—é æ¸¬æœ€å°è·é›¢
    min_current_distance = torch.min(current_distances, dim=-1)[0]  # (num_envs,)
    predicted_min_distance = min_current_distance - predicted_displacement
    predicted_min_distance = torch.clamp(predicted_min_distance, min=0.0)
    
    # ä½¿ç”¨é æ¸¬è·é›¢è¨ˆç®— CBF çå‹µ
    h_predicted = predicted_min_distance - safe_distance
    
    # å¦‚æœé æ¸¬æœƒé€²å…¥å±éšªå€ï¼Œçµ¦å¤§æ‡²ç½°
    danger_mask = h_predicted < 0
    danger_penalty = torch.abs(h_predicted) * 2.0  # é æ¸¬è¶Šå±éšªï¼Œæ‡²ç½°è¶Šå¤§
    
    # å¦‚æœé æ¸¬ä»ç„¶å®‰å…¨ï¼Œçµ¦å°çå‹µ
    safe_reward = torch.ones_like(h_predicted) * 0.05
    
    reward = torch.where(danger_mask, -danger_penalty, safe_reward)
    
    return reward



