# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""çå‹µå‡½æ•¸å®šç¾© - å°èˆªä»»å‹™çå‹µ"""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import RayCaster
import isaaclab.utils.math as math_utils

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def progress_to_goal_reward(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """çå‹µï¼šæ¥è¿‘ç›®æ¨™çš„é€²åº¦

    è¨ˆç®—ç•¶å‰æ­¥èˆ‡ä¸Šä¸€æ­¥çš„ç›®æ¨™è·é›¢å·®ç•°ï¼Œçå‹µæ¥è¿‘ç›®æ¨™çš„è¡Œç‚º
    æ­£çå‹µï¼šæ©Ÿå™¨äººæ¥è¿‘ç›®æ¨™ï¼ˆè·é›¢æ¸›å°‘ï¼‰
    è² çå‹µï¼šæ©Ÿå™¨äººé é›¢ç›®æ¨™ï¼ˆè·é›¢å¢åŠ ï¼‰
    
    Returns:
        é€²åº¦çå‹µ tensorï¼Œshape (num_envs,)
        æ­£å€¼è¡¨ç¤ºæ¥è¿‘ç›®æ¨™ï¼Œè² å€¼è¡¨ç¤ºé é›¢ç›®æ¨™
    """
    # ç²å–ç›®æ¨™æŒ‡ä»¤
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]

    # ç²å–æ©Ÿå™¨äººä½ç½®
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w

    # è¨ˆç®—ç•¶å‰è·é›¢ (åªè€ƒæ…® x, yï¼Œå¿½ç•¥ z)
    current_distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1)  # (num_envs,)

    # åˆå§‹åŒ–æˆ–ç²å–ä¸Šä¸€æ­¥è·é›¢ï¼ˆå„²å­˜ç‚ºç’°å¢ƒç‹€æ…‹è®Šæ•¸ï¼‰
    # é€™å€‹è®Šæ•¸æœƒåœ¨æ‰€æœ‰ä¸¦è¡Œç’°å¢ƒé–“å…±äº«ï¼Œæ¯å€‹ç’°å¢ƒæœ‰è‡ªå·±çš„è·é›¢å€¼
    if not hasattr(env, "_prev_goal_distance"):
        # é¦–æ¬¡èª¿ç”¨ï¼šåˆå§‹åŒ–ç‚ºç•¶å‰è·é›¢ï¼Œé€²åº¦ç‚º 0
        env._prev_goal_distance = current_distance.clone()
        progress = torch.zeros_like(current_distance)
    else:
        # è¨ˆç®—é€²åº¦ = ä¸Šä¸€æ­¥è·é›¢ - ç•¶å‰è·é›¢
        # å¦‚æœæ¥è¿‘ç›®æ¨™ï¼ˆè·é›¢æ¸›å°‘ï¼‰â†’ æ­£å€¼
        # å¦‚æœé é›¢ç›®æ¨™ï¼ˆè·é›¢å¢åŠ ï¼‰â†’ è² å€¼
        progress = env._prev_goal_distance - current_distance
        
        # è™•ç†ç’°å¢ƒé‡ç½®ï¼šå¦‚æœæŸå€‹ç’°å¢ƒå‰›é‡ç½® (episode_length == 1)ï¼Œå…¶é€²åº¦æ‡‰ç‚º 0
        # å› ç‚ºç¬¬ä¸€æ­¥æ²’æœ‰ã€Œä¸Šä¸€æ­¥ã€å¯ä»¥æ¯”è¼ƒ
        reset_mask = (env.episode_length_buf == 1)
        progress = torch.where(reset_mask, torch.zeros_like(progress), progress)
        
        # æ›´æ–°ä¸Šä¸€æ­¥è·é›¢ï¼ˆæ‰€æœ‰ç’°å¢ƒéƒ½æ›´æ–°ï¼‰
        env._prev_goal_distance = current_distance.clone()

    return progress


def reached_goal_reward(env: ManagerBasedRLEnv, command_name: str, threshold: float = 0.5) -> torch.Tensor:
    """çå‹µï¼šåˆ°é”ç›®æ¨™

    ç•¶æ©Ÿå™¨äººåˆ°é”ç›®æ¨™ç¯„åœå…§æ™‚çµ¦äºˆå¤§é‡çå‹µ
    """
    # ç²å–ç›®æ¨™æŒ‡ä»¤
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]

    # ç²å–æ©Ÿå™¨äººä½ç½®
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w

    # è¨ˆç®—è·é›¢
    distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1)

    # åˆ°é”ç›®æ¨™çå‹µ
    reward = (distance < threshold).float()

    return reward


def obstacle_proximity_penalty(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    safe_distance: float = 1.0,
) -> torch.Tensor:
    """æ‡²ç½°ï¼šéæ–¼æ¥è¿‘éšœç¤™ç‰©

    ç•¶ LiDAR åµæ¸¬åˆ°éšœç¤™ç‰©åœ¨å®‰å…¨è·é›¢å…§æ™‚çµ¦äºˆæ‡²ç½°
    """
    # ç²å– LiDAR æ„Ÿæ¸¬å™¨
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data

    # ç²å–è·é›¢æ•¸æ“šï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ APIï¼‰
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        # Isaac Sim 5.0+ / Isaac Lab 2025+ï¼šéœ€æ‰‹å‹•è¨ˆç®—è·é›¢
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    elif hasattr(data, "hit_distances"):
        distances = data.hit_distances.squeeze(-1)  # Isaac Lab 2025.1
    elif hasattr(data, "distances"):
        distances = data.distances.squeeze(-1)  # Isaac Lab 2024.1
    elif hasattr(data, "ray_distances"):
        distances = data.ray_distances.squeeze(-1)  # Isaac Lab â‰¤ 2023.1
    else:
        raise AttributeError(f"RayCasterData has no recognized distance attribute")
    
    distances = distances.squeeze(-1) if distances.dim() > 2 else distances  # ç¢ºä¿ (num_envs, num_rays)

    # è¨ˆç®—æ¯å€‹ç’°å¢ƒä¸­æœ€è¿‘çš„éšœç¤™ç‰©è·é›¢
    min_distance, _ = torch.min(distances, dim=-1)

    # å¦‚æœæœ€è¿‘è·é›¢å°æ–¼å®‰å…¨è·é›¢ï¼Œçµ¦äºˆæ‡²ç½°
    penalty = torch.clamp(safe_distance - min_distance, min=0.0, max=safe_distance)

    return -penalty


def collision_penalty(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    collision_threshold: float = 0.3,
) -> torch.Tensor:
    """æ‡²ç½°ï¼šç¢°æ’

    ç•¶ LiDAR åµæ¸¬åˆ°éå¸¸è¿‘çš„éšœç¤™ç‰©æ™‚åˆ¤å®šç‚ºç¢°æ’
    """
    # ç²å– LiDAR æ„Ÿæ¸¬å™¨
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data

    # ç²å–è·é›¢æ•¸æ“šï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ APIï¼‰
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        # Isaac Sim 5.0+ / Isaac Lab 2025+ï¼šéœ€æ‰‹å‹•è¨ˆç®—è·é›¢
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    elif hasattr(data, "hit_distances"):
        distances = data.hit_distances.squeeze(-1)  # Isaac Lab 2025.1
    elif hasattr(data, "distances"):
        distances = data.distances.squeeze(-1)  # Isaac Lab 2024.1
    elif hasattr(data, "ray_distances"):
        distances = data.ray_distances.squeeze(-1)  # Isaac Lab â‰¤ 2023.1
    else:
        raise AttributeError(f"RayCasterData has no recognized distance attribute")
    
    distances = distances.squeeze(-1) if distances.dim() > 2 else distances  # ç¢ºä¿ (num_envs, num_rays)

    # æª¢æŸ¥æ˜¯å¦æœ‰å°„ç·šè·é›¢å°æ–¼é–¾å€¼
    min_distance, _ = torch.min(distances, dim=-1)
    collision = (min_distance < collision_threshold).float()

    return -collision


def action_rate_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """æ‡²ç½°ï¼šå‹•ä½œè®ŠåŒ–ç‡ï¼ˆé¼“å‹µå¹³æ»‘æ§åˆ¶ï¼‰

    è¨ˆç®—ç•¶å‰å‹•ä½œèˆ‡ä¸Šä¸€æ­¥å‹•ä½œçš„ L2 è·é›¢
    """
    # æ³¨æ„ï¼šé€™éœ€è¦ç’°å¢ƒå„²å­˜ä¸Šä¸€æ­¥çš„å‹•ä½œ
    # ç°¡åŒ–ç‰ˆæœ¬ï¼šæ‡²ç½°å¤§çš„è§’é€Ÿåº¦
    asset = env.scene[asset_cfg.name]
    ang_vel = asset.data.root_ang_vel_b

    # æ‡²ç½°å¤§çš„è§’é€Ÿåº¦ï¼ˆå¹³æ–¹ï¼‰
    penalty = torch.sum(ang_vel**2, dim=-1)

    return -penalty


def standstill_penalty(env: ManagerBasedRLEnv) -> torch.Tensor:
    """ã€ä¿®æ­£ç‰ˆã€‘æ‡²ç½°ï¼šéœæ­¢ä¸å‹•
    
    ä¿®æ­£ç†ç”±ï¼ˆ2025-10-30ï¼‰ï¼š
    èˆŠç‰ˆè¿”å› -penaltyï¼Œä½† TensorBoard é¡¯ç¤ºç‚ºæ­£å€¼ï¼Œ
    å¯èƒ½å°è‡´ç¬¦è™Ÿæ··æ·†æˆ–å¯¦éš›æœªç”Ÿæ•ˆã€‚
    
    æ–°ç‰ˆæ˜ç¢ºè¿”å›è² å€¼ï¼Œç¢ºä¿æ‡²ç½°æ­£ç¢ºæ–½åŠ ã€‚
    
    Returns:
        è² å€¼ï¼šé€Ÿåº¦è¶Šå°ï¼Œæ‡²ç½°è¶Šå¤§ï¼ˆçµ•å°å€¼è¶Šå¤§ï¼‰
    """
    robot = env.scene["robot"]
    lin_vel = robot.data.root_lin_vel_b

    # è¨ˆç®—é€Ÿåº¦å¤§å°
    speed = torch.norm(lin_vel[:, :2], dim=-1)

    # å¦‚æœé€Ÿåº¦å¾ˆå°ï¼Œçµ¦äºˆæ‡²ç½°ï¼ˆæŒ‡æ•¸è¡°æ¸›ï¼‰
    # speed = 0 â†’ penalty = -1.0
    # speed = 0.1 â†’ penalty = -0.368
    # speed > 0.5 â†’ penalty â‰ˆ 0
    penalty = -torch.exp(-10.0 * speed)

    return penalty


def anti_idle_penalty(
    env: ManagerBasedRLEnv,
    v_threshold: float = 0.05,
) -> torch.Tensor:
    """ã€æ–°å¢ã€‘åé–’ç½®æ‡²ç½°
    
    è¨­è¨ˆç†å¿µï¼ˆ2025-10-30ï¼‰ï¼š
    è§£æ±ºã€ŒåŸåœ°ä¸å‹•ã€ç­–ç•¥çš„è£œå……æ‡²ç½°ã€‚
    èˆ‡ standstill ä¸åŒï¼Œé€™æ˜¯ç·šæ€§æ‡²ç½°è€ŒéæŒ‡æ•¸ï¼Œ
    ç¢ºä¿å³ä½¿ standstill é£½å’Œï¼Œä¹Ÿæœ‰æŒçºŒæ¨åŠ›ã€‚
    
    Args:
        v_threshold: é€Ÿåº¦é–¾å€¼ï¼Œä½æ–¼æ­¤çµ¦äºˆå›ºå®šæ‡²ç½°
    
    Returns:
        è² å€¼ï¼šé€Ÿåº¦ä½æ–¼é–¾å€¼æ™‚è¿”å› -1.0ï¼Œå¦å‰‡ 0
    """
    robot = env.scene["robot"]
    lin_vel = robot.data.root_lin_vel_b
    speed = torch.norm(lin_vel[:, :2], dim=-1)
    
    # ç°¡å–®äºŒå…ƒæ‡²ç½°ï¼šé€Ÿåº¦å¤ªæ…¢ = -1.0
    is_idle = speed < v_threshold
    penalty = torch.where(is_idle, torch.ones_like(speed) * -1.0, torch.zeros_like(speed))
    
    return penalty


def spin_penalty(
    env: ManagerBasedRLEnv,
    w_threshold: float = 0.5,
    v_threshold: float = 0.1,
) -> torch.Tensor:
    """ã€æ–°å¢ã€‘åŸåœ°æ—‹è½‰æ‡²ç½°
    
    è¨­è¨ˆç†å¿µï¼ˆ2025-10-30ï¼‰ï¼š
    æŠ‘åˆ¶ã€Œé«˜è§’é€Ÿåº¦ + ä½ç·šé€Ÿåº¦ã€è¡Œç‚ºï¼ˆåŸåœ°ç˜‹ç‹‚è½‰åœˆï¼‰ã€‚
    
    Args:
        w_threshold: è§’é€Ÿåº¦é–¾å€¼ï¼ˆrad/sï¼‰
        v_threshold: ç·šé€Ÿåº¦é–¾å€¼ï¼ˆm/sï¼‰
    
    Returns:
        è² å€¼ï¼šåŸåœ°æ—‹è½‰æ™‚çµ¦äºˆæ‡²ç½°
    """
    robot = env.scene["robot"]
    lin_vel = robot.data.root_lin_vel_b  # (num_envs, 3)
    ang_vel = robot.data.root_ang_vel_b  # (num_envs, 3)
    
    # ç·šé€Ÿåº¦èˆ‡è§’é€Ÿåº¦
    v_lin = torch.norm(lin_vel[:, :2], dim=-1)
    w_z = torch.abs(ang_vel[:, 2])  # zè»¸è§’é€Ÿåº¦ï¼ˆyawï¼‰
    
    # æª¢æ¸¬åŸåœ°æ—‹è½‰ï¼šé«˜è§’é€Ÿåº¦ + ä½ç·šé€Ÿåº¦
    is_spinning = (w_z > w_threshold) & (v_lin < v_threshold)
    
    # æ‡²ç½°å¤§å°èˆ‡è§’é€Ÿåº¦æˆæ­£æ¯”
    penalty_magnitude = w_z / (w_threshold + 1e-6)
    penalty = torch.where(is_spinning, -penalty_magnitude, torch.zeros_like(w_z))
    
    return penalty


def time_penalty(env: ManagerBasedRLEnv) -> torch.Tensor:
    """ã€æ–°å¢ã€‘æ™‚é–“æˆæœ¬æ‡²ç½°
    
    è¨­è¨ˆç†å¿µï¼ˆ2025-10-30ï¼‰ï¼š
    æ¯æ­¥çµ¦äºˆå°é¡è² å€¼ï¼Œé¼“å‹µã€Œç›¡å¿«å®Œæˆä»»å‹™ã€ï¼Œ
    å°æŠ—ã€Œä¹¾ç­‰è¶…æ™‚ã€ç­–ç•¥ã€‚
    
    å…¸å‹ç´¯ç©ï¼ˆ30ç§’ episodeï¼Œ10Hz æ§åˆ¶ï¼‰ï¼š
    -0.02 Ã— 300 steps = -6.0
    
    é€™å€‹æˆæœ¬è¶³ä»¥å£“åˆ¶ã€ŒåŸåœ°ç­‰å¾…ã€çš„è™›å‡é«˜çå‹µã€‚
    
    Returns:
        å¸¸æ•¸è² å€¼ï¼šæ¯æ­¥ -1.0ï¼ˆç”±æ¬Šé‡ç¸®æ”¾ï¼‰
    """
    # è¿”å›å¸¸æ•¸è² å€¼ï¼ˆæ‰¹æ¬¡å¤§å°ï¼‰
    robot = env.scene["robot"]
    batch_size = robot.data.root_pos_w.shape[0]
    return -torch.ones(batch_size, device=robot.data.root_pos_w.device)


def near_goal_shaping(
    env: ManagerBasedRLEnv, 
    command_name: str,
    radius: float = 1.5
) -> torch.Tensor:
    """ã€æ–°å¢ã€‘è¿‘è·é›¢å¡‘å½¢çå‹µ
    
    è¨­è¨ˆç†å¿µï¼š
    è§£æ±º"æœ€å¾Œä¸€å…¬é‡Œ"å•é¡Œã€‚ç•¶Agentæ¥è¿‘ç›®æ¨™ï¼ˆ< radiusï¼‰ï¼Œçµ¦äºˆé¡å¤–çš„æ­£çå‹µï¼Œ
    çå‹µéš¨è‘—è·é›¢æ¸›å°‘è€Œå¢åŠ ï¼Œå¹«åŠ©Agentå­¸æœƒ"æœ€å¾Œé€¼è¿‘"ã€‚
    
    ç‚ºä»€éº¼éœ€è¦é€™å€‹ï¼Ÿ
    - progress_to_goal åªçå‹µ"æ¥è¿‘"çš„è¡Œç‚ºï¼ˆè·é›¢å·®ï¼‰
    - ä½†åœ¨æœ€å¾Œéšæ®µï¼ŒAgentå¯èƒ½åœ¨ç›®æ¨™é™„è¿‘å¾˜å¾Šï¼Œä¸çŸ¥é“å¦‚ä½•"è¡åˆº"
    - é€™å€‹å¡‘å½¢çå‹µæä¾›æ˜ç¢ºçš„æ¢¯åº¦ï¼šè¶Šè¿‘è¶Šå¥½
    
    æ•¸å­¸å…¬å¼ï¼š
    reward = max(0, (radius - distance) / radius)
    - è·é›¢ = 0ç±³ â†’ reward = 1.0ï¼ˆæœ€é«˜çå‹µï¼‰
    - è·é›¢ = radius â†’ reward = 0.0ï¼ˆé–‹å§‹é€²å…¥ç¯„åœï¼‰
    - è·é›¢ > radius â†’ reward = 0.0ï¼ˆè¶…å‡ºç¯„åœï¼‰
    
    Args:
        env: ç’°å¢ƒç‰©ä»¶
        command_name: ç›®æ¨™æŒ‡ä»¤åç¨±
        radius: å¡‘å½¢åŠå¾‘ï¼ˆç±³ï¼‰ï¼Œåœ¨æ­¤ç¯„åœå…§çµ¦äºˆçå‹µ
    
    Returns:
        shape (num_envs,) çš„å¡‘å½¢çå‹µ
        0.0 ~ 1.0ï¼Œè¶Šæ¥è¿‘ç›®æ¨™çå‹µè¶Šé«˜
    
    æ¸¬è©¦æ–¹æ³•ï¼š
    - è§€å¯Ÿ Episode_Reward/near_goal_shaping
    - å¦‚æœå¹³å‡ > 0.3ï¼Œèªªæ˜Agentç¶“å¸¸é€²å…¥1.5ç±³ç¯„åœ
    - å¦‚æœå¹³å‡ > 0.6ï¼Œèªªæ˜Agentèƒ½æ¥è¿‘åˆ°0.6ç±³ä»¥å…§
    """
    # ç²å–ç›®æ¨™æŒ‡ä»¤
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]
    
    # ç²å–æ©Ÿå™¨äººä½ç½®
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    
    # è¨ˆç®—2Dè·é›¢
    distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1)  # (num_envs,)
    
    # å¡‘å½¢çå‹µï¼šç·šæ€§éæ¸›ï¼Œå¾1.0ï¼ˆè·é›¢=0ï¼‰åˆ°0.0ï¼ˆè·é›¢=radiusï¼‰
    shaping = torch.clamp((radius - distance) / radius, min=0.0, max=1.0)
    
    return shaping


def heading_alignment_reward(
    env: ManagerBasedRLEnv,
    command_name: str,
    v_min: float = 0.1,
) -> torch.Tensor:
    """ã€ä¿®æ­£ç‰ˆã€‘æœå‘å°é½Šçå‹µï¼ˆæ¢ä»¶å¼ - é¿å…åŸåœ°è½‰å‘æ‹¿åˆ†ï¼‰

    è¨­è¨ˆç†å¿µï¼š
    åªæœ‰åœ¨ã€Œå‰é€² + æœå‘æ­£ç¢ºã€æ™‚æ‰çµ¦çå‹µï¼Œé¿å… reward hackingã€‚
    
    ä¿®æ­£ç†ç”±ï¼ˆ2025-10-30ï¼‰ï¼š
    èˆŠç‰ˆæ¯æ­¥éƒ½çµ¦åˆ†ï¼Œå°è‡´ Agent å­¸æœƒã€ŒåŸåœ°æœå‘ç›®æ¨™ã€ç­–ç•¥ï¼Œ
    Mean Reward è™›é«˜ï¼ˆ190+ï¼‰ï¼Œä½† Success Rate 0%ã€‚
    
    æ–°é‚è¼¯ï¼š
    1. è¨ˆç®— cos(heading_error)
    2. æª¢æŸ¥ v_lin > v_minï¼ˆæœ‰åœ¨å‰é€²ï¼‰
    3. æª¢æŸ¥ cos(heading) > 0ï¼ˆæœå‘ç›®æ¨™åŠç©ºé–“ï¼‰
    4. åŒæ™‚æ»¿è¶³æ‰çµ¦çå‹µï¼Œå¦å‰‡ç‚º 0
    
    Args:
        v_min: æœ€å°é€Ÿåº¦é–¾å€¼ï¼ˆm/sï¼‰ï¼Œä½æ–¼æ­¤è¦–ç‚ºéœæ­¢
    
    Returns:
        æ¢ä»¶å¼çå‹µï¼šåªæœ‰å‰é€²ä¸”æœå‘æ­£ç¢ºæ™‚ç‚ºæ­£å€¼
    """
    # ç›®æ¨™åœ¨ä¸–ç•Œåº§æ¨™
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]

    # æ©Ÿå™¨äººä½å§¿èˆ‡é€Ÿåº¦
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    robot_quat_w = robot.data.root_quat_w
    robot_vel_b = robot.data.root_lin_vel_b  # (num_envs, 3)

    # ç›®æ¨™ç›¸å°ä½ç½®ï¼ˆä¸–ç•Œï¼‰â†’ è½‰åˆ°æ©Ÿå™¨äººåº§æ¨™
    goal_rel_w = goal_pos_w - robot_pos_w
    goal_rel_b = math_utils.quat_apply_inverse(robot_quat_w, goal_rel_w)
    vec_xy = goal_rel_b[:, :2]

    # cos(heading_error) = dx / ||[dx, dy]||
    denom = torch.norm(vec_xy, dim=-1).clamp(min=1e-6)
    cos_heading = vec_xy[:, 0] / denom

    # æª¢æŸ¥é€Ÿåº¦æ¢ä»¶ï¼šv_lin > v_min
    speed = torch.norm(robot_vel_b[:, :2], dim=-1)  # (num_envs,)
    is_moving = speed > v_min
    
    # æª¢æŸ¥æœå‘æ¢ä»¶ï¼šcos(heading) > 0
    is_aligned = cos_heading > 0.0
    
    # åªæœ‰ã€Œç§»å‹• + å°é½Šã€æ‰çµ¦çå‹µ
    gating_mask = is_moving & is_aligned
    reward = torch.where(gating_mask, cos_heading, torch.zeros_like(cos_heading))

    return reward


def cbf_safety_reward(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    safe_distance: float = 1.5,
    critical_distance: float = 0.5,
) -> torch.Tensor:
    """ã€PCCBF æ ¸å¿ƒã€‘æ§åˆ¶å±éšœå‡½æ•¸ï¼ˆCBFï¼‰å®‰å…¨çå‹µ
    
    è¨­è¨ˆç†å¿µï¼š
    åŸºæ–¼ PCCBF-MPC è«–æ–‡çš„ã€Œæ§åˆ¶å±éšœå‡½æ•¸ã€æ¦‚å¿µï¼Œé€™æ˜¯è«–æ–‡çš„æ ¸å¿ƒå‰µæ–°ã€‚
    CBF å®šç¾©äº†ä¸€å€‹ã€Œå®‰å…¨é›†åˆã€ï¼Œç¢ºä¿ç³»çµ±ç‹€æ…‹æ°¸é ä¸æœƒé›¢é–‹é€™å€‹é›†åˆã€‚
    åœ¨å°èˆªä¸­ï¼ŒCBF ä¿è­‰æ©Ÿå™¨äººèˆ‡éšœç¤™ç‰©çš„è·é›¢å§‹çµ‚å¤§æ–¼å®‰å…¨é–¾å€¼ã€‚
    
    è«–æ–‡ä¸­çš„ CBF æ•¸å­¸è¡¨é”ï¼š
    h(x) = ||x_robot - x_obstacle|| - d_safe â‰¥ 0
    
    æˆ‘å€‘çš„å¯¦ä½œç­–ç•¥ï¼š
    - å°‡ CBF è½‰æ›ç‚ºã€Œè»Ÿç´„æŸã€çå‹µï¼ˆå› ç‚º PPO ç„¡æ³•è™•ç†ç¡¬ç´„æŸï¼‰
    - ä½¿ç”¨ã€ŒæŒ‡æ•¸è¡°æ¸›ã€çå‹µï¼šè·é›¢è¶Šè¿‘ï¼Œæ‡²ç½°æŒ‡æ•¸å¢é•·
    - å€åˆ†ã€Œå®‰å…¨å€ã€å’Œã€Œå±éšªå€ã€ï¼Œçµ¦äºˆä¸åŒç¨‹åº¦çš„çå‹µ/æ‡²ç½°
    
    Args:
        env: ç’°å¢ƒç‰©ä»¶
        sensor_cfg: LiDAR æ„Ÿæ¸¬å™¨é…ç½®
        safe_distance: å®‰å…¨è·é›¢é–¾å€¼ï¼ˆç±³ï¼‰ï¼Œè¶…éæ­¤è·é›¢ â†’ å®Œå…¨å®‰å…¨
        critical_distance: è‡¨ç•Œè·é›¢ï¼ˆç±³ï¼‰ï¼Œä½æ–¼æ­¤è·é›¢ â†’ æ¥µåº¦å±éšª
    
    Returns:
        shape (num_envs,) çš„å®‰å…¨çå‹µ
        æ­£å€¼ â†’ å®‰å…¨è¡Œç‚ºï¼Œè² å€¼ â†’ å±éšªè¡Œç‚º
    
    æ•™å­¸ï¼šç‚ºä»€éº¼ç”¨ CBFï¼Ÿ
    - PCCBF è«–æ–‡è­‰æ˜ï¼šCBF èƒ½ä¿è­‰ã€Œå‰å‘ä¸è®Šæ€§ã€ï¼Œå³ä¸€æ—¦å®‰å…¨å°±æ°¸é å®‰å…¨
    - å‚³çµ±çå‹µï¼ˆå¦‚ obstacle_proximity_penaltyï¼‰åªã€Œæ‡²ç½°æ¥è¿‘ã€ï¼Œç„¡æ³•ä¿è­‰å®‰å…¨
    - CBF æä¾›æ•¸å­¸ä¸Šçš„å®‰å…¨ä¿è­‰ï¼Œé€™æ˜¯è«–æ–‡çš„é—œéµè²¢ç»
    - åœ¨å¼·åŒ–å­¸ç¿’ä¸­ï¼Œæˆ‘å€‘ç”¨ã€Œçå‹µ shapingã€è¿‘ä¼¼ CBF çš„æ•ˆæœ
    
    æ¸¬è©¦æ–¹æ³•ï¼š
    è¨“ç·´å¾Œï¼Œæª¢æŸ¥ Episode_Reward/cbf_safety çš„å€¼ï¼š
    - å¦‚æœå¹³å‡ > 0.5 â†’ Agent å­¸æœƒä¿æŒå®‰å…¨è·é›¢
    - å¦‚æœå¹³å‡ < -1.0 â†’ Agent ä»ç„¶ç¶“å¸¸é€²å…¥å±éšªå€
    """
    # ç²å– LiDAR æ„Ÿæ¸¬å™¨
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data

    # ç²å–è·é›¢æ•¸æ“šï¼ˆå…¼å®¹å¤šç‰ˆæœ¬ APIï¼‰
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        # Isaac Sim 5.0+ / Isaac Lab 2025+ï¼šéœ€æ‰‹å‹•è¨ˆç®—è·é›¢
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    elif hasattr(data, "hit_distances"):
        distances = data.hit_distances.squeeze(-1)  # Isaac Lab 2025.1
    elif hasattr(data, "distances"):
        distances = data.distances.squeeze(-1)  # Isaac Lab 2024.1
    else:
        raise AttributeError(f"RayCasterData has no recognized distance attribute")
    
    distances = distances.squeeze(-1) if distances.dim() > 2 else distances  # ç¢ºä¿ (num_envs, num_rays)

    # è¨ˆç®—æœ€å°éšœç¤™ç‰©è·é›¢ï¼ˆæœ€å±éšªçš„æ–¹å‘ï¼‰
    min_distance, _ = torch.min(distances, dim=-1)  # (num_envs,)

    # ğŸ”¥ CBF æ ¸å¿ƒé‚è¼¯ï¼šå®šç¾©å®‰å…¨å‡½æ•¸ h(x)
    # h(x) = min_distance - safe_distance
    # h(x) > 0 â†’ å®‰å…¨å€ï¼ˆçµ¦æ­£çå‹µï¼‰
    # h(x) < 0 â†’ å±éšªå€ï¼ˆçµ¦è² çå‹µï¼‰
    h = min_distance - safe_distance

    # å®‰å…¨çå‹µè¨ˆç®—ï¼ˆåˆ†æ®µå‡½æ•¸ï¼‰
    # 1. æ¥µåº¦å±éšªå€ï¼ˆ< critical_distanceï¼‰ï¼šæŒ‡æ•¸ç´šæ‡²ç½°
    critical_mask = min_distance < critical_distance
    critical_penalty = torch.exp(-5.0 * (min_distance - critical_distance))  # è·é›¢è¶Šè¿‘ï¼Œæ‡²ç½°è¶Šå¤§
    
    # 2. è­¦å‘Šå€ï¼ˆcritical_distance ~ safe_distanceï¼‰ï¼šç·šæ€§æ‡²ç½°
    warning_mask = (min_distance >= critical_distance) & (min_distance < safe_distance)
    warning_penalty = (safe_distance - min_distance) / (safe_distance - critical_distance)
    
    # 3. å®‰å…¨å€ï¼ˆ> safe_distanceï¼‰ï¼šå°é¡æ­£çå‹µï¼ˆé¼“å‹µä¿æŒå®‰å…¨ï¼‰
    safe_mask = min_distance >= safe_distance
    safe_reward = torch.ones_like(min_distance) * 0.1  # å°é¡çå‹µ
    
    # çµ„åˆçå‹µ
    reward = torch.zeros_like(min_distance)
    reward = torch.where(critical_mask, -critical_penalty, reward)
    reward = torch.where(warning_mask, -warning_penalty, reward)
    reward = torch.where(safe_mask, safe_reward, reward)

    return reward


def predicted_cbf_safety_reward(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    prediction_horizon: int = 3,
    safe_distance: float = 1.5,
) -> torch.Tensor:
    """ã€PCCBF å®Œæ•´ç‰ˆã€‘åŸºæ–¼é æ¸¬çš„ CBF å®‰å…¨çå‹µ
    
    è¨­è¨ˆç†å¿µï¼š
    é€™æ˜¯ PCCBF è«–æ–‡çš„å®Œæ•´å¯¦ç¾ï¼šçµåˆã€Œé æ¸¬ã€å’Œã€ŒCBFã€ã€‚
    è«–æ–‡ä¸­çš„ã€Œå‰ç»æ™‚åŸŸåœ°åœ–ï¼ˆFTD Mapï¼‰ã€åœ¨é€™è£¡é«”ç¾ç‚ºï¼š
    è©•ä¼°æœªä¾† N æ­¥çš„å®‰å…¨æ€§ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ç•¶å‰æ™‚åˆ»ã€‚
    
    å·®ç•°å°æ¯”ï¼š
    - cbf_safety_rewardï¼šåªçœ‹ã€Œç•¶å‰ã€éšœç¤™ç‰©è·é›¢ï¼ˆå‚³çµ± CBFï¼‰
    - predicted_cbf_safety_rewardï¼šçœ‹ã€Œæœªä¾†ã€éšœç¤™ç‰©è·é›¢ï¼ˆPCCBFï¼‰
    
    ç‚ºä»€éº¼éœ€è¦é æ¸¬ç‰ˆï¼Ÿ
    - æ©Ÿå™¨äººæœ‰æ…£æ€§ï¼Œç„¡æ³•ç¬é–“åœæ­¢
    - å¦‚æœåªçœ‹ç•¶å‰ï¼Œé«˜é€Ÿæ¥è¿‘éšœç¤™ç‰©æ™‚æœƒä¾†ä¸åŠå‰è»Š
    - é æ¸¬ç‰ˆèƒ½æå‰ 3-5 æ­¥ã€Œçœ‹åˆ°ã€å±éšªï¼Œçµ¦ Agent åæ‡‰æ™‚é–“
    
    Args:
        env: ç’°å¢ƒç‰©ä»¶
        sensor_cfg: LiDAR æ„Ÿæ¸¬å™¨é…ç½®
        prediction_horizon: é æ¸¬æœªä¾†å¹¾æ­¥
        safe_distance: å®‰å…¨è·é›¢é–¾å€¼
    
    Returns:
        shape (num_envs,) çš„é æ¸¬å®‰å…¨çå‹µ
    
    æ•™å­¸ï¼šPCCBF vs CBF
    - CBFï¼ˆå‚³çµ±ï¼‰ï¼šh(x_t) â‰¥ 0ï¼ˆç•¶å‰æ™‚åˆ»å®‰å…¨ï¼‰
    - PCCBFï¼ˆè«–æ–‡ï¼‰ï¼šh(x_{t+k}) â‰¥ 0 for k=1..Nï¼ˆæœªä¾† N æ­¥éƒ½å®‰å…¨ï¼‰
    - é€™è®“æ§åˆ¶æ›´ã€Œä¿å®ˆã€ï¼Œä½†æ›´å®‰å…¨
    """
    # ç²å–ç•¶å‰ LiDAR è·é›¢
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data
    
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        hit_points = data.ray_hits_w  # (num_envs, num_rays, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        current_distances = torch.norm(hit_points - sensor_pos, dim=-1)  # (num_envs, num_rays)
    else:
        raise AttributeError("éœ€è¦ Isaac Sim 5.0+ çš„ ray_hits_w API")
    
    # ç²å–æ©Ÿå™¨äººé€Ÿåº¦ï¼ˆç”¨æ–¼é æ¸¬ï¼‰
    robot = env.scene["robot"]
    robot_vel = robot.data.root_lin_vel_b  # (num_envs, 3)
    
    # é æ¸¬æœªä¾†ä½ç½®
    dt = env.step_dt * prediction_horizon
    predicted_displacement = torch.norm(robot_vel[:, :2], dim=-1) * dt  # (num_envs,)
    
    # è¨ˆç®—é æ¸¬æœ€å°è·é›¢
    min_current_distance = torch.min(current_distances, dim=-1)[0]  # (num_envs,)
    predicted_min_distance = min_current_distance - predicted_displacement
    predicted_min_distance = torch.clamp(predicted_min_distance, min=0.0)
    
    # ä½¿ç”¨é æ¸¬è·é›¢è¨ˆç®— CBF çå‹µ
    h_predicted = predicted_min_distance - safe_distance
    
    # å¦‚æœé æ¸¬æœƒé€²å…¥å±éšªå€ï¼Œçµ¦å¤§æ‡²ç½°
    danger_mask = h_predicted < 0
    danger_penalty = torch.abs(h_predicted) * 2.0  # é æ¸¬è¶Šå±éšªï¼Œæ‡²ç½°è¶Šå¤§
    
    # å¦‚æœé æ¸¬ä»ç„¶å®‰å…¨ï¼Œçµ¦å°çå‹µ
    safe_reward = torch.ones_like(h_predicted) * 0.05
    
    reward = torch.where(danger_mask, -danger_penalty, safe_reward)
    
    return reward



