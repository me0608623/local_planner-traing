#!/usr/bin/env python3
"""
Nova Carter è¨“ç·´æ—¥èªŒåˆ†æå·¥å…·

è‡ªå‹•åˆ†æè¨“ç·´çµæœä¸¦æä¾›è¨ºæ–·å»ºè­°

âš ï¸ ä½¿ç”¨æ–¹æ³•:
    ./isaaclab.sh -p scripts/analyze_training_log.py
    ./isaaclab.sh -p scripts/analyze_training_log.py --file logs/rsl_rl/your_training.log
    ./isaaclab.sh -p scripts/analyze_training_log.py --stdin
    
    æ³¨æ„ï¼šå¿…é ˆä½¿ç”¨ isaaclab.sh è€Œä¸æ˜¯ç³»çµ± pythonï¼
"""

import re
import argparse
from pathlib import Path
from typing import Dict, List, Tuple


class TrainingAnalyzer:
    """è¨“ç·´æ—¥èªŒåˆ†æå™¨"""
    
    def __init__(self, log_text: str):
        self.log_text = log_text
        self.metrics = self.extract_metrics()
        
    def extract_metrics(self) -> Dict[str, float]:
        """å¾æ—¥èªŒä¸­æå–é—œéµæŒ‡æ¨™"""
        metrics = {}
        
        patterns = {
            'mean_reward': r'Mean reward:\s*([-\d.]+)',
            'episode_length': r'Mean episode length:\s*([\d.]+)',
            'reached_goal': r'Episode_Reward/reached_goal:\s*([\d.]+)',
            'progress_to_goal': r'Episode_Reward/progress_to_goal:\s*([-\d.]+)',
            'collision_penalty': r'Episode_Reward/collision_penalty:\s*([-\d.]+)',
            'obstacle_penalty': r'Episode_Reward/obstacle_proximity_penalty:\s*([-\d.]+)',
            'time_out_rate': r'Episode_Termination/time_out:\s*([\d.]+)',
            'goal_reached_rate': r'Episode_Termination/goal_reached:\s*([\d.]+)',
            'collision_rate': r'Episode_Termination/collision:\s*([\d.]+)',
            'position_error': r'position_error:\s*([\d.]+)',
            'value_loss': r'Mean value_function loss:\s*([\d.]+)',
            'entropy_loss': r'Mean entropy loss:\s*([\d.]+)',
        }
        
        for name, pattern in patterns.items():
            match = re.search(pattern, self.log_text)
            if match:
                metrics[name] = float(match.group(1))
            else:
                metrics[name] = 0.0
                
        return metrics
    
    def diagnose(self) -> Tuple[str, List[str], List[str]]:
        """è¨ºæ–·è¨“ç·´ç‹€æ…‹ä¸¦æä¾›å»ºè­°"""
        issues = []
        suggestions = []
        
        m = self.metrics
        
        # æ•´é«”ç‹€æ…‹è©•ä¼°
        if m['mean_reward'] > 0:
            status = "âœ… è‰¯å¥½"
        elif m['mean_reward'] > -500:
            status = "âš ï¸ éœ€è¦æ”¹é€²"
        else:
            status = "âŒ è¡¨ç¾ä¸ä½³"
        
        # å•é¡Œè¨ºæ–·
        if m['mean_reward'] < -1000:
            issues.append("âŒ å¹³å‡çå‹µéä½ ({:.2f})".format(m['mean_reward']))
            suggestions.append("èª¿æ•´çå‹µå‡½æ•¸æ¬Šé‡ï¼Œå¢åŠ æ­£å‘å¼•å°çå‹µ")
        
        if m['goal_reached_rate'] == 0.0:
            issues.append("âŒ å¾æœªæˆåŠŸåˆ°é”ç›®æ¨™")
            suggestions.append("ç°¡åŒ–ç’°å¢ƒï¼šæ¸›å°‘éšœç¤™ç‰©æ•¸é‡ï¼Œç¸®çŸ­ç›®æ¨™è·é›¢")
            suggestions.append("å¢åŠ  episode æ™‚é–“é™åˆ¶")
        elif m['goal_reached_rate'] < 0.1:
            issues.append("âš ï¸ æˆåŠŸç‡éä½ ({:.1%})".format(m['goal_reached_rate']))
            suggestions.append("èª¿æ•´çå‹µå‡½æ•¸ä»¥æä¾›æ›´å¥½çš„å¼•å°")
        
        if m['time_out_rate'] > 0.9:
            issues.append("âŒ å¹¾ä¹æ‰€æœ‰ episode éƒ½è¶…æ™‚ ({:.1%})".format(m['time_out_rate']))
            suggestions.append("å¢åŠ  episode_length_s é…ç½®")
            suggestions.append("ç°¡åŒ–ä»»å‹™é›£åº¦")
        
        if m['position_error'] > 4.0:
            issues.append("âŒ è·é›¢ç›®æ¨™å¤ªé  ({:.2f}m)".format(m['position_error']))
            suggestions.append("æª¢æŸ¥è§€æ¸¬ç©ºé–“æ˜¯å¦åŒ…å«ç›®æ¨™ä½ç½®ä¿¡æ¯")
            suggestions.append("å¢åŠ æœå‘ç›®æ¨™çš„çå‹µå¼•å°")
        
        if m['collision_rate'] > 0.5:
            issues.append("âš ï¸ ç¢°æ’ç‡éé«˜ ({:.1%})".format(m['collision_rate']))
            suggestions.append("å¢åŠ é¿éšœçå‹µæ¬Šé‡")
            suggestions.append("æª¢æŸ¥ LiDAR è§€æ¸¬æ˜¯å¦æ­£ç¢º")
        
        if m['value_loss'] > 1000:
            issues.append("âš ï¸ Value function loss éé«˜ ({:.2f})".format(m['value_loss']))
            suggestions.append("é™ä½å­¸ç¿’ç‡")
            suggestions.append("æª¢æŸ¥çå‹µå°ºåº¦æ˜¯å¦éå¤§")
        
        if m['progress_to_goal'] < -50:
            issues.append("âŒ æ²’æœ‰æ¥è¿‘ç›®æ¨™çš„è¡Œç‚º ({:.2f})".format(m['progress_to_goal']))
            suggestions.append("å¢åŠ  progress_to_goal_weight")
            suggestions.append("æ¸›å°‘å…¶ä»–æ‡²ç½°æ¬Šé‡")
        
        # æ­£é¢åé¥‹
        if not issues:
            issues.append("âœ… è¨“ç·´ç‹€æ…‹è‰¯å¥½ï¼")
            suggestions.append("ç¹¼çºŒè¨“ç·´ä»¥é€²ä¸€æ­¥æå‡æ€§èƒ½")
        
        return status, issues, suggestions
    
    def print_analysis(self):
        """æ‰“å°è©³ç´°åˆ†æå ±å‘Š"""
        print("=" * 80)
        print("ğŸ“Š Nova Carter è¨“ç·´çµæœåˆ†æ")
        print("=" * 80)
        
        # é—œéµæŒ‡æ¨™
        print("\nğŸ“ˆ é—œéµæŒ‡æ¨™:")
        print(f"   å¹³å‡çå‹µ: {self.metrics['mean_reward']:.2f}")
        print(f"   æˆåŠŸç‡: {self.metrics['goal_reached_rate']:.1%}")
        print(f"   è¶…æ™‚ç‡: {self.metrics['time_out_rate']:.1%}")
        print(f"   ç¢°æ’ç‡: {self.metrics['collision_rate']:.1%}")
        print(f"   å¹³å‡è·é›¢èª¤å·®: {self.metrics['position_error']:.2f}m")
        print(f"   å¹³å‡ episode é•·åº¦: {self.metrics['episode_length']:.2f}")
        
        # çå‹µåˆ†è§£
        print("\nğŸ’° çå‹µåˆ†è§£:")
        print(f"   åˆ°é”ç›®æ¨™çå‹µ: {self.metrics['reached_goal']:.2f}")
        print(f"   æ¥è¿‘ç›®æ¨™çå‹µ: {self.metrics['progress_to_goal']:.2f}")
        print(f"   ç¢°æ’æ‡²ç½°: {self.metrics['collision_penalty']:.2f}")
        print(f"   éšœç¤™ç‰©æ¥è¿‘æ‡²ç½°: {self.metrics['obstacle_penalty']:.2f}")
        
        # è¨“ç·´æŒ‡æ¨™
        print("\nğŸ”§ è¨“ç·´æŒ‡æ¨™:")
        print(f"   Value function loss: {self.metrics['value_loss']:.2f}")
        print(f"   Entropy loss: {self.metrics['entropy_loss']:.2f}")
        
        # è¨ºæ–·çµæœ
        status, issues, suggestions = self.diagnose()
        
        print(f"\nğŸ¯ æ•´é«”ç‹€æ…‹: {status}")
        
        print("\nğŸ” ç™¼ç¾çš„å•é¡Œ:")
        for issue in issues:
            print(f"   {issue}")
        
        print("\nğŸ’¡ æ”¹é€²å»ºè­°:")
        for i, suggestion in enumerate(suggestions, 1):
            print(f"   {i}. {suggestion}")
        
        # é…ç½®å»ºè­°
        print("\nâš™ï¸ å»ºè­°çš„é…ç½®èª¿æ•´:")
        if self.metrics['goal_reached_rate'] == 0.0:
            print("   # ç°¡åŒ–ç’°å¢ƒé…ç½®")
            print("   scene.obstacles.num_obstacles = 3  # æ¸›å°‘éšœç¤™ç‰©")
            print("   commands.goal_command.ranges.distance = (2.0, 4.0)  # ç¸®çŸ­è·é›¢")
            print("   episode_length_s = 30.0  # å¢åŠ æ™‚é–“")
        
        if self.metrics['mean_reward'] < -1000:
            print("   # èª¿æ•´çå‹µæ¬Šé‡")
            print("   progress_to_goal_weight = 5.0  # å¢åŠ å¼•å°")
            print("   collision_penalty_weight = -5.0  # æ¸›å°‘æ‡²ç½°")
        
        if self.metrics['value_loss'] > 1000:
            print("   # èª¿æ•´è¨“ç·´è¶…åƒæ•¸")
            print("   learning_rate = 3e-4  # é™ä½å­¸ç¿’ç‡")
            print("   num_steps_per_env = 24  # å¢åŠ æ­¥æ•¸")
        
        print("\n" + "=" * 80)


def analyze_log_file(log_file: Path):
    """åˆ†ææ—¥èªŒæ–‡ä»¶"""
    with open(log_file, 'r') as f:
        log_text = f.read()
    
    # æ‰¾åˆ°æœ€å¾Œä¸€æ¬¡è¿­ä»£çš„æ—¥èªŒ
    iterations = re.findall(
        r'Learning iteration \d+/\d+.*?(?=Learning iteration|\Z)',
        log_text,
        re.DOTALL
    )
    
    if not iterations:
        print("âŒ ç„¡æ³•åœ¨æ—¥èªŒæ–‡ä»¶ä¸­æ‰¾åˆ°è¨“ç·´è¿­ä»£ä¿¡æ¯")
        return
    
    # åˆ†ææœ€å¾Œä¸€æ¬¡è¿­ä»£
    last_iteration = iterations[-1]
    analyzer = TrainingAnalyzer(last_iteration)
    analyzer.print_analysis()


def analyze_text(text: str):
    """åˆ†ææ–‡æœ¬è¼¸å…¥"""
    analyzer = TrainingAnalyzer(text)
    analyzer.print_analysis()


def main():
    parser = argparse.ArgumentParser(
        description="Nova Carter è¨“ç·´æ—¥èªŒåˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ†ææ—¥èªŒæ–‡ä»¶
  python analyze_training_log.py --file logs/rsl_rl/local_planner/output.log
  
  # å¾å‰ªè²¼æ¿åˆ†æï¼ˆå°‡è¨“ç·´è¼¸å‡ºç²˜è²¼å¾ŒæŒ‰ Ctrl+Dï¼‰
  python analyze_training_log.py --stdin
        """
    )
    
    parser.add_argument(
        '--file', '-f',
        type=Path,
        help='è¨“ç·´æ—¥èªŒæ–‡ä»¶è·¯å¾‘'
    )
    
    parser.add_argument(
        '--stdin',
        action='store_true',
        help='å¾æ¨™æº–è¼¸å…¥è®€å–æ—¥èªŒ'
    )
    
    args = parser.parse_args()
    
    if args.file:
        if not args.file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            return
        analyze_log_file(args.file)
    elif args.stdin:
        print("ğŸ“‹ è«‹ç²˜è²¼è¨“ç·´æ—¥èªŒï¼ˆå®Œæˆå¾ŒæŒ‰ Ctrl+Dï¼‰:")
        import sys
        text = sys.stdin.read()
        analyze_text(text)
    else:
        # åˆ†æç¤ºä¾‹æ—¥èªŒ
        example_log = """
                      Learning iteration 999/1000                       

                       Computation: 20 steps/s (collection: 1.634s, learning 0.656s)
             Mean action noise std: 1.07
          Mean value_function loss: 3154.6131
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.9716
                       Mean reward: -2598.61
               Mean episode length: 181.36
   Episode_Reward/progress_to_goal: -125.6530
       Episode_Reward/reached_goal: 0.0000
Episode_Reward/obstacle_proximity_penalty: 0.0000
  Episode_Reward/collision_penalty: 0.0000
    Episode_Reward/ang_vel_penalty: 0.3417
 Episode_Reward/standstill_penalty: 0.0002
Metrics/goal_command/position_error: 5.1919
Metrics/goal_command/orientation_error: 0.0000
      Episode_Termination/time_out: 1.0000
  Episode_Termination/goal_reached: 0.0000
     Episode_Termination/collision: 0.0000
        """
        print("ä½¿ç”¨ç¤ºä¾‹æ—¥èªŒé€²è¡Œåˆ†æ...\n")
        analyze_text(example_log)
        print("\næç¤º: ä½¿ç”¨ --file æˆ– --stdin åƒæ•¸åˆ†æå¯¦éš›æ—¥èªŒ")


if __name__ == "__main__":
    main()
