# ğŸ”„ RSL-RL è¨“ç·´æµç¨‹è©³è§£

> **ç›®çš„**ï¼šåœ–è§£ `train.py` çš„å®Œæ•´åŸ·è¡Œæµç¨‹ï¼Œå¹«åŠ©ç†è§£æ¯å€‹æ­¥é©Ÿçš„ä½œç”¨

---

## ğŸ“Š è¨“ç·´æµç¨‹ç¸½è¦½

```
ç”¨æˆ¶åŸ·è¡ŒæŒ‡ä»¤
    â†“
./isaaclab.sh -p train.py --task PCCBF-Simple-v0 --num_envs 256
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¬¬ä¸€éšæ®µï¼šå•Ÿå‹•æº–å‚™ï¼ˆç¬¬ 1-98 è¡Œï¼‰                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. è§£æå‘½ä»¤è¡Œåƒæ•¸ï¼ˆtask, num_envs, max_iterations...ï¼‰        â”‚
â”‚  2. å•Ÿå‹• Isaac Sim æ¨¡æ“¬å™¨ï¼ˆAppLauncherï¼‰                      â”‚
â”‚  3. æª¢æŸ¥ RSL-RL ç‰ˆæœ¬                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¬¬äºŒéšæ®µï¼šmain() å‡½æ•¸åŸ·è¡Œï¼ˆç¬¬ 198-401 è¡Œï¼‰                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ­¥é©Ÿ 1ï¼šé…ç½®åƒæ•¸è¦†è“‹                                         â”‚
â”‚    - å‘½ä»¤è¡Œåƒæ•¸ > é…ç½®æ–‡ä»¶åƒæ•¸                                â”‚
â”‚    - ä¾‹å¦‚ï¼š--num_envs 256 æœƒè¦†è“‹é…ç½®æ–‡ä»¶ä¸­çš„å€¼                 â”‚
â”‚                                                             â”‚
â”‚  æ­¥é©Ÿ 2ï¼šè¨­ç½®éš¨æ©Ÿç¨®å­                                         â”‚
â”‚    - ç¢ºä¿çµæœå¯é‡ç¾                                          â”‚
â”‚    - åˆ†æ•£å¼è¨“ç·´ï¼šæ¯å€‹ GPU ç”¨ä¸åŒç¨®å­                          â”‚
â”‚                                                             â”‚
â”‚  æ­¥é©Ÿ 3ï¼šå‰µå»ºæ—¥èªŒç›®éŒ„                                         â”‚
â”‚    - logs/rsl_rl/local_planner_carter/2025-10-25_14-30-45/  â”‚
â”‚                                                             â”‚
â”‚  æ­¥é©Ÿ 4ï¼šå‰µå»º RL ç’°å¢ƒ                                         â”‚
â”‚    - gym.make(task_name, cfg=env_cfg)                      â”‚
â”‚    - è¼‰å…¥ PCCBF é…ç½®ï¼ˆè§€æ¸¬ã€çå‹µã€çµ‚æ­¢æ¢ä»¶ï¼‰                   â”‚
â”‚                                                             â”‚
â”‚  æ­¥é©Ÿ 5ï¼šåŒ…è£ç’°å¢ƒ                                            â”‚
â”‚    - è¦–é »éŒ„è£½åŒ…è£ï¼ˆå¦‚æœéœ€è¦ï¼‰                                 â”‚
â”‚    - RSL-RL æ¥å£åŒ…è£ï¼ˆRslRlVecEnvWrapperï¼‰                   â”‚
â”‚                                                             â”‚
â”‚  æ­¥é©Ÿ 6ï¼šå‰µå»ºè¨“ç·´å™¨                                          â”‚
â”‚    - OnPolicyRunnerï¼ˆRSL-RL æ ¸å¿ƒï¼‰                          â”‚
â”‚    - åˆå§‹åŒ– PPO ç®—æ³•ã€ç¥ç¶“ç¶²çµ¡                               â”‚
â”‚                                                             â”‚
â”‚  æ­¥é©Ÿ 7ï¼šåŸ·è¡Œè¨“ç·´å¾ªç’°                                        â”‚
â”‚    - runner.learn(max_iterations)                          â”‚
â”‚    - é€™æ˜¯è¨“ç·´çš„æ ¸å¿ƒï¼                                        â”‚
â”‚                                                             â”‚
â”‚  æ­¥é©Ÿ 8ï¼šä¿å­˜æ¨¡å‹å’Œæ—¥èªŒ                                      â”‚
â”‚    - model_*.ptï¼ˆæ¯ 100 iterationsï¼‰                        â”‚
â”‚    - TensorBoard æ—¥èªŒ                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¬¬ä¸‰éšæ®µï¼šæ¸…ç†è³‡æºï¼ˆç¬¬ 407-414 è¡Œï¼‰                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. env.close() - é—œé–‰ç’°å¢ƒ                                   â”‚
â”‚  2. simulation_app.close() - é—œé–‰ Isaac Sim                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” è©³ç´°æµç¨‹åœ–è§£

### éšæ®µ 1ï¼šå‘½ä»¤è¡Œåƒæ•¸è§£æ

```
ç”¨æˆ¶è¼¸å…¥ï¼š
./isaaclab.sh -p train.py --task PCCBF-Simple-v0 --num_envs 256 --max_iterations 1000 --headless

è§£æçµæœï¼š
â”œâ”€ args_cli.task = "Isaac-Navigation-LocalPlanner-PCCBF-Simple-v0"
â”œâ”€ args_cli.num_envs = 256
â”œâ”€ args_cli.max_iterations = 1000
â””â”€ args_cli.headless = True  ï¼ˆä¸é¡¯ç¤º GUIï¼‰
```

### éšæ®µ 2ï¼šé…ç½®è¼‰å…¥ï¼ˆHydra + Gymnasiumï¼‰

```
@hydra_task_config è£é£¾å™¨åŸ·è¡Œï¼š
    â†“
æŸ¥æ‰¾ task è¨»å†Šä¿¡æ¯ï¼š
    gym.envs.registry["Isaac-Navigation-LocalPlanner-PCCBF-Simple-v0"]
    â†“
è¼‰å…¥é…ç½®å…¥å£é»ï¼š
    â”œâ”€ env_cfg_entry_point: local_planner_env_cfg_pccbf_simple:LocalPlannerEnvCfg_PCCBF_SIMPLE
    â””â”€ agent_cfg_entry_point: rsl_rl_ppo_cfg:LocalPlannerPPORunnerCfg
    â†“
å¯¦ä¾‹åŒ–é…ç½®å°è±¡ï¼š
    â”œâ”€ env_cfg = LocalPlannerEnvCfg_PCCBF_SIMPLE()
    â”‚   â”œâ”€ scene.num_envs = 256
    â”‚   â”œâ”€ episode_length_s = 35.0
    â”‚   â”œâ”€ rewards = PCCBFSimpleRewardsCfg()
    â”‚   â””â”€ observations = ObservationsCfg()
    â”‚
    â””â”€ agent_cfg = LocalPlannerPPORunnerCfg()
        â”œâ”€ learning_rate = 0.0003
        â”œâ”€ max_iterations = 1000
        â””â”€ policy hidden_dims = [256, 256, 128]
```

### éšæ®µ 3ï¼šç’°å¢ƒå‰µå»º

```
gym.make(task_name, cfg=env_cfg)
    â†“
å‰µå»º ManagerBasedRLEnv å¯¦ä¾‹
    â†“
åˆå§‹åŒ–å ´æ™¯ï¼š
    â”œâ”€ è¼‰å…¥ Nova Carter æ©Ÿå™¨äººï¼ˆnova_carter.usdï¼‰
    â”œâ”€ å‰µå»º LiDAR æ„Ÿæ¸¬å™¨ï¼ˆ360 æ¢å°„ç·šï¼‰
    â”œâ”€ ç”Ÿæˆéšœç¤™ç‰©ï¼ˆéœæ…‹æ–¹å¡Šï¼‰
    â””â”€ å‰µå»ºåœ°å½¢ï¼ˆå¹³é¢ï¼‰
    â†“
åˆå§‹åŒ– MDP ç®¡ç†å™¨ï¼š
    â”œâ”€ ObservationManagerï¼ˆè¨ˆç®—è§€æ¸¬ï¼‰
    â”œâ”€ ActionManagerï¼ˆè™•ç†å‹•ä½œï¼‰
    â”œâ”€ RewardManagerï¼ˆè¨ˆç®—çå‹µï¼‰
    â”œâ”€ TerminationManagerï¼ˆæª¢æŸ¥çµ‚æ­¢æ¢ä»¶ï¼‰
    â””â”€ CommandManagerï¼ˆç”Ÿæˆç›®æ¨™æŒ‡ä»¤ï¼‰
    â†“
é‡ç½®ç’°å¢ƒï¼ˆenv.reset()ï¼‰
    â”œâ”€ éš¨æ©ŸåŒ–æ©Ÿå™¨äººä½ç½®
    â”œâ”€ ç”Ÿæˆéš¨æ©Ÿç›®æ¨™ï¼ˆ0.5-2.0 ç±³ï¼‰
    â””â”€ è¿”å›åˆå§‹è§€æ¸¬ (num_envs, 369)
```

### éšæ®µ 4ï¼šè¨“ç·´å¾ªç’°ï¼ˆæ ¸å¿ƒï¼ï¼‰

```
runner.learn(num_learning_iterations=1000)
    â†“
for iteration in range(1000):
    â”‚
    â”œâ”€ æ”¶é›†ç¶“é©—ï¼ˆRolloutï¼‰ï¼š
    â”‚   â”‚
    â”‚   for step in range(24):  # num_steps_per_env
    â”‚       â”‚
    â”‚       â”œâ”€ ç¥ç¶“ç¶²çµ¡å‰å‘å‚³æ’­ï¼š
    â”‚       â”‚   obs (256, 369) â†’ Actor â†’ action (256, 2)
    â”‚       â”‚                  â†’ Critic â†’ value (256, 1)
    â”‚       â”‚
    â”‚       â”œâ”€ ç’°å¢ƒåŸ·è¡Œå‹•ä½œï¼š
    â”‚       â”‚   env.step(action)
    â”‚       â”‚   â”œâ”€ ç‰©ç†æ¨¡æ“¬ï¼ˆ4 æ­¥ @ 0.01s = 0.04sï¼‰
    â”‚       â”‚   â”œâ”€ è¨ˆç®—è§€æ¸¬ï¼ˆLiDAR, é€Ÿåº¦, ç›®æ¨™ï¼‰
    â”‚       â”‚   â”œâ”€ è¨ˆç®—çå‹µï¼ˆprogress_to_goal, cbf_safety, ...ï¼‰
    â”‚       â”‚   â”œâ”€ æª¢æŸ¥çµ‚æ­¢æ¢ä»¶ï¼ˆtime_out, goal_reached, collisionï¼‰
    â”‚       â”‚   â””â”€ è¿”å› next_obs, reward, done
    â”‚       â”‚
    â”‚       â””â”€ å­˜å„²ç¶“é©—ï¼š
    â”‚           (obs, action, reward, value, done) â†’ Buffer
    â”‚
    â”œâ”€ è¨ˆç®—å„ªå‹¢å‡½æ•¸ï¼ˆAdvantageï¼‰ï¼š
    â”‚   ä½¿ç”¨ GAEï¼ˆGeneralized Advantage Estimationï¼‰
    â”‚   A = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...
    â”‚   å…¶ä¸­ Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
    â”‚
    â”œâ”€ PPO æ›´æ–°ï¼ˆå¤šå€‹ epochsï¼‰ï¼š
    â”‚   for epoch in range(5):  # num_learning_epochs
    â”‚       â”‚
    â”‚       for mini_batch in range(4):  # num_mini_batches
    â”‚           â”‚
    â”‚           â”œâ”€ è¨ˆç®—ç­–ç•¥æå¤±ï¼ˆPolicy Lossï¼‰ï¼š
    â”‚           â”‚   ratio = Ï€_new(a|s) / Ï€_old(a|s)
    â”‚           â”‚   L_CLIP = min(ratio Ã— A, clip(ratio, 1-Îµ, 1+Îµ) Ã— A)
    â”‚           â”‚
    â”‚           â”œâ”€ è¨ˆç®—åƒ¹å€¼æå¤±ï¼ˆValue Lossï¼‰ï¼š
    â”‚           â”‚   L_VF = (V_pred - V_target)Â²
    â”‚           â”‚
    â”‚           â”œâ”€ è¨ˆç®—ç†µæå¤±ï¼ˆEntropy Lossï¼‰ï¼š
    â”‚           â”‚   L_ENT = -H(Ï€) = -âˆ‘Ï€ log(Ï€)
    â”‚           â”‚
    â”‚           â”œâ”€ ç¸½æå¤±ï¼š
    â”‚           â”‚   L = L_CLIP - câ‚Ã—L_VF + câ‚‚Ã—L_ENT
    â”‚           â”‚
    â”‚           â”œâ”€ åå‘å‚³æ’­ï¼š
    â”‚           â”‚   optimizer.zero_grad()
    â”‚           â”‚   loss.backward()
    â”‚           â”‚   optimizer.step()
    â”‚           â”‚
    â”‚           â””â”€ æ›´æ–°ç¥ç¶“ç¶²çµ¡æ¬Šé‡
    â”‚
    â”œâ”€ è¨˜éŒ„è¨“ç·´æŒ‡æ¨™ï¼š
    â”‚   â”œâ”€ Mean rewardï¼ˆå¹³å‡çå‹µï¼‰
    â”‚   â”œâ”€ Episode lengthï¼ˆå›åˆé•·åº¦ï¼‰
    â”‚   â”œâ”€ å„é …çå‹µåˆ†è§£ï¼ˆprogress_to_goal, cbf_safety, ...ï¼‰
    â”‚   â””â”€ çµ‚æ­¢åŸå› çµ±è¨ˆï¼ˆgoal_reached, collision, timeoutï¼‰
    â”‚
    â””â”€ ä¿å­˜æ¨¡å‹æª¢æŸ¥é»ï¼š
        æ¯ 100 iterations ä¿å­˜ä¸€æ¬¡ model_{iteration}.pt
```

---

## ğŸ§® è¨“ç·´æ•¸å­¸åŸç†

### PPO ç®—æ³•æ ¸å¿ƒ

**ç›®æ¨™**ï¼šæ‰¾åˆ°æœ€å„ªç­–ç•¥ Ï€*ï¼Œæœ€å¤§åŒ–æœŸæœ›ç´¯ç©çå‹µï¼š

```
J(Ï€) = E[âˆ‘_{t=0}^{âˆ} Î³áµ— r_t]
```

**PPO æå¤±å‡½æ•¸**ï¼š

```
L_CLIP = E[min(r_t(Î¸) Ã— Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) Ã— Ã‚_t)]

å…¶ä¸­ï¼š
- r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  # ç­–ç•¥æ¯”ç‡
- Ã‚_t = GAE å„ªå‹¢å‡½æ•¸
- Îµ = clip_param = 0.2  # è£å‰ªåƒæ•¸
```

**ç‚ºä»€éº¼ç”¨ PPOï¼Ÿ**
- âœ… ç©©å®šæ€§é«˜ï¼šé€šéè£å‰ªé¿å…ç­–ç•¥æ›´æ–°éå¤§
- âœ… æ¨£æœ¬æ•ˆç‡ï¼šOn-Policy ç®—æ³•ï¼Œç›´æ¥å„ªåŒ–ç•¶å‰ç­–ç•¥
- âœ… æ˜“æ–¼èª¿åƒï¼šè¶…åƒæ•¸å°å¤šç¨®ä»»å‹™éƒ½æœ‰æ•ˆ

---

## ğŸ“¦ è¨“ç·´ç”¢ç‰©

### ç›®éŒ„çµæ§‹

```
logs/rsl_rl/local_planner_carter/2025-10-25_14-30-45/
â”œâ”€â”€ model_0.pt                  # åˆå§‹æ¨¡å‹ï¼ˆéš¨æ©Ÿæ¬Šé‡ï¼‰
â”œâ”€â”€ model_100.pt                # ç¬¬ 100 æ¬¡è¿­ä»£çš„æ¨¡å‹
â”œâ”€â”€ model_200.pt                # ç¬¬ 200 æ¬¡è¿­ä»£çš„æ¨¡å‹
â”œâ”€â”€ ...
â”œâ”€â”€ model_1000.pt               # æœ€çµ‚æ¨¡å‹
â”œâ”€â”€ events.out.tfevents.*       # TensorBoard æ—¥èªŒæ–‡ä»¶
â”œâ”€â”€ params/                     # è¨“ç·´é…ç½®å‚™ä»½
â”‚   â”œâ”€â”€ env.yaml                # ç’°å¢ƒé…ç½®ï¼ˆäººé¡å¯è®€ï¼‰
â”‚   â”œâ”€â”€ env.pkl                 # ç’°å¢ƒé…ç½®ï¼ˆPython å°è±¡ï¼‰
â”‚   â”œâ”€â”€ agent.yaml              # Agent é…ç½®ï¼ˆäººé¡å¯è®€ï¼‰
â”‚   â””â”€â”€ agent.pkl               # Agent é…ç½®ï¼ˆPython å°è±¡ï¼‰
â”œâ”€â”€ git/                        # Git ç‹€æ…‹è¨˜éŒ„
â”‚   â””â”€â”€ git_state.txt           # Commit hash, branch ç­‰ä¿¡æ¯
â””â”€â”€ videos/                     # è¨“ç·´è¦–é »ï¼ˆå¦‚æœå•Ÿç”¨ --videoï¼‰
    â””â”€â”€ train/
        â”œâ”€â”€ rl-video-episode-0.mp4
        â”œâ”€â”€ rl-video-episode-2000.mp4
        â””â”€â”€ ...
```

### æ¨¡å‹æ–‡ä»¶å…§å®¹

```python
model_1000.pt = {
    'model_state_dict': {
        'actor.0.weight': tensor(...),  # Actor ç¶²çµ¡æ¬Šé‡
        'actor.0.bias': tensor(...),
        'critic.0.weight': tensor(...), # Critic ç¶²çµ¡æ¬Šé‡
        ...
    },
    'optimizer_state_dict': {
        'state': {...},                 # Adam å„ªåŒ–å™¨ç‹€æ…‹
        'param_groups': [...]
    },
    'iter': 1000,                       # è¿­ä»£æ¬¡æ•¸
    'infos': {...}                      # å…¶ä»–å…ƒæ•¸æ“š
}
```

---

## ğŸ”¬ è¨“ç·´ä¸­çš„é—œéµè¨ˆç®—

### æ¯å€‹ Iteration çš„è¨ˆç®—æµç¨‹

```
Iteration N (ä¾‹å¦‚ï¼šIteration 500)
    â†“
1. Rollout éšæ®µï¼ˆæ”¶é›†ç¶“é©—ï¼‰
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   for env in range(256):                    # 256 å€‹ä¸¦è¡Œç’°å¢ƒ
       for step in range(24):                # æ¯å€‹ç’°å¢ƒ 24 æ­¥
           â”œâ”€ è§€æ¸¬ï¼šobs = env.get_obs()
           â”‚   â”œâ”€ LiDAR: (256, 360)
           â”‚   â”œâ”€ é€Ÿåº¦: (256, 6)
           â”‚   â””â”€ ç›®æ¨™: (256, 3)
           â”‚   â†’ åˆä½µï¼š(256, 369)
           â”‚
           â”œâ”€ æ¨ç†ï¼šaction, value = policy(obs)
           â”‚   â”œâ”€ Actor(obs) â†’ action (256, 2)
           â”‚   â””â”€ Critic(obs) â†’ value (256, 1)
           â”‚
           â”œâ”€ åŸ·è¡Œï¼šnext_obs, reward, done = env.step(action)
           â”‚   â”œâ”€ ç‰©ç†æ¨¡æ“¬ 4 æ­¥ï¼ˆ0.04 ç§’ï¼‰
           â”‚   â”œâ”€ è¨ˆç®—çå‹µï¼š
           â”‚   â”‚   progress_to_goal = prev_dist - curr_dist
           â”‚   â”‚   near_goal_shaping = (radius - dist) / radius
           â”‚   â”‚   cbf_safety = ...
           â”‚   â”‚   total_reward = âˆ‘(weight Ã— reward_term)
           â”‚   â””â”€ æª¢æŸ¥çµ‚æ­¢ï¼š
           â”‚       if dist < 0.8m â†’ done (goal_reached)
           â”‚       if time > 35s â†’ done (timeout)
           â”‚
           â””â”€ å­˜å„²ï¼šbuffer.add(obs, action, reward, value, done)
   
   ç¸½æ”¶é›†ï¼š256 envs Ã— 24 steps = 6144 å€‹æ¨£æœ¬

2. å­¸ç¿’éšæ®µï¼ˆæ›´æ–°ç¥ç¶“ç¶²çµ¡ï¼‰
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   è¨ˆç®— GAE å„ªå‹¢ï¼š
       for t in reversed(range(24)):
           Î´_t = reward[t] + Î³ Ã— value[t+1] - value[t]
           advantage[t] = Î´_t + (Î³Î») Ã— advantage[t+1]
   
   PPO æ›´æ–°ï¼ˆ5 å€‹ epochsï¼‰ï¼š
       for epoch in range(5):
           shuffle(buffer)  # æ‰“äº‚æ•¸æ“š
           for mini_batch in range(4):
               â”œâ”€ å–æ¨£ï¼š1536 å€‹æ¨£æœ¬ï¼ˆ6144 / 4ï¼‰
               â”œâ”€ å‰å‘å‚³æ’­ï¼š
               â”‚   action_new, value_new = policy(obs_batch)
               â”‚   ratio = Ï€_new / Ï€_old
               â”œâ”€ è¨ˆç®—æå¤±ï¼š
               â”‚   L_CLIP = min(ratioÃ—A, clip(ratio)Ã—A)
               â”‚   L_VF = (value_new - returns)Â²
               â”‚   L_ENT = -entropy(action_distribution)
               â”‚   L_total = L_CLIP - L_VF + 0.01Ã—L_ENT
               â”œâ”€ åå‘å‚³æ’­ï¼š
               â”‚   loss.backward()
               â”‚   clip_grad_norm_(parameters, max_norm=1.0)
               â”‚   optimizer.step()
               â””â”€ æ›´æ–°æ¬Šé‡

3. è¨˜éŒ„èˆ‡ä¿å­˜
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”œâ”€ TensorBoard è¨˜éŒ„ï¼š
   â”‚   writer.add_scalar("Mean_reward", mean_reward, iteration)
   â”‚   writer.add_scalar("Episode_Reward/progress_to_goal", ...)
   â”‚
   â””â”€ ä¿å­˜æ¨¡å‹ï¼ˆæ¯ 100 iterationsï¼‰ï¼š
       torch.save(model_state_dict, "model_500.pt")
```

---

## ğŸ“Š è¨“ç·´æŒ‡æ¨™è§£è®€

### çµ‚ç«¯è¼¸å‡ºè§£æ

```
Learning iteration 999/1000                       
                       Computation: 1960 steps/s (collection: 3.089s, learning 0.045s)
```
- **1960 steps/s**ï¼šæ¯ç§’è™•ç† 1960 å€‹ç’°å¢ƒæ­¥æ•¸ï¼ˆæ€§èƒ½æŒ‡æ¨™ï¼‰
- **collection: 3.089s**ï¼šæ”¶é›†ç¶“é©—èŠ±è²» 3.089 ç§’ï¼ˆRollout éšæ®µï¼‰
- **learning: 0.045s**ï¼šæ›´æ–°ç¥ç¶“ç¶²çµ¡èŠ±è²» 0.045 ç§’ï¼ˆPPO æ›´æ–°éšæ®µï¼‰

```
             Mean action noise std: 3.17
```
- **å‹•ä½œå™ªéŸ³æ¨™æº–å·®**ï¼š3.17
- ç”¨æ–¼æ¢ç´¢ï¼ˆå™ªéŸ³è¶Šå¤§ï¼Œæ¢ç´¢è¶Šå¤šï¼‰
- éš¨è‘—è¨“ç·´é€²è¡Œé€šå¸¸æœƒé€æ¼¸é™ä½

```
          Mean value_function loss: 0.4984
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 4.4793
```
- **Value function loss**ï¼šCritic çš„é æ¸¬èª¤å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
- **Surrogate loss**ï¼šPPO çš„ç­–ç•¥æå¤±ï¼ˆè² å€¼æ­£å¸¸ï¼Œçµ•å°å€¼è¶Šå°è¶Šå¥½ï¼‰
- **Entropy loss**ï¼šç­–ç•¥çš„ç†µï¼ˆè¶Šé«˜è¡¨ç¤ºè¶Šéš¨æ©Ÿï¼Œ4.4 è¡¨ç¤ºé‚„åœ¨æ¢ç´¢ï¼‰

```
                       Mean reward: 37.72
```
- **å¹³å‡çå‹µ**ï¼šæ‰€æœ‰ç’°å¢ƒåœ¨çµ±è¨ˆé€±æœŸå…§çš„å¹³å‡çå‹µ
- é€™æ˜¯**æœ€é‡è¦çš„æŒ‡æ¨™**ï¼Œæ‡‰è©²æŒçºŒä¸Šå‡

```
               Mean episode length: 624.64
```
- **å¹³å‡ episode é•·åº¦**ï¼š624 æ­¥ï¼ˆç´„ 25 ç§’ï¼‰
- ç†æƒ³æƒ…æ³ï¼šéš¨è‘—è¨“ç·´ï¼ŒæˆåŠŸçš„ episodes æœƒæ›´çŸ­

```
   Episode_Reward/progress_to_goal: 0.4958
```
- **æ¥è¿‘ç›®æ¨™çå‹µ**ï¼šå¹³å‡æ¯æ­¥æ¥è¿‘ 0.4958 ç±³ï¼Ÿä¸ï¼
- é€™æ˜¯**ç´¯ç©çå‹µ**ï¼ˆæ•´å€‹ episode çš„ç¸½å’Œï¼‰ï¼Œä¸æ˜¯å–®æ­¥
- 0.4958 Ã· 30ï¼ˆæ¬Šé‡ï¼‰= 0.0165 ç±³/æ­¥çš„é€²åº¦

```
       Episode_Reward/reached_goal: 0.0000
```
- **åˆ°é”ç›®æ¨™çå‹µ**ï¼šåœ¨é€™å€‹çµ±è¨ˆé€±æœŸå…§ï¼Œæ²’æœ‰æˆåŠŸæ¡ˆä¾‹
- å¦‚æœæˆåŠŸï¼Œé€™å€‹å€¼æœƒæ˜¯ 200.0ï¼ˆæ¬Šé‡ï¼‰

```
         Episode_Reward/cbf_safety: 0.8000
```
- **CBF å®‰å…¨çå‹µ**ï¼š0.8ï¼ˆæ­£å€¼ï¼‰
- è¡¨ç¤º Agent å¤§éƒ¨åˆ†æ™‚é–“ä¿æŒå®‰å…¨è·é›¢

```
      Episode_Termination/time_out: 0.9922
  Episode_Termination/goal_reached: 0.0078
     Episode_Termination/collision: 0.0000
```
- **çµ‚æ­¢åŸå› çµ±è¨ˆ**ï¼ˆæ¯”ä¾‹ï¼‰ï¼š
  - 99.22% è¶…æ™‚ï¼ˆæ²’åˆ°é”ç›®æ¨™ï¼‰
  - 0.78% æˆåŠŸï¼ˆåˆ°é”ç›®æ¨™ï¼‰
  - 0% ç¢°æ’

---

## ğŸ¯ å¦‚ä½•å„ªåŒ–è¨“ç·´

### èª¿åƒç­–ç•¥

| å•é¡Œ | èª¿æ•´åƒæ•¸ | ä½ç½® | æ•ˆæœ |
|------|---------|------|------|
| æˆåŠŸç‡å¤ªä½ | å¢åŠ  `progress_to_goal` æ¬Šé‡ | `local_planner_env_cfg_pccbf_simple.py` | æ›´å¼·çš„å¼•å° |
| ç¢°æ’ç‡å¤ªé«˜ | å¢åŠ  `cbf_safety` æ¬Šé‡ | åŒä¸Š | æ›´å®‰å…¨ |
| è¨“ç·´ä¸ç©©å®š | é™ä½ `learning_rate` | `rsl_rl_ppo_cfg.py` | æ›´ç©©å®š |
| æ¢ç´¢ä¸è¶³ | å¢åŠ  `entropy_coef` | åŒä¸Š | æ›´å¤šæ¢ç´¢ |
| å­¸ç¿’å¤ªæ…¢ | å¢åŠ  `num_steps_per_env` | åŒä¸Š | æ›´å¤šæ¨£æœ¬ |

---

## ğŸ”„ å®Œæ•´è¨“ç·´æµç¨‹æ™‚é–“ç·š

```
0 ç§’ï¼šå•Ÿå‹• Isaac Sim
    â†“
5-10 ç§’ï¼šè¼‰å…¥ç’°å¢ƒå’Œè³‡ç”¢
    â†“
10 ç§’ï¼šé–‹å§‹ç¬¬ä¸€å€‹ iteration
    â†“
3.1 ç§’/iteration Ã— 1000 = 51 åˆ†é˜
    â†“
æ¯ 100 iterations ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    â†“
51 åˆ†é˜å¾Œï¼šè¨“ç·´å®Œæˆ
    â†“
é—œé–‰ç’°å¢ƒå’Œæ¨¡æ“¬å™¨
```

---

## ğŸ’¡ å­¸ç¿’é‡é»

1. **train.py çš„è·è²¬**ï¼š
   - å•Ÿå‹•æ¨¡æ“¬å™¨
   - å‰µå»ºç’°å¢ƒ
   - åˆå§‹åŒ–è¨“ç·´å™¨
   - **ä¸è² è²¬**ï¼šå®šç¾©è§€æ¸¬ã€çå‹µã€å‹•ä½œï¼ˆé€™äº›åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼‰

2. **è¨“ç·´çš„æ ¸å¿ƒå¾ªç’°**ï¼š
   ```
   æ”¶é›†ç¶“é©— â†’ è¨ˆç®—å„ªå‹¢ â†’ PPO æ›´æ–° â†’ é‡è¤‡
   ```

3. **é…ç½®çš„é‡è¦æ€§**ï¼š
   - 90% çš„èª¿å„ªåœ¨é…ç½®æ–‡ä»¶ä¸­ï¼ˆçå‹µæ¬Šé‡ã€ç›®æ¨™è·é›¢ç­‰ï¼‰
   - train.py å¾ˆå°‘éœ€è¦ä¿®æ”¹

4. **æ—¥èªŒçš„åƒ¹å€¼**ï¼š
   - TensorBoard è®“æ‚¨çœ‹åˆ°è¨“ç·´æ›²ç·š
   - æ¨¡å‹æª¢æŸ¥é»è®“æ‚¨æ¢å¾©è¨“ç·´
   - YAML é…ç½®è®“æ‚¨é‡ç¾å¯¦é©—

---

é€™ä»½æ–‡æª”å¹«åŠ©æ‚¨ç†è§£ `train.py` çš„å®Œæ•´æµç¨‹ï¼å¦‚æœæœ‰ä»»ä½•ç–‘å•ï¼Œéš¨æ™‚æå‡ºã€‚ğŸš€

