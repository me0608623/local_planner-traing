# 🔄 RSL-RL 訓練流程詳解

> **目的**：圖解 `train.py` 的完整執行流程，幫助理解每個步驟的作用

---

## 📊 訓練流程總覽

```
用戶執行指令
    ↓
./isaaclab.sh -p train.py --task PCCBF-Simple-v0 --num_envs 256
    ↓
┌─────────────────────────────────────────────────────────────┐
│  第一階段：啟動準備（第 1-98 行）                              │
├─────────────────────────────────────────────────────────────┤
│  1. 解析命令行參數（task, num_envs, max_iterations...）        │
│  2. 啟動 Isaac Sim 模擬器（AppLauncher）                      │
│  3. 檢查 RSL-RL 版本                                         │
└─────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────┐
│  第二階段：main() 函數執行（第 198-401 行）                    │
├─────────────────────────────────────────────────────────────┤
│  步驟 1：配置參數覆蓋                                         │
│    - 命令行參數 > 配置文件參數                                │
│    - 例如：--num_envs 256 會覆蓋配置文件中的值                 │
│                                                             │
│  步驟 2：設置隨機種子                                         │
│    - 確保結果可重現                                          │
│    - 分散式訓練：每個 GPU 用不同種子                          │
│                                                             │
│  步驟 3：創建日誌目錄                                         │
│    - logs/rsl_rl/local_planner_carter/2025-10-25_14-30-45/  │
│                                                             │
│  步驟 4：創建 RL 環境                                         │
│    - gym.make(task_name, cfg=env_cfg)                      │
│    - 載入 PCCBF 配置（觀測、獎勵、終止條件）                   │
│                                                             │
│  步驟 5：包裝環境                                            │
│    - 視頻錄製包裝（如果需要）                                 │
│    - RSL-RL 接口包裝（RslRlVecEnvWrapper）                   │
│                                                             │
│  步驟 6：創建訓練器                                          │
│    - OnPolicyRunner（RSL-RL 核心）                          │
│    - 初始化 PPO 算法、神經網絡                               │
│                                                             │
│  步驟 7：執行訓練循環                                        │
│    - runner.learn(max_iterations)                          │
│    - 這是訓練的核心！                                        │
│                                                             │
│  步驟 8：保存模型和日誌                                      │
│    - model_*.pt（每 100 iterations）                        │
│    - TensorBoard 日誌                                       │
└─────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────┐
│  第三階段：清理資源（第 407-414 行）                           │
├─────────────────────────────────────────────────────────────┤
│  1. env.close() - 關閉環境                                   │
│  2. simulation_app.close() - 關閉 Isaac Sim                 │
└─────────────────────────────────────────────────────────────┘
```

---

## 🔍 詳細流程圖解

### 階段 1：命令行參數解析

```
用戶輸入：
./isaaclab.sh -p train.py --task PCCBF-Simple-v0 --num_envs 256 --max_iterations 1000 --headless

解析結果：
├─ args_cli.task = "Isaac-Navigation-LocalPlanner-PCCBF-Simple-v0"
├─ args_cli.num_envs = 256
├─ args_cli.max_iterations = 1000
└─ args_cli.headless = True  （不顯示 GUI）
```

### 階段 2：配置載入（Hydra + Gymnasium）

```
@hydra_task_config 裝飾器執行：
    ↓
查找 task 註冊信息：
    gym.envs.registry["Isaac-Navigation-LocalPlanner-PCCBF-Simple-v0"]
    ↓
載入配置入口點：
    ├─ env_cfg_entry_point: local_planner_env_cfg_pccbf_simple:LocalPlannerEnvCfg_PCCBF_SIMPLE
    └─ agent_cfg_entry_point: rsl_rl_ppo_cfg:LocalPlannerPPORunnerCfg
    ↓
實例化配置對象：
    ├─ env_cfg = LocalPlannerEnvCfg_PCCBF_SIMPLE()
    │   ├─ scene.num_envs = 256
    │   ├─ episode_length_s = 35.0
    │   ├─ rewards = PCCBFSimpleRewardsCfg()
    │   └─ observations = ObservationsCfg()
    │
    └─ agent_cfg = LocalPlannerPPORunnerCfg()
        ├─ learning_rate = 0.0003
        ├─ max_iterations = 1000
        └─ policy hidden_dims = [256, 256, 128]
```

### 階段 3：環境創建

```
gym.make(task_name, cfg=env_cfg)
    ↓
創建 ManagerBasedRLEnv 實例
    ↓
初始化場景：
    ├─ 載入 Nova Carter 機器人（nova_carter.usd）
    ├─ 創建 LiDAR 感測器（360 條射線）
    ├─ 生成障礙物（靜態方塊）
    └─ 創建地形（平面）
    ↓
初始化 MDP 管理器：
    ├─ ObservationManager（計算觀測）
    ├─ ActionManager（處理動作）
    ├─ RewardManager（計算獎勵）
    ├─ TerminationManager（檢查終止條件）
    └─ CommandManager（生成目標指令）
    ↓
重置環境（env.reset()）
    ├─ 隨機化機器人位置
    ├─ 生成隨機目標（0.5-2.0 米）
    └─ 返回初始觀測 (num_envs, 369)
```

### 階段 4：訓練循環（核心！）

```
runner.learn(num_learning_iterations=1000)
    ↓
for iteration in range(1000):
    │
    ├─ 收集經驗（Rollout）：
    │   │
    │   for step in range(24):  # num_steps_per_env
    │       │
    │       ├─ 神經網絡前向傳播：
    │       │   obs (256, 369) → Actor → action (256, 2)
    │       │                  → Critic → value (256, 1)
    │       │
    │       ├─ 環境執行動作：
    │       │   env.step(action)
    │       │   ├─ 物理模擬（4 步 @ 0.01s = 0.04s）
    │       │   ├─ 計算觀測（LiDAR, 速度, 目標）
    │       │   ├─ 計算獎勵（progress_to_goal, cbf_safety, ...）
    │       │   ├─ 檢查終止條件（time_out, goal_reached, collision）
    │       │   └─ 返回 next_obs, reward, done
    │       │
    │       └─ 存儲經驗：
    │           (obs, action, reward, value, done) → Buffer
    │
    ├─ 計算優勢函數（Advantage）：
    │   使用 GAE（Generalized Advantage Estimation）
    │   A = δ_t + (γλ)δ_{t+1} + (γλ)²δ_{t+2} + ...
    │   其中 δ_t = r_t + γV(s_{t+1}) - V(s_t)
    │
    ├─ PPO 更新（多個 epochs）：
    │   for epoch in range(5):  # num_learning_epochs
    │       │
    │       for mini_batch in range(4):  # num_mini_batches
    │           │
    │           ├─ 計算策略損失（Policy Loss）：
    │           │   ratio = π_new(a|s) / π_old(a|s)
    │           │   L_CLIP = min(ratio × A, clip(ratio, 1-ε, 1+ε) × A)
    │           │
    │           ├─ 計算價值損失（Value Loss）：
    │           │   L_VF = (V_pred - V_target)²
    │           │
    │           ├─ 計算熵損失（Entropy Loss）：
    │           │   L_ENT = -H(π) = -∑π log(π)
    │           │
    │           ├─ 總損失：
    │           │   L = L_CLIP - c₁×L_VF + c₂×L_ENT
    │           │
    │           ├─ 反向傳播：
    │           │   optimizer.zero_grad()
    │           │   loss.backward()
    │           │   optimizer.step()
    │           │
    │           └─ 更新神經網絡權重
    │
    ├─ 記錄訓練指標：
    │   ├─ Mean reward（平均獎勵）
    │   ├─ Episode length（回合長度）
    │   ├─ 各項獎勵分解（progress_to_goal, cbf_safety, ...）
    │   └─ 終止原因統計（goal_reached, collision, timeout）
    │
    └─ 保存模型檢查點：
        每 100 iterations 保存一次 model_{iteration}.pt
```

---

## 🧮 訓練數學原理

### PPO 算法核心

**目標**：找到最優策略 π*，最大化期望累積獎勵：

```
J(π) = E[∑_{t=0}^{∞} γᵗ r_t]
```

**PPO 損失函數**：

```
L_CLIP = E[min(r_t(θ) × Â_t, clip(r_t(θ), 1-ε, 1+ε) × Â_t)]

其中：
- r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)  # 策略比率
- Â_t = GAE 優勢函數
- ε = clip_param = 0.2  # 裁剪參數
```

**為什麼用 PPO？**
- ✅ 穩定性高：通過裁剪避免策略更新過大
- ✅ 樣本效率：On-Policy 算法，直接優化當前策略
- ✅ 易於調參：超參數對多種任務都有效

---

## 📦 訓練產物

### 目錄結構

```
logs/rsl_rl/local_planner_carter/2025-10-25_14-30-45/
├── model_0.pt                  # 初始模型（隨機權重）
├── model_100.pt                # 第 100 次迭代的模型
├── model_200.pt                # 第 200 次迭代的模型
├── ...
├── model_1000.pt               # 最終模型
├── events.out.tfevents.*       # TensorBoard 日誌文件
├── params/                     # 訓練配置備份
│   ├── env.yaml                # 環境配置（人類可讀）
│   ├── env.pkl                 # 環境配置（Python 對象）
│   ├── agent.yaml              # Agent 配置（人類可讀）
│   └── agent.pkl               # Agent 配置（Python 對象）
├── git/                        # Git 狀態記錄
│   └── git_state.txt           # Commit hash, branch 等信息
└── videos/                     # 訓練視頻（如果啟用 --video）
    └── train/
        ├── rl-video-episode-0.mp4
        ├── rl-video-episode-2000.mp4
        └── ...
```

### 模型文件內容

```python
model_1000.pt = {
    'model_state_dict': {
        'actor.0.weight': tensor(...),  # Actor 網絡權重
        'actor.0.bias': tensor(...),
        'critic.0.weight': tensor(...), # Critic 網絡權重
        ...
    },
    'optimizer_state_dict': {
        'state': {...},                 # Adam 優化器狀態
        'param_groups': [...]
    },
    'iter': 1000,                       # 迭代次數
    'infos': {...}                      # 其他元數據
}
```

---

## 🔬 訓練中的關鍵計算

### 每個 Iteration 的計算流程

```
Iteration N (例如：Iteration 500)
    ↓
1. Rollout 階段（收集經驗）
   ─────────────────────────────
   for env in range(256):                    # 256 個並行環境
       for step in range(24):                # 每個環境 24 步
           ├─ 觀測：obs = env.get_obs()
           │   ├─ LiDAR: (256, 360)
           │   ├─ 速度: (256, 6)
           │   └─ 目標: (256, 3)
           │   → 合併：(256, 369)
           │
           ├─ 推理：action, value = policy(obs)
           │   ├─ Actor(obs) → action (256, 2)
           │   └─ Critic(obs) → value (256, 1)
           │
           ├─ 執行：next_obs, reward, done = env.step(action)
           │   ├─ 物理模擬 4 步（0.04 秒）
           │   ├─ 計算獎勵：
           │   │   progress_to_goal = prev_dist - curr_dist
           │   │   near_goal_shaping = (radius - dist) / radius
           │   │   cbf_safety = ...
           │   │   total_reward = ∑(weight × reward_term)
           │   └─ 檢查終止：
           │       if dist < 0.8m → done (goal_reached)
           │       if time > 35s → done (timeout)
           │
           └─ 存儲：buffer.add(obs, action, reward, value, done)
   
   總收集：256 envs × 24 steps = 6144 個樣本

2. 學習階段（更新神經網絡）
   ─────────────────────────────
   計算 GAE 優勢：
       for t in reversed(range(24)):
           δ_t = reward[t] + γ × value[t+1] - value[t]
           advantage[t] = δ_t + (γλ) × advantage[t+1]
   
   PPO 更新（5 個 epochs）：
       for epoch in range(5):
           shuffle(buffer)  # 打亂數據
           for mini_batch in range(4):
               ├─ 取樣：1536 個樣本（6144 / 4）
               ├─ 前向傳播：
               │   action_new, value_new = policy(obs_batch)
               │   ratio = π_new / π_old
               ├─ 計算損失：
               │   L_CLIP = min(ratio×A, clip(ratio)×A)
               │   L_VF = (value_new - returns)²
               │   L_ENT = -entropy(action_distribution)
               │   L_total = L_CLIP - L_VF + 0.01×L_ENT
               ├─ 反向傳播：
               │   loss.backward()
               │   clip_grad_norm_(parameters, max_norm=1.0)
               │   optimizer.step()
               └─ 更新權重

3. 記錄與保存
   ─────────────────────────────
   ├─ TensorBoard 記錄：
   │   writer.add_scalar("Mean_reward", mean_reward, iteration)
   │   writer.add_scalar("Episode_Reward/progress_to_goal", ...)
   │
   └─ 保存模型（每 100 iterations）：
       torch.save(model_state_dict, "model_500.pt")
```

---

## 📊 訓練指標解讀

### 終端輸出解析

```
Learning iteration 999/1000                       
                       Computation: 1960 steps/s (collection: 3.089s, learning 0.045s)
```
- **1960 steps/s**：每秒處理 1960 個環境步數（性能指標）
- **collection: 3.089s**：收集經驗花費 3.089 秒（Rollout 階段）
- **learning: 0.045s**：更新神經網絡花費 0.045 秒（PPO 更新階段）

```
             Mean action noise std: 3.17
```
- **動作噪音標準差**：3.17
- 用於探索（噪音越大，探索越多）
- 隨著訓練進行通常會逐漸降低

```
          Mean value_function loss: 0.4984
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 4.4793
```
- **Value function loss**：Critic 的預測誤差（越小越好）
- **Surrogate loss**：PPO 的策略損失（負值正常，絕對值越小越好）
- **Entropy loss**：策略的熵（越高表示越隨機，4.4 表示還在探索）

```
                       Mean reward: 37.72
```
- **平均獎勵**：所有環境在統計週期內的平均獎勵
- 這是**最重要的指標**，應該持續上升

```
               Mean episode length: 624.64
```
- **平均 episode 長度**：624 步（約 25 秒）
- 理想情況：隨著訓練，成功的 episodes 會更短

```
   Episode_Reward/progress_to_goal: 0.4958
```
- **接近目標獎勵**：平均每步接近 0.4958 米？不！
- 這是**累積獎勵**（整個 episode 的總和），不是單步
- 0.4958 ÷ 30（權重）= 0.0165 米/步的進度

```
       Episode_Reward/reached_goal: 0.0000
```
- **到達目標獎勵**：在這個統計週期內，沒有成功案例
- 如果成功，這個值會是 200.0（權重）

```
         Episode_Reward/cbf_safety: 0.8000
```
- **CBF 安全獎勵**：0.8（正值）
- 表示 Agent 大部分時間保持安全距離

```
      Episode_Termination/time_out: 0.9922
  Episode_Termination/goal_reached: 0.0078
     Episode_Termination/collision: 0.0000
```
- **終止原因統計**（比例）：
  - 99.22% 超時（沒到達目標）
  - 0.78% 成功（到達目標）
  - 0% 碰撞

---

## 🎯 如何優化訓練

### 調參策略

| 問題 | 調整參數 | 位置 | 效果 |
|------|---------|------|------|
| 成功率太低 | 增加 `progress_to_goal` 權重 | `local_planner_env_cfg_pccbf_simple.py` | 更強的引導 |
| 碰撞率太高 | 增加 `cbf_safety` 權重 | 同上 | 更安全 |
| 訓練不穩定 | 降低 `learning_rate` | `rsl_rl_ppo_cfg.py` | 更穩定 |
| 探索不足 | 增加 `entropy_coef` | 同上 | 更多探索 |
| 學習太慢 | 增加 `num_steps_per_env` | 同上 | 更多樣本 |

---

## 🔄 完整訓練流程時間線

```
0 秒：啟動 Isaac Sim
    ↓
5-10 秒：載入環境和資產
    ↓
10 秒：開始第一個 iteration
    ↓
3.1 秒/iteration × 1000 = 51 分鐘
    ↓
每 100 iterations 保存一次模型
    ↓
51 分鐘後：訓練完成
    ↓
關閉環境和模擬器
```

---

## 💡 學習重點

1. **train.py 的職責**：
   - 啟動模擬器
   - 創建環境
   - 初始化訓練器
   - **不負責**：定義觀測、獎勵、動作（這些在配置文件中）

2. **訓練的核心循環**：
   ```
   收集經驗 → 計算優勢 → PPO 更新 → 重複
   ```

3. **配置的重要性**：
   - 90% 的調優在配置文件中（獎勵權重、目標距離等）
   - train.py 很少需要修改

4. **日誌的價值**：
   - TensorBoard 讓您看到訓練曲線
   - 模型檢查點讓您恢復訓練
   - YAML 配置讓您重現實驗

---

這份文檔幫助您理解 `train.py` 的完整流程！如果有任何疑問，隨時提出。🚀

