# ğŸ—ï¸ Nova Carter å°èˆªè¨“ç·´æ¶æ§‹å®Œæ•´èªªæ˜

> **æ ¸å¿ƒæ–‡æª”**ï¼šæœ¬æ–‡æª”è©³ç´°èªªæ˜æ•´å€‹è¨“ç·´ç³»çµ±çš„è¨­è¨ˆã€æ¶æ§‹å’Œåƒæ•¸é…ç½®

---

## ğŸ“‹ ç›®éŒ„

1. [è¨“ç·´ç›®æ¨™](#è¨“ç·´ç›®æ¨™)
2. [è§€æ¸¬ç©ºé–“ (State) è¨­è¨ˆ](#è§€æ¸¬ç©ºé–“-state-è¨­è¨ˆ)
3. [å‹•ä½œç©ºé–“ (Action) è¨­è¨ˆ](#å‹•ä½œç©ºé–“-action-è¨­è¨ˆ)
4. [çå‹µå‡½æ•¸ (Reward) è¨­è¨ˆ](#çå‹µå‡½æ•¸-reward-è¨­è¨ˆ)
5. [Agent ç¥ç¶“ç¶²è·¯æ¶æ§‹](#agent-ç¥ç¶“ç¶²è·¯æ¶æ§‹)
6. [è¨“ç·´åƒæ•¸é…ç½®](#è¨“ç·´åƒæ•¸é…ç½®)
7. [å¦‚ä½•è§€å¯Ÿè¨“ç·´éç¨‹](#å¦‚ä½•è§€å¯Ÿè¨“ç·´éç¨‹)
8. [ä»£ç¢¼æ¶æ§‹èªªæ˜](#ä»£ç¢¼æ¶æ§‹èªªæ˜)
9. [è¨“ç·´çµæœä½ç½®](#è¨“ç·´çµæœä½ç½®)

---

## ğŸ¯ è¨“ç·´ç›®æ¨™

**ä»»å‹™**ï¼šè¨“ç·´ Nova Carter æ©Ÿå™¨äººåœ¨æœ‰éšœç¤™ç‰©çš„ç’°å¢ƒä¸­ï¼Œä½¿ç”¨ LiDAR æ„Ÿæ¸¬å™¨é€²è¡Œè‡ªä¸»å°èˆªï¼Œå¾èµ·é»åˆ°é”éš¨æ©Ÿç”Ÿæˆçš„ç›®æ¨™é»ã€‚

**æˆåŠŸæ¨™æº–**ï¼š
- æ©Ÿå™¨äººåˆ°é”ç›®æ¨™é»ï¼ˆè·é›¢ < 0.5mï¼‰
- é¿å…ç¢°æ’éšœç¤™ç‰©
- é‹å‹•å¹³æ»‘ï¼Œé¿å…åŸåœ°æ‰“è½‰

**ç’°å¢ƒè¨­å®š**ï¼š
- åœ°å½¢ï¼š40m Ã— 40m å¹³é¢
- éšœç¤™ç‰©ï¼šéœæ…‹æ–¹å¡Šï¼ˆéš¨æ©Ÿåˆ†å¸ƒï¼‰
- æ„Ÿæ¸¬å™¨ï¼š360Â° LiDARï¼ˆæ¯åº¦ 1 æ¢å°„ç·šï¼Œå…± 360 æ¢ï¼‰
- ä¸¦è¡Œè¨“ç·´ï¼š48 å€‹ç’°å¢ƒåŒæ™‚é‹è¡Œ

---

## ğŸ‘ï¸ è§€æ¸¬ç©ºé–“ (State) è¨­è¨ˆ

### ç¸½ç¶­åº¦ï¼š369 ç¶­

Agent æ¯ä¸€æ­¥æ¥æ”¶çš„è§€æ¸¬ç”±ä»¥ä¸‹éƒ¨åˆ†çµ„æˆï¼š

### 1. LiDAR è·é›¢æ•¸æ“š [360 ç¶­]

**ä¾†æº**ï¼šRayCaster æ„Ÿæ¸¬å™¨  
**é…ç½®ä½ç½®**ï¼š`local_planner_env_cfg.py` ç¬¬ 57-70 è¡Œ

```python
lidar = RayCasterCfg(
    prim_path="{ENV_REGEX_NS}/Robot/chassis_link/Lidar",
    offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.0)),
    attach_yaw_only=True,
    pattern_cfg=patterns.LidarPatternCfg(
        channels=1,
        vertical_fov_range=(0.0, 0.0),  # 2D LiDAR
        horizontal_fov_range=(-180, 180),
        horizontal_res=1.0,  # æ¯åº¦ 1 æ¢å°„ç·š
    ),
    max_distance=10.0,  # æœ€å¤§åµæ¸¬è·é›¢
    drift_range=(-0.0, 0.0),
)
```

**ç‰¹æ€§**ï¼š
- 360Â° æ°´å¹³æƒæï¼Œæ¯åº¦ 1 æ¢å°„ç·š
- æ¨™æº–åŒ–åˆ° [0, 1]ï¼Œ0 è¡¨ç¤ºè¿‘ï¼Œ1 è¡¨ç¤ºé 
- æœ€å¤§åµæ¸¬è·é›¢ï¼š10 ç±³

**å¯¦ç¾ä½ç½®**ï¼š`mdp/observations.py` ç¬¬ 21-66 è¡Œ

```python
def lidar_obs(env, sensor_cfg) -> torch.Tensor:
    """è®€å– LiDAR æ•¸æ“šä¸¦æ¨™æº–åŒ–"""
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data
    
    # Isaac Sim 5.0 API: æ‰‹å‹•è¨ˆç®—è·é›¢
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        hit_points = data.ray_hits_w  # (num_envs, 360, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)
    
    # æ¨™æº–åŒ–
    distances = distances / sensor.cfg.max_distance
    distances = torch.clamp(distances, 0.0, 1.0)
    
    return distances  # (num_envs, 360)
```

### 2. æ©Ÿå™¨äººç·šé€Ÿåº¦ [3 ç¶­]

**å…§å®¹**ï¼š`[vx, vy, vz]` åœ¨æ©Ÿå™¨äººåº§æ¨™ç³»ä¸­  
**å¯¦ç¾**ï¼š`mdp/observations.py` ç¬¬ 69-76 è¡Œ

```python
def base_lin_vel(env, asset_cfg) -> torch.Tensor:
    """æ©Ÿå™¨äººåŸºåº§ç·šé€Ÿåº¦ï¼ˆæ©Ÿå™¨äººåº§æ¨™ç³»ï¼‰"""
    asset = env.scene[asset_cfg.name]
    return asset.data.root_lin_vel_b  # (num_envs, 3)
```

### 3. æ©Ÿå™¨äººè§’é€Ÿåº¦ [3 ç¶­]

**å…§å®¹**ï¼š`[wx, wy, wz]` åœ¨æ©Ÿå™¨äººåº§æ¨™ç³»ä¸­  
**å¯¦ç¾**ï¼š`mdp/observations.py` ç¬¬ 79-86 è¡Œ

```python
def base_ang_vel(env, asset_cfg) -> torch.Tensor:
    """æ©Ÿå™¨äººåŸºåº§è§’é€Ÿåº¦ï¼ˆæ©Ÿå™¨äººåº§æ¨™ç³»ï¼‰"""
    asset = env.scene[asset_cfg.name]
    return asset.data.root_ang_vel_b  # (num_envs, 3)
```

### 4. ç›®æ¨™ç›¸å°ä½ç½® [2 ç¶­]

**å…§å®¹**ï¼š`[dx, dy]` ç›®æ¨™åœ¨æ©Ÿå™¨äººåº§æ¨™ç³»ä¸­çš„ç›¸å°ä½ç½®  
**å¯¦ç¾**ï¼š`mdp/observations.py` ç¬¬ 89-111 è¡Œ

```python
def goal_position_in_robot_frame(env, command_name) -> torch.Tensor:
    """ç›®æ¨™ä½ç½®ï¼ˆæ©Ÿå™¨äººåº§æ¨™ç³»ï¼‰"""
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]  # ä¸–ç•Œåº§æ¨™
    
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    robot_quat_w = robot.data.root_quat_w
    
    # è¨ˆç®—ç›¸å°ä½ç½®ï¼ˆä¸–ç•Œåº§æ¨™ï¼‰
    goal_pos_rel_w = goal_pos_w - robot_pos_w
    
    # è½‰æ›åˆ°æ©Ÿå™¨äººåº§æ¨™ç³»
    goal_pos_rel_b = math_utils.quat_apply_inverse(robot_quat_w, goal_pos_rel_w)
    
    return goal_pos_rel_b[:, :2]  # åªè¿”å› x, y
```

**ç‚ºä»€éº¼ä½¿ç”¨æ©Ÿå™¨äººåº§æ¨™ç³»ï¼Ÿ**
- Agent å­¸ç¿’ã€Œå‰æ–¹å¤šé ã€è€Œéã€Œä¸–ç•Œåº§æ¨™ (10, 20)ã€
- æ³›åŒ–æ€§æ›´å¥½ï¼Œä¸ä¾è³´çµ•å°ä½ç½®
- ç¬¦åˆæ©Ÿå™¨äººæ„ŸçŸ¥ç¿’æ…£

### 5. åˆ°ç›®æ¨™çš„è·é›¢ [1 ç¶­]

**å…§å®¹**ï¼šæ­å¹¾é‡Œå¾—è·é›¢æ¨™é‡  
**å¯¦ç¾**ï¼š`mdp/observations.py` ç¬¬ 114-131 è¡Œ

```python
def distance_to_goal(env, command_name) -> torch.Tensor:
    """åˆ°ç›®æ¨™çš„è·é›¢"""
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]
    
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    
    # è¨ˆç®— 2D è·é›¢ï¼ˆå¿½ç•¥ zï¼‰
    distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1, keepdim=True)
    
    return distance  # (num_envs, 1)
```

### è§€æ¸¬ç©ºé–“è¨»å†Š

**é…ç½®ä½ç½®**ï¼š`local_planner_env_cfg.py` ç¬¬ 163-194 è¡Œ

```python
@configclass
class ObservationsCfg:
    @configclass
    class PolicyCfg(ObsGroup):
        lidar_obs = ObsTerm(
            func=mdp.lidar_obs,
            params={"sensor_cfg": SceneEntityCfg("lidar")},
        )  # 360 ç¶­
        
        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)  # 3 ç¶­
        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)  # 3 ç¶­
        
        goal_position_in_robot_frame = ObsTerm(
            func=mdp.goal_position_in_robot_frame,
            params={"command_name": "goal_command"},
        )  # 2 ç¶­
        
        distance_to_goal = ObsTerm(
            func=mdp.distance_to_goal,
            params={"command_name": "goal_command"},
        )  # 1 ç¶­
        
    policy: PolicyCfg = PolicyCfg()

# ç¸½è¨ˆï¼š360 + 3 + 3 + 2 + 1 = 369 ç¶­
```

---

## ğŸ® å‹•ä½œç©ºé–“ (Action) è¨­è¨ˆ

### ç¸½ç¶­åº¦ï¼š2 ç¶­

Agent è¼¸å‡ºé€£çºŒå‹•ä½œï¼š

### 1. ç·šé€Ÿåº¦æŒ‡ä»¤ [1 ç¶­]

**ç¯„åœ**ï¼š-2.0 åˆ° +2.0 m/s  
**å«ç¾©**ï¼šå‰é€²ï¼ˆæ­£å€¼ï¼‰æˆ–å¾Œé€€ï¼ˆè² å€¼ï¼‰é€Ÿåº¦

### 2. è§’é€Ÿåº¦æŒ‡ä»¤ [1 ç¶­]

**ç¯„åœ**ï¼š-Ï€ åˆ° +Ï€ rad/s  
**å«ç¾©**ï¼šå·¦è½‰ï¼ˆæ­£å€¼ï¼‰æˆ–å³è½‰ï¼ˆè² å€¼ï¼‰è§’é€Ÿåº¦

### å‹•ä½œè½‰æ›ç‚ºè¼ªé€Ÿ

**é…ç½®ä½ç½®**ï¼š`local_planner_env_cfg.py` ç¬¬ 142-156 è¡Œ

```python
@configclass
class ActionsCfg:
    joint_vel = mdp.JointVelocityActionCfg(
        asset_name="robot",
        joint_names=[".*wheel.*"],  # åŒ¹é…å·¦å³è¼ª
        scale=1.0,
        use_default_offset=True,
    )

# å·®é€Ÿé©…å‹•æ¨¡å‹ï¼š
# å·¦è¼ªé€Ÿåº¦ = ç·šé€Ÿåº¦ - è§’é€Ÿåº¦ Ã— è¼ªè·/2
# å³è¼ªé€Ÿåº¦ = ç·šé€Ÿåº¦ + è§’é€Ÿåº¦ Ã— è¼ªè·/2
```

**ç‚ºä»€éº¼ä½¿ç”¨å·®é€Ÿé©…å‹•ï¼Ÿ**
- Nova Carter æ˜¯å·®é€Ÿé©…å‹•æ©Ÿå™¨äºº
- å…©å€‹å‹•ä½œç¶­åº¦ç°¡å–®ç›´è§€
- ä¾¿æ–¼ Agent å­¸ç¿’

---

## ğŸ çå‹µå‡½æ•¸ (Reward) è¨­è¨ˆ

### è¨­è¨ˆåŸå‰‡ï¼šå¯†é›†çå‹µ (Dense Rewards)

ä½¿ç”¨å¤šé …çå‹µçµ„åˆï¼Œæ¯ä¸€æ­¥éƒ½çµ¦äºˆåé¥‹ï¼Œå¼•å° Agent å­¸ç¿’ã€‚

### çå‹µçµ„æˆ

**é…ç½®ä½ç½®**ï¼š`local_planner_env_cfg.py` ç¬¬ 219-261 è¡Œ

#### 1. æ¥è¿‘ç›®æ¨™çå‹µ â­ (æ¬Šé‡: 10.0)

```python
progress_to_goal = RewTerm(
    func=mdp.progress_to_goal,
    weight=10.0,
    params={"command_name": "goal_command"},
)
```

**å¯¦ç¾**ï¼š`mdp/rewards.py`

```python
def progress_to_goal(env, command_name: str) -> torch.Tensor:
    """æ¯ä¸€æ­¥æ¥è¿‘ç›®æ¨™çš„è·é›¢çå‹µ"""
    # ç•¶å‰è·é›¢
    current_distance = distance_to_goal(env, command_name)
    
    # å¦‚æœæœ‰ä¸Šä¸€æ­¥è·é›¢ï¼Œè¨ˆç®—é€²æ­¥
    if hasattr(env, '_last_distance'):
        progress = env._last_distance - current_distance
        reward = progress  # æ¥è¿‘ = æ­£çå‹µï¼Œé é›¢ = è² çå‹µ
    else:
        reward = torch.zeros_like(current_distance)
    
    env._last_distance = current_distance.clone()
    return reward
```

**ä½œç”¨**ï¼šæ¯ä¸€æ­¥é¼“å‹µæ©Ÿå™¨äººé è¿‘ç›®æ¨™

#### 2. åˆ°é”ç›®æ¨™çå‹µ â­ (æ¬Šé‡: 100.0)

```python
reached_goal = RewTerm(
    func=mdp.reached_goal,
    weight=100.0,
    params={"command_name": "goal_command", "threshold": 0.5},
)
```

**å¯¦ç¾**ï¼šæª¢æŸ¥è·é›¢ < 0.5m æ™‚çµ¦äºˆå¤§é¡çå‹µ

**ä½œç”¨**ï¼šæ¿€å‹µ Agent å®Œæˆä»»å‹™

#### 3. éšœç¤™ç‰©æ¥è¿‘æ‡²ç½° (æ¬Šé‡: -5.0)

```python
obstacle_proximity_penalty = RewTerm(
    func=mdp.obstacle_proximity_penalty,
    weight=5.0,
    params={"sensor_cfg": SceneEntityCfg("lidar"), "threshold": 0.3},
)
```

**ä½œç”¨**ï¼šç•¶ LiDAR åµæ¸¬åˆ° < 3m çš„éšœç¤™ç‰©æ™‚ï¼Œçµ¦äºˆæ‡²ç½°

#### 4. ç¢°æ’æ‡²ç½° (æ¬Šé‡: -50.0)

```python
collision_penalty = RewTerm(
    func=mdp.collision_penalty,
    weight=50.0,
    params={"sensor_cfg": SceneEntityCfg("contact_forces"), "threshold": 1.0},
)
```

**ä½œç”¨**ï¼šç™¼ç”Ÿç¢°æ’æ™‚çµ¦äºˆå¤§é¡æ‡²ç½°

#### 5. è§’é€Ÿåº¦æ‡²ç½° (æ¬Šé‡: -0.01)

```python
ang_vel_penalty = RewTerm(
    func=mdp.ang_vel_penalty,
    weight=0.01,
)
```

**ä½œç”¨**ï¼šé¿å…æ©Ÿå™¨äººç˜‹ç‹‚æ—‹è½‰

#### 6. éœæ­¢æ‡²ç½° (æ¬Šé‡: -0.1)

```python
standstill_penalty = RewTerm(
    func=mdp.standstill_penalty,
    weight=0.1,
)
```

**ä½œç”¨**ï¼šé¿å…æ©Ÿå™¨äººåŸåœ°ä¸å‹•

### ç¸½çå‹µè¨ˆç®—

```
ç¸½çå‹µ = 
  + progress_to_goal Ã— 10.0
  + reached_goal Ã— 100.0
  - obstacle_proximity Ã— 5.0
  - collision Ã— 50.0
  - ang_vel Ã— 0.01
  - standstill Ã— 0.1
```

---

## ğŸ§  Agent ç¥ç¶“ç¶²è·¯æ¶æ§‹

### æ¼”ç®—æ³•ï¼šPPO (Proximal Policy Optimization)

**é…ç½®ä½ç½®**ï¼š`agents/rsl_rl_ppo_cfg.py` ç¬¬ 34-55 è¡Œ

### Actor Network (ç­–ç•¥ç¶²è·¯)

**åŠŸèƒ½**ï¼šè¼¸å…¥è§€æ¸¬ï¼Œè¼¸å‡ºå‹•ä½œ

```
è¼¸å…¥: è§€æ¸¬ [369 ç¶­]
  â†“
FC Layer 1: [369] â†’ [256]
  â†“ ELU æ¿€æ´»
FC Layer 2: [256] â†’ [256]
  â†“ ELU æ¿€æ´»
FC Layer 3: [256] â†’ [128]
  â†“ ELU æ¿€æ´»
è¼¸å‡º: å‹•ä½œå‡å€¼ [2 ç¶­] + æ¨™æº–å·®
```

**é…ç½®ä»£ç¢¼**ï¼š

```python
policy = RslRlPpoActorCriticCfg(
    init_noise_std=1.0,
    actor_hidden_dims=[256, 256, 128],
    critic_hidden_dims=[256, 256, 128],
    activation="elu",
)
```

### Critic Network (åƒ¹å€¼ç¶²è·¯)

**åŠŸèƒ½**ï¼šè¼¸å…¥è§€æ¸¬ï¼Œä¼°è¨ˆç‹€æ…‹åƒ¹å€¼

```
è¼¸å…¥: è§€æ¸¬ [369 ç¶­]
  â†“
FC Layer 1: [369] â†’ [256]
  â†“ ELU æ¿€æ´»
FC Layer 2: [256] â†’ [256]
  â†“ ELU æ¿€æ´»
FC Layer 3: [256] â†’ [128]
  â†“ ELU æ¿€æ´»
è¼¸å‡º: State Value [1 ç¶­]
```

### ç¸½åƒæ•¸é‡

ç´„ **250K-300K** åƒæ•¸

### ç‚ºä»€éº¼é¸æ“‡é€™å€‹æ¶æ§‹ï¼Ÿ

- **3 å±¤éš±è—å±¤**ï¼šè¶³å¤ æ·±åº¦å­¸ç¿’è¤‡é›œå°èˆªç­–ç•¥
- **256-256-128 ç¶­åº¦**ï¼šå¹³è¡¡è¡¨é”èƒ½åŠ›å’Œè¨“ç·´é€Ÿåº¦
- **ELU æ¿€æ´»**ï¼šæ¯” ReLU æ›´å¹³æ»‘ï¼Œè¨“ç·´æ›´ç©©å®š
- **åˆå§‹å™ªè² 1.0**ï¼šè¶³å¤ æ¢ç´¢

---

## âš™ï¸ è¨“ç·´åƒæ•¸é…ç½®

### PPO è¶…åƒæ•¸

**é…ç½®ä½ç½®**ï¼š`agents/rsl_rl_ppo_cfg.py` ç¬¬ 17-29 è¡Œ

```python
algorithm = RslRlPpoAlgorithmCfg(
    value_loss_coef=1.0,        # Value loss æ¬Šé‡
    use_clipped_value_loss=True,
    clip_param=0.2,              # PPO clip ç¯„åœ
    entropy_coef=0.01,           # Entropy bonus
    num_learning_epochs=5,       # æ¯æ¬¡è¿­ä»£å­¸ç¿’ 5 å€‹ epoch
    num_mini_batches=4,          # æ¯å€‹ epoch 4 å€‹ mini-batch
    learning_rate=1e-3,          # å­¸ç¿’ç‡
    schedule="adaptive",         # è‡ªé©æ‡‰å­¸ç¿’ç‡
    gamma=0.99,                  # æŠ˜æ‰£å› å­
    lam=0.95,                    # GAE lambda
    desired_kl=0.01,             # ç›®æ¨™ KL æ•£åº¦
    max_grad_norm=1.0,           # æ¢¯åº¦è£å‰ª
)
```

### è¨“ç·´å¾ªç’°åƒæ•¸

```python
runner = RslRlOnPolicyRunnerCfg(
    num_steps_per_env=24,        # æ¯å€‹ç’°å¢ƒæ”¶é›† 24 æ­¥
    max_iterations=3000,         # ç¸½å…± 3000 æ¬¡è¿­ä»£
    save_interval=100,           # æ¯ 100 æ¬¡ä¿å­˜æ¨¡å‹
    experiment_name="local_planner_carter",
    run_name="",
    logger="tensorboard",
    neptune_project=None,
)
```

### ç’°å¢ƒåƒæ•¸

**é…ç½®ä½ç½®**ï¼š`local_planner_env_cfg.py` ç¬¬ 328-334 è¡Œ

```python
env_cfg = ManagerBasedRLEnvCfg(
    num_envs=48,                 # ä¸¦è¡Œ 48 å€‹ç’°å¢ƒ
    env_spacing=10.0,            # ç’°å¢ƒé–“è· 10m
    episode_length_s=30.0,       # æ¯å€‹å›åˆ 30 ç§’
    decimation=2,                # æ¯ 2 å€‹ç‰©ç†æ­¥åŸ·è¡Œ 1 æ¬¡ RL æ­¥
    sim=SimulationCfg(
        dt=1.0 / 60.0,           # ç‰©ç†æ¨¡æ“¬ 60 Hz
    ),
)
```

### è¨“ç·´ç¸½é‡è¨ˆç®—

```
ç¸½æ­¥æ•¸ = num_envs Ã— num_steps_per_env Ã— max_iterations
      = 48 Ã— 24 Ã— 3000
      = 3,456,000 æ­¥

å¯¦éš›æ™‚é–“ â‰ˆ 60-90 åˆ†é˜ï¼ˆGPUï¼šRTX 3090/4090ï¼‰
```

---

## ğŸ‘€ å¦‚ä½•è§€å¯Ÿè¨“ç·´éç¨‹

### æ–¹æ³• 1: çµ‚ç«¯è¼¸å‡º

è¨“ç·´æ™‚æœƒé¡¯ç¤ºï¼š

```
Learning iteration 999/3000
  Mean reward: -2598.61
  Mean episode length: 181.36
  Episode_Reward/progress_to_goal: -125.65
  Episode_Reward/reached_goal: 0.00
  Episode_Termination/goal_reached: 0.00
  Episode_Termination/time_out: 1.00
```

**é—œéµæŒ‡æ¨™**ï¼š
- **Mean reward**ï¼šæ‡‰è©²é€æ¼¸ä¸Šå‡
- **reached_goal**ï¼šæˆåŠŸçå‹µï¼ˆæ‡‰è©²å¢åŠ ï¼‰
- **goal_reached**ï¼šæˆåŠŸç‡ï¼ˆæ‡‰è©²å¢åŠ ï¼‰
- **time_out**ï¼šè¶…æ™‚ç‡ï¼ˆæ‡‰è©²æ¸›å°‘ï¼‰

### æ–¹æ³• 2: TensorBoardï¼ˆæ¨è–¦ï¼‰â­

**å•Ÿå‹•å‘½ä»¤**ï¼š

```bash
cd /home/aa/IsaacLab
tensorboard --logdir logs/rsl_rl/
```

**ç€è¦½å™¨**ï¼š`http://localhost:6006`

**è§€å¯Ÿæ›²ç·š**ï¼š
- ğŸ“ˆ **Mean Reward**ï¼šæ•´é«”è¡¨ç¾è¶¨å‹¢
- ğŸ¯ **Episode_Reward/reached_goal**ï¼šæˆåŠŸçå‹µ
- ğŸ“Š **Episode_Termination/goal_reached**ï¼šæˆåŠŸç‡
- âŒ **Episode_Termination/collision**ï¼šç¢°æ’ç‡
- â±ï¸ **Episode_Termination/time_out**ï¼šè¶…æ™‚ç‡
- ğŸ“‰ **Loss/value_function**ï¼šCritic è¨“ç·´ç‹€æ³
- ğŸ“‰ **Loss/surrogate**ï¼šActor è¨“ç·´ç‹€æ³

### æ–¹æ³• 3: æ¸¬è©¦æ¨¡å‹ï¼ˆå¯è¦–åŒ–ï¼‰

**å‘½ä»¤**ï¼š

```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 1 \
    --checkpoint logs/rsl_rl/local_planner_carter/[æ—¥æœŸ]/model_2999.pt
```

**è§€å¯Ÿ**ï¼š
- æ©Ÿå™¨äººé‹å‹•æ˜¯å¦å¹³æ»‘
- æ˜¯å¦èƒ½åˆ°é”ç›®æ¨™
- é¿éšœè¡Œç‚ºæ˜¯å¦åˆç†

---

## ğŸ“ ä»£ç¢¼æ¶æ§‹èªªæ˜

### å®Œæ•´ç›®éŒ„çµæ§‹

```
IsaacLab/
â”œâ”€ source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/
â”‚  â””â”€ local_planner/
â”‚     â”œâ”€ local_planner_env_cfg.py          â† ç’°å¢ƒé…ç½®ï¼ˆä¸»æ–‡ä»¶ï¼‰â­
â”‚     â”œâ”€ __init__.py                       â† ç’°å¢ƒè¨»å†Š
â”‚     â”‚
â”‚     â”œâ”€ agents/                           â† æ¼”ç®—æ³•é…ç½®
â”‚     â”‚  â”œâ”€ rsl_rl_ppo_cfg.py             â† PPO é…ç½® â­
â”‚     â”‚  â””â”€ sb3_ppo_cfg.py                â† Stable Baselines3 é…ç½®
â”‚     â”‚
â”‚     â””â”€ mdp/                              â† MDP çµ„ä»¶å¯¦ç¾ â­
â”‚        â”œâ”€ observations.py               â† è§€æ¸¬å‡½æ•¸å¯¦ç¾
â”‚        â”œâ”€ actions.py                    â† å‹•ä½œè½‰æ›
â”‚        â”œâ”€ rewards.py                    â† çå‹µè¨ˆç®—
â”‚        â””â”€ terminations.py               â† çµ‚æ­¢æ¢ä»¶
â”‚
â””â”€ scripts/reinforcement_learning/rsl_rl/
   â”œâ”€ train.py                            â† è¨“ç·´è…³æœ¬ â­
   â””â”€ play.py                             â† æ¸¬è©¦è…³æœ¬
```

### æ ¸å¿ƒæ–‡ä»¶èªªæ˜

#### 1. local_planner_env_cfg.pyï¼ˆç’°å¢ƒé…ç½®ï¼‰

**åŠŸèƒ½**ï¼šå®šç¾©æ•´å€‹ RL ç’°å¢ƒ

**é—œéµéƒ¨åˆ†**ï¼š
- **LocalPlannerSceneCfg** (ç¬¬ 37-135 è¡Œ)ï¼šå ´æ™¯çµ„ä»¶
  - åœ°å½¢ã€æ©Ÿå™¨äººã€LiDARã€éšœç¤™ç‰©
- **ObservationsCfg** (ç¬¬ 163-194 è¡Œ)ï¼šè§€æ¸¬ç©ºé–“å®šç¾©
- **ActionsCfg** (ç¬¬ 142-156 è¡Œ)ï¼šå‹•ä½œç©ºé–“å®šç¾©
- **RewardsCfg** (ç¬¬ 219-261 è¡Œ)ï¼šçå‹µå‡½æ•¸é…ç½®
- **TerminationsCfg** (ç¬¬ 265-281 è¡Œ)ï¼šçµ‚æ­¢æ¢ä»¶
- **CommandsCfg** (ç¬¬ 198-215 è¡Œ)ï¼šç›®æ¨™ç”Ÿæˆé…ç½®

#### 2. agents/rsl_rl_ppo_cfg.pyï¼ˆPPO é…ç½®ï¼‰

**åŠŸèƒ½**ï¼šå®šç¾© PPO æ¼”ç®—æ³•è¶…åƒæ•¸å’Œç¶²è·¯æ¶æ§‹

**é—œéµéƒ¨åˆ†**ï¼š
- **RslRlPpoAlgorithmCfg** (ç¬¬ 17-29 è¡Œ)ï¼šPPO è¶…åƒæ•¸
- **RslRlPpoActorCriticCfg** (ç¬¬ 34-39 è¡Œ)ï¼šç¶²è·¯æ¶æ§‹
- **RslRlOnPolicyRunnerCfg** (ç¬¬ 43-55 è¡Œ)ï¼šè¨“ç·´å¾ªç’°é…ç½®

#### 3. mdp/observations.pyï¼ˆè§€æ¸¬å¯¦ç¾ï¼‰

**åŠŸèƒ½**ï¼šå¯¦ç¾æ‰€æœ‰è§€æ¸¬å‡½æ•¸

**åŒ…å«å‡½æ•¸**ï¼š
- `lidar_obs()` (ç¬¬ 21-66 è¡Œ)ï¼šLiDAR æ•¸æ“šè®€å–
- `base_lin_vel()` (ç¬¬ 69-76 è¡Œ)ï¼šç·šé€Ÿåº¦
- `base_ang_vel()` (ç¬¬ 79-86 è¡Œ)ï¼šè§’é€Ÿåº¦
- `goal_position_in_robot_frame()` (ç¬¬ 89-111 è¡Œ)ï¼šç›®æ¨™ç›¸å°ä½ç½®
- `distance_to_goal()` (ç¬¬ 114-131 è¡Œ)ï¼šåˆ°ç›®æ¨™è·é›¢

#### 4. mdp/rewards.pyï¼ˆçå‹µå¯¦ç¾ï¼‰

**åŠŸèƒ½**ï¼šå¯¦ç¾æ‰€æœ‰çå‹µè¨ˆç®—å‡½æ•¸

**ä¸»è¦çå‹µ**ï¼š
- `progress_to_goal()`ï¼šæ¥è¿‘ç›®æ¨™çå‹µ
- `reached_goal()`ï¼šåˆ°é”ç›®æ¨™çå‹µ
- `obstacle_proximity_penalty()`ï¼šéšœç¤™ç‰©æ¥è¿‘æ‡²ç½°
- `collision_penalty()`ï¼šç¢°æ’æ‡²ç½°

#### 5. train.pyï¼ˆè¨“ç·´è…³æœ¬ï¼‰

**åŠŸèƒ½**ï¼šè¨“ç·´ä¸»å¾ªç’°

**æµç¨‹**ï¼š
1. è§£æå‘½ä»¤è¡Œåƒæ•¸
2. å‰µå»ºç’°å¢ƒ
3. å‰µå»º PPO Agent
4. è¨“ç·´å¾ªç’°ï¼ˆ3000 æ¬¡è¿­ä»£ï¼‰
5. ä¿å­˜æ¨¡å‹å’Œæ—¥èªŒ

### è¨“ç·´æµç¨‹åœ–

```
train.py
  â†“
1. å‰µå»ºç’°å¢ƒ (LocalPlannerEnvCfg)
  â”œâ”€ åˆå§‹åŒ–å ´æ™¯ï¼ˆåœ°å½¢ã€æ©Ÿå™¨äººã€éšœç¤™ç‰©ï¼‰
  â”œâ”€ åˆå§‹åŒ– LiDAR
  â””â”€ ç”Ÿæˆéš¨æ©Ÿç›®æ¨™
  â†“
2. å‰µå»º PPO Agent
  â”œâ”€ Actor Network [256, 256, 128]
  â”œâ”€ Critic Network [256, 256, 128]
  â””â”€ å„ªåŒ–å™¨ (Adam, lr=1e-3)
  â†“
3. è¨“ç·´å¾ªç’° (3000 æ¬¡è¿­ä»£)
  â”‚
  For iteration = 1 to 3000:
    â”‚
    3.1 æ”¶é›†ç¶“é©— (Rollout)
    â”œâ”€ 48 å€‹ç’°å¢ƒ Ã— 24 æ­¥ = 1152 æ­¥
    â”œâ”€ ç²å–è§€æ¸¬ (369 ç¶­)
    â”œâ”€ Actor è¼¸å‡ºå‹•ä½œ (2 ç¶­)
    â”œâ”€ ç’°å¢ƒåŸ·è¡Œå‹•ä½œ
    â”œâ”€ è¨ˆç®—çå‹µ
    â””â”€ å­˜å„² (s, a, r, s')
    â”‚
    3.2 è¨ˆç®—å„ªå‹¢å‡½æ•¸ (GAE)
    â””â”€ ä½¿ç”¨ Critic ä¼°è¨ˆ Value
    â”‚
    3.3 æ›´æ–°ç­–ç•¥ (PPO)
    â”œâ”€ 5 å€‹ epoch
    â”œâ”€ æ¯å€‹ epoch 4 å€‹ mini-batch
    â”œâ”€ è¨ˆç®— loss (policy + value + entropy)
    â””â”€ åå‘å‚³æ’­æ›´æ–°åƒæ•¸
    â”‚
    3.4 ä¿å­˜æ¨¡å‹ï¼ˆæ¯ 100 æ¬¡ï¼‰
    â””â”€ model_[iteration].pt
  â†“
4. è¨“ç·´å®Œæˆ
â””â”€ ä¿å­˜æœ€çµ‚æ¨¡å‹ model_2999.pt
```

---

## ğŸ“Š è¨“ç·´çµæœä½ç½®

### çµæœç›®éŒ„

```
logs/rsl_rl/local_planner_carter/
â””â”€ [è¨“ç·´æ—¥æœŸæ™‚é–“]/
   â”œâ”€ model_0.pt              # åˆå§‹æ¨¡å‹
   â”œâ”€ model_100.pt            # ç¬¬ 100 æ¬¡è¿­ä»£
   â”œâ”€ model_200.pt            # ç¬¬ 200 æ¬¡è¿­ä»£
   â”œâ”€ ...
   â”œâ”€ model_2999.pt           # æœ€çµ‚æ¨¡å‹
   â”œâ”€ events.out.tfevents.*   # TensorBoard æ—¥èªŒ
   â”œâ”€ params/                 # è¨“ç·´é…ç½®
   â”‚  â”œâ”€ env.json
   â”‚  â””â”€ agent.json
   â””â”€ git/                    # Git ç‰ˆæœ¬ä¿¡æ¯
```

### æ¨¡å‹æ–‡ä»¶å…§å®¹

æ¯å€‹ `model_*.pt` åŒ…å«ï¼š
- Actor ç¶²è·¯æ¬Šé‡
- Critic ç¶²è·¯æ¬Šé‡
- å„ªåŒ–å™¨ç‹€æ…‹
- è¨“ç·´è¿­ä»£æ¬¡æ•¸

### å¦‚ä½•ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹

**æ¸¬è©¦æ¨¡å‹**ï¼š

```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 1 \
    --checkpoint logs/rsl_rl/local_planner_carter/[æ—¥æœŸ]/model_2999.pt
```

**ç¹¼çºŒè¨“ç·´**ï¼š

```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 48 \
    --resume \
    --load_run logs/rsl_rl/local_planner_carter/[æ—¥æœŸ]
```

---

## ğŸ¯ ç¸½çµ

### è¨“ç·´æ¶æ§‹æ ¸å¿ƒè¦é»

| çµ„ä»¶ | é…ç½® | æ–‡ä»¶ä½ç½® |
|------|------|---------|
| **è§€æ¸¬ç©ºé–“** | 369 ç¶­ (LiDAR + é€Ÿåº¦ + ç›®æ¨™) | `observations.py` |
| **å‹•ä½œç©ºé–“** | 2 ç¶­ (ç·šé€Ÿåº¦ + è§’é€Ÿåº¦) | `local_planner_env_cfg.py` ç¬¬ 142-156 è¡Œ |
| **çå‹µå‡½æ•¸** | 6 é …çµ„åˆï¼ˆæ¥è¿‘+åˆ°é”-ç¢°æ’-...ï¼‰ | `local_planner_env_cfg.py` ç¬¬ 219-261 è¡Œ |
| **ç¥ç¶“ç¶²è·¯** | Actor-Critic [256,256,128] | `agents/rsl_rl_ppo_cfg.py` ç¬¬ 34-39 è¡Œ |
| **æ¼”ç®—æ³•** | PPO (lr=1e-3, clip=0.2) | `agents/rsl_rl_ppo_cfg.py` ç¬¬ 17-29 è¡Œ |
| **è¨“ç·´åƒæ•¸** | 48 envs Ã— 24 steps Ã— 3000 iter | `agents/rsl_rl_ppo_cfg.py` ç¬¬ 43-55 è¡Œ |

### é—œéµè¨­è¨ˆæ±ºç­–

1. **å¯†é›†çå‹µ**ï¼šæ¯ä¸€æ­¥éƒ½çµ¦äºˆåé¥‹ï¼ŒåŠ é€Ÿå­¸ç¿’
2. **æ©Ÿå™¨äººåº§æ¨™ç³»**ï¼šç›®æ¨™ç”¨ç›¸å°ä½ç½®ï¼Œæå‡æ³›åŒ–æ€§
3. **2D LiDAR**ï¼š360Â° æ°´å¹³æƒæï¼Œç°¡åŒ–æ„ŸçŸ¥
4. **å·®é€Ÿé©…å‹•**ï¼š2 ç¶­å‹•ä½œï¼Œç°¡å–®ç›´è§€
5. **ä¸¦è¡Œè¨“ç·´**ï¼š48 å€‹ç’°å¢ƒï¼Œå¤§å¹…åŠ é€Ÿ

### è§€å¯Ÿè¨“ç·´çš„æœ€ä½³æ–¹å¼

1. **å¯¦æ™‚ç›£æ§**ï¼šçµ‚ç«¯è¼¸å‡ºæŸ¥çœ‹é—œéµæŒ‡æ¨™
2. **æ·±å…¥åˆ†æ**ï¼šTensorBoard æŸ¥çœ‹è©³ç´°æ›²ç·š
3. **ç›´è§€è©•ä¼°**ï¼šPlay æ¨¡å¼æ¸¬è©¦å¯¦éš›è¡¨ç¾

---

**æœ¬æ–‡æª”æ¶µè“‹äº†è¨“ç·´ç³»çµ±çš„æ‰€æœ‰æ ¸å¿ƒè¨­è¨ˆï¼** ğŸ¯

