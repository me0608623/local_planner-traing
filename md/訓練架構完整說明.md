# 🏗️ Nova Carter 導航訓練架構完整說明

> **核心文檔**：本文檔詳細說明整個訓練系統的設計、架構和參數配置

---

## 📋 目錄

1. [訓練目標](#訓練目標)
2. [觀測空間 (State) 設計](#觀測空間-state-設計)
3. [動作空間 (Action) 設計](#動作空間-action-設計)
4. [獎勵函數 (Reward) 設計](#獎勵函數-reward-設計)
5. [Agent 神經網路架構](#agent-神經網路架構)
6. [訓練參數配置](#訓練參數配置)
7. [如何觀察訓練過程](#如何觀察訓練過程)
8. [代碼架構說明](#代碼架構說明)
9. [訓練結果位置](#訓練結果位置)

---

## 🎯 訓練目標

**任務**：訓練 Nova Carter 機器人在有障礙物的環境中，使用 LiDAR 感測器進行自主導航，從起點到達隨機生成的目標點。

**成功標準**：
- 機器人到達目標點（距離 < 0.5m）
- 避免碰撞障礙物
- 運動平滑，避免原地打轉

**環境設定**：
- 地形：40m × 40m 平面
- 障礙物：靜態方塊（隨機分布）
- 感測器：360° LiDAR（每度 1 條射線，共 360 條）
- 並行訓練：48 個環境同時運行

---

## 👁️ 觀測空間 (State) 設計

### 總維度：369 維

Agent 每一步接收的觀測由以下部分組成：

### 1. LiDAR 距離數據 [360 維]

**來源**：RayCaster 感測器  
**配置位置**：`local_planner_env_cfg.py` 第 57-70 行

```python
lidar = RayCasterCfg(
    prim_path="{ENV_REGEX_NS}/Robot/chassis_link/Lidar",
    offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.0)),
    attach_yaw_only=True,
    pattern_cfg=patterns.LidarPatternCfg(
        channels=1,
        vertical_fov_range=(0.0, 0.0),  # 2D LiDAR
        horizontal_fov_range=(-180, 180),
        horizontal_res=1.0,  # 每度 1 條射線
    ),
    max_distance=10.0,  # 最大偵測距離
    drift_range=(-0.0, 0.0),
)
```

**特性**：
- 360° 水平掃描，每度 1 條射線
- 標準化到 [0, 1]，0 表示近，1 表示遠
- 最大偵測距離：10 米

**實現位置**：`mdp/observations.py` 第 21-66 行

```python
def lidar_obs(env, sensor_cfg) -> torch.Tensor:
    """讀取 LiDAR 數據並標準化"""
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    data = sensor.data
    
    # Isaac Sim 5.0 API: 手動計算距離
    if hasattr(data, "ray_hits_w") and hasattr(data, "pos_w"):
        hit_points = data.ray_hits_w  # (num_envs, 360, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)
    
    # 標準化
    distances = distances / sensor.cfg.max_distance
    distances = torch.clamp(distances, 0.0, 1.0)
    
    return distances  # (num_envs, 360)
```

### 2. 機器人線速度 [3 維]

**內容**：`[vx, vy, vz]` 在機器人座標系中  
**實現**：`mdp/observations.py` 第 69-76 行

```python
def base_lin_vel(env, asset_cfg) -> torch.Tensor:
    """機器人基座線速度（機器人座標系）"""
    asset = env.scene[asset_cfg.name]
    return asset.data.root_lin_vel_b  # (num_envs, 3)
```

### 3. 機器人角速度 [3 維]

**內容**：`[wx, wy, wz]` 在機器人座標系中  
**實現**：`mdp/observations.py` 第 79-86 行

```python
def base_ang_vel(env, asset_cfg) -> torch.Tensor:
    """機器人基座角速度（機器人座標系）"""
    asset = env.scene[asset_cfg.name]
    return asset.data.root_ang_vel_b  # (num_envs, 3)
```

### 4. 目標相對位置 [2 維]

**內容**：`[dx, dy]` 目標在機器人座標系中的相對位置  
**實現**：`mdp/observations.py` 第 89-111 行

```python
def goal_position_in_robot_frame(env, command_name) -> torch.Tensor:
    """目標位置（機器人座標系）"""
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]  # 世界座標
    
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    robot_quat_w = robot.data.root_quat_w
    
    # 計算相對位置（世界座標）
    goal_pos_rel_w = goal_pos_w - robot_pos_w
    
    # 轉換到機器人座標系
    goal_pos_rel_b = math_utils.quat_apply_inverse(robot_quat_w, goal_pos_rel_w)
    
    return goal_pos_rel_b[:, :2]  # 只返回 x, y
```

**為什麼使用機器人座標系？**
- Agent 學習「前方多遠」而非「世界座標 (10, 20)」
- 泛化性更好，不依賴絕對位置
- 符合機器人感知習慣

### 5. 到目標的距離 [1 維]

**內容**：歐幾里得距離標量  
**實現**：`mdp/observations.py` 第 114-131 行

```python
def distance_to_goal(env, command_name) -> torch.Tensor:
    """到目標的距離"""
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]
    
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    
    # 計算 2D 距離（忽略 z）
    distance = torch.norm(goal_pos_w[:, :2] - robot_pos_w[:, :2], dim=-1, keepdim=True)
    
    return distance  # (num_envs, 1)
```

### 觀測空間註冊

**配置位置**：`local_planner_env_cfg.py` 第 163-194 行

```python
@configclass
class ObservationsCfg:
    @configclass
    class PolicyCfg(ObsGroup):
        lidar_obs = ObsTerm(
            func=mdp.lidar_obs,
            params={"sensor_cfg": SceneEntityCfg("lidar")},
        )  # 360 維
        
        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)  # 3 維
        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)  # 3 維
        
        goal_position_in_robot_frame = ObsTerm(
            func=mdp.goal_position_in_robot_frame,
            params={"command_name": "goal_command"},
        )  # 2 維
        
        distance_to_goal = ObsTerm(
            func=mdp.distance_to_goal,
            params={"command_name": "goal_command"},
        )  # 1 維
        
    policy: PolicyCfg = PolicyCfg()

# 總計：360 + 3 + 3 + 2 + 1 = 369 維
```

---

## 🎮 動作空間 (Action) 設計

### 總維度：2 維

Agent 輸出連續動作：

### 1. 線速度指令 [1 維]

**範圍**：-2.0 到 +2.0 m/s  
**含義**：前進（正值）或後退（負值）速度

### 2. 角速度指令 [1 維]

**範圍**：-π 到 +π rad/s  
**含義**：左轉（正值）或右轉（負值）角速度

### 動作轉換為輪速

**配置位置**：`local_planner_env_cfg.py` 第 142-156 行

```python
@configclass
class ActionsCfg:
    joint_vel = mdp.JointVelocityActionCfg(
        asset_name="robot",
        joint_names=[".*wheel.*"],  # 匹配左右輪
        scale=1.0,
        use_default_offset=True,
    )

# 差速驅動模型：
# 左輪速度 = 線速度 - 角速度 × 輪距/2
# 右輪速度 = 線速度 + 角速度 × 輪距/2
```

**為什麼使用差速驅動？**
- Nova Carter 是差速驅動機器人
- 兩個動作維度簡單直觀
- 便於 Agent 學習

---

## 🎁 獎勵函數 (Reward) 設計

### 設計原則：密集獎勵 (Dense Rewards)

使用多項獎勵組合，每一步都給予反饋，引導 Agent 學習。

### 獎勵組成

**配置位置**：`local_planner_env_cfg.py` 第 219-261 行

#### 1. 接近目標獎勵 ⭐ (權重: 10.0)

```python
progress_to_goal = RewTerm(
    func=mdp.progress_to_goal,
    weight=10.0,
    params={"command_name": "goal_command"},
)
```

**實現**：`mdp/rewards.py`

```python
def progress_to_goal(env, command_name: str) -> torch.Tensor:
    """每一步接近目標的距離獎勵"""
    # 當前距離
    current_distance = distance_to_goal(env, command_name)
    
    # 如果有上一步距離，計算進步
    if hasattr(env, '_last_distance'):
        progress = env._last_distance - current_distance
        reward = progress  # 接近 = 正獎勵，遠離 = 負獎勵
    else:
        reward = torch.zeros_like(current_distance)
    
    env._last_distance = current_distance.clone()
    return reward
```

**作用**：每一步鼓勵機器人靠近目標

#### 2. 到達目標獎勵 ⭐ (權重: 100.0)

```python
reached_goal = RewTerm(
    func=mdp.reached_goal,
    weight=100.0,
    params={"command_name": "goal_command", "threshold": 0.5},
)
```

**實現**：檢查距離 < 0.5m 時給予大額獎勵

**作用**：激勵 Agent 完成任務

#### 3. 障礙物接近懲罰 (權重: -5.0)

```python
obstacle_proximity_penalty = RewTerm(
    func=mdp.obstacle_proximity_penalty,
    weight=5.0,
    params={"sensor_cfg": SceneEntityCfg("lidar"), "threshold": 0.3},
)
```

**作用**：當 LiDAR 偵測到 < 3m 的障礙物時，給予懲罰

#### 4. 碰撞懲罰 (權重: -50.0)

```python
collision_penalty = RewTerm(
    func=mdp.collision_penalty,
    weight=50.0,
    params={"sensor_cfg": SceneEntityCfg("contact_forces"), "threshold": 1.0},
)
```

**作用**：發生碰撞時給予大額懲罰

#### 5. 角速度懲罰 (權重: -0.01)

```python
ang_vel_penalty = RewTerm(
    func=mdp.ang_vel_penalty,
    weight=0.01,
)
```

**作用**：避免機器人瘋狂旋轉

#### 6. 靜止懲罰 (權重: -0.1)

```python
standstill_penalty = RewTerm(
    func=mdp.standstill_penalty,
    weight=0.1,
)
```

**作用**：避免機器人原地不動

### 總獎勵計算

```
總獎勵 = 
  + progress_to_goal × 10.0
  + reached_goal × 100.0
  - obstacle_proximity × 5.0
  - collision × 50.0
  - ang_vel × 0.01
  - standstill × 0.1
```

---

## 🧠 Agent 神經網路架構

### 演算法：PPO (Proximal Policy Optimization)

**配置位置**：`agents/rsl_rl_ppo_cfg.py` 第 34-55 行

### Actor Network (策略網路)

**功能**：輸入觀測，輸出動作

```
輸入: 觀測 [369 維]
  ↓
FC Layer 1: [369] → [256]
  ↓ ELU 激活
FC Layer 2: [256] → [256]
  ↓ ELU 激活
FC Layer 3: [256] → [128]
  ↓ ELU 激活
輸出: 動作均值 [2 維] + 標準差
```

**配置代碼**：

```python
policy = RslRlPpoActorCriticCfg(
    init_noise_std=1.0,
    actor_hidden_dims=[256, 256, 128],
    critic_hidden_dims=[256, 256, 128],
    activation="elu",
)
```

### Critic Network (價值網路)

**功能**：輸入觀測，估計狀態價值

```
輸入: 觀測 [369 維]
  ↓
FC Layer 1: [369] → [256]
  ↓ ELU 激活
FC Layer 2: [256] → [256]
  ↓ ELU 激活
FC Layer 3: [256] → [128]
  ↓ ELU 激活
輸出: State Value [1 維]
```

### 總參數量

約 **250K-300K** 參數

### 為什麼選擇這個架構？

- **3 層隱藏層**：足夠深度學習複雜導航策略
- **256-256-128 維度**：平衡表達能力和訓練速度
- **ELU 激活**：比 ReLU 更平滑，訓練更穩定
- **初始噪聲 1.0**：足夠探索

---

## ⚙️ 訓練參數配置

### PPO 超參數

**配置位置**：`agents/rsl_rl_ppo_cfg.py` 第 17-29 行

```python
algorithm = RslRlPpoAlgorithmCfg(
    value_loss_coef=1.0,        # Value loss 權重
    use_clipped_value_loss=True,
    clip_param=0.2,              # PPO clip 範圍
    entropy_coef=0.01,           # Entropy bonus
    num_learning_epochs=5,       # 每次迭代學習 5 個 epoch
    num_mini_batches=4,          # 每個 epoch 4 個 mini-batch
    learning_rate=1e-3,          # 學習率
    schedule="adaptive",         # 自適應學習率
    gamma=0.99,                  # 折扣因子
    lam=0.95,                    # GAE lambda
    desired_kl=0.01,             # 目標 KL 散度
    max_grad_norm=1.0,           # 梯度裁剪
)
```

### 訓練循環參數

```python
runner = RslRlOnPolicyRunnerCfg(
    num_steps_per_env=24,        # 每個環境收集 24 步
    max_iterations=3000,         # 總共 3000 次迭代
    save_interval=100,           # 每 100 次保存模型
    experiment_name="local_planner_carter",
    run_name="",
    logger="tensorboard",
    neptune_project=None,
)
```

### 環境參數

**配置位置**：`local_planner_env_cfg.py` 第 328-334 行

```python
env_cfg = ManagerBasedRLEnvCfg(
    num_envs=48,                 # 並行 48 個環境
    env_spacing=10.0,            # 環境間距 10m
    episode_length_s=30.0,       # 每個回合 30 秒
    decimation=2,                # 每 2 個物理步執行 1 次 RL 步
    sim=SimulationCfg(
        dt=1.0 / 60.0,           # 物理模擬 60 Hz
    ),
)
```

### 訓練總量計算

```
總步數 = num_envs × num_steps_per_env × max_iterations
      = 48 × 24 × 3000
      = 3,456,000 步

實際時間 ≈ 60-90 分鐘（GPU：RTX 3090/4090）
```

---

## 👀 如何觀察訓練過程

### 方法 1: 終端輸出

訓練時會顯示：

```
Learning iteration 999/3000
  Mean reward: -2598.61
  Mean episode length: 181.36
  Episode_Reward/progress_to_goal: -125.65
  Episode_Reward/reached_goal: 0.00
  Episode_Termination/goal_reached: 0.00
  Episode_Termination/time_out: 1.00
```

**關鍵指標**：
- **Mean reward**：應該逐漸上升
- **reached_goal**：成功獎勵（應該增加）
- **goal_reached**：成功率（應該增加）
- **time_out**：超時率（應該減少）

### 方法 2: TensorBoard（推薦）⭐

**啟動命令**：

```bash
cd /home/aa/IsaacLab
tensorboard --logdir logs/rsl_rl/
```

**瀏覽器**：`http://localhost:6006`

**觀察曲線**：
- 📈 **Mean Reward**：整體表現趨勢
- 🎯 **Episode_Reward/reached_goal**：成功獎勵
- 📊 **Episode_Termination/goal_reached**：成功率
- ❌ **Episode_Termination/collision**：碰撞率
- ⏱️ **Episode_Termination/time_out**：超時率
- 📉 **Loss/value_function**：Critic 訓練狀況
- 📉 **Loss/surrogate**：Actor 訓練狀況

### 方法 3: 測試模型（可視化）

**命令**：

```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 1 \
    --checkpoint logs/rsl_rl/local_planner_carter/[日期]/model_2999.pt
```

**觀察**：
- 機器人運動是否平滑
- 是否能到達目標
- 避障行為是否合理

---

## 📁 代碼架構說明

### 完整目錄結構

```
IsaacLab/
├─ source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/
│  └─ local_planner/
│     ├─ local_planner_env_cfg.py          ← 環境配置（主文件）⭐
│     ├─ __init__.py                       ← 環境註冊
│     │
│     ├─ agents/                           ← 演算法配置
│     │  ├─ rsl_rl_ppo_cfg.py             ← PPO 配置 ⭐
│     │  └─ sb3_ppo_cfg.py                ← Stable Baselines3 配置
│     │
│     └─ mdp/                              ← MDP 組件實現 ⭐
│        ├─ observations.py               ← 觀測函數實現
│        ├─ actions.py                    ← 動作轉換
│        ├─ rewards.py                    ← 獎勵計算
│        └─ terminations.py               ← 終止條件
│
└─ scripts/reinforcement_learning/rsl_rl/
   ├─ train.py                            ← 訓練腳本 ⭐
   └─ play.py                             ← 測試腳本
```

### 核心文件說明

#### 1. local_planner_env_cfg.py（環境配置）

**功能**：定義整個 RL 環境

**關鍵部分**：
- **LocalPlannerSceneCfg** (第 37-135 行)：場景組件
  - 地形、機器人、LiDAR、障礙物
- **ObservationsCfg** (第 163-194 行)：觀測空間定義
- **ActionsCfg** (第 142-156 行)：動作空間定義
- **RewardsCfg** (第 219-261 行)：獎勵函數配置
- **TerminationsCfg** (第 265-281 行)：終止條件
- **CommandsCfg** (第 198-215 行)：目標生成配置

#### 2. agents/rsl_rl_ppo_cfg.py（PPO 配置）

**功能**：定義 PPO 演算法超參數和網路架構

**關鍵部分**：
- **RslRlPpoAlgorithmCfg** (第 17-29 行)：PPO 超參數
- **RslRlPpoActorCriticCfg** (第 34-39 行)：網路架構
- **RslRlOnPolicyRunnerCfg** (第 43-55 行)：訓練循環配置

#### 3. mdp/observations.py（觀測實現）

**功能**：實現所有觀測函數

**包含函數**：
- `lidar_obs()` (第 21-66 行)：LiDAR 數據讀取
- `base_lin_vel()` (第 69-76 行)：線速度
- `base_ang_vel()` (第 79-86 行)：角速度
- `goal_position_in_robot_frame()` (第 89-111 行)：目標相對位置
- `distance_to_goal()` (第 114-131 行)：到目標距離

#### 4. mdp/rewards.py（獎勵實現）

**功能**：實現所有獎勵計算函數

**主要獎勵**：
- `progress_to_goal()`：接近目標獎勵
- `reached_goal()`：到達目標獎勵
- `obstacle_proximity_penalty()`：障礙物接近懲罰
- `collision_penalty()`：碰撞懲罰

#### 5. train.py（訓練腳本）

**功能**：訓練主循環

**流程**：
1. 解析命令行參數
2. 創建環境
3. 創建 PPO Agent
4. 訓練循環（3000 次迭代）
5. 保存模型和日誌

### 訓練流程圖

```
train.py
  ↓
1. 創建環境 (LocalPlannerEnvCfg)
  ├─ 初始化場景（地形、機器人、障礙物）
  ├─ 初始化 LiDAR
  └─ 生成隨機目標
  ↓
2. 創建 PPO Agent
  ├─ Actor Network [256, 256, 128]
  ├─ Critic Network [256, 256, 128]
  └─ 優化器 (Adam, lr=1e-3)
  ↓
3. 訓練循環 (3000 次迭代)
  │
  For iteration = 1 to 3000:
    │
    3.1 收集經驗 (Rollout)
    ├─ 48 個環境 × 24 步 = 1152 步
    ├─ 獲取觀測 (369 維)
    ├─ Actor 輸出動作 (2 維)
    ├─ 環境執行動作
    ├─ 計算獎勵
    └─ 存儲 (s, a, r, s')
    │
    3.2 計算優勢函數 (GAE)
    └─ 使用 Critic 估計 Value
    │
    3.3 更新策略 (PPO)
    ├─ 5 個 epoch
    ├─ 每個 epoch 4 個 mini-batch
    ├─ 計算 loss (policy + value + entropy)
    └─ 反向傳播更新參數
    │
    3.4 保存模型（每 100 次）
    └─ model_[iteration].pt
  ↓
4. 訓練完成
└─ 保存最終模型 model_2999.pt
```

---

## 📊 訓練結果位置

### 結果目錄

```
logs/rsl_rl/local_planner_carter/
└─ [訓練日期時間]/
   ├─ model_0.pt              # 初始模型
   ├─ model_100.pt            # 第 100 次迭代
   ├─ model_200.pt            # 第 200 次迭代
   ├─ ...
   ├─ model_2999.pt           # 最終模型
   ├─ events.out.tfevents.*   # TensorBoard 日誌
   ├─ params/                 # 訓練配置
   │  ├─ env.json
   │  └─ agent.json
   └─ git/                    # Git 版本信息
```

### 模型文件內容

每個 `model_*.pt` 包含：
- Actor 網路權重
- Critic 網路權重
- 優化器狀態
- 訓練迭代次數

### 如何使用訓練好的模型

**測試模型**：

```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 1 \
    --checkpoint logs/rsl_rl/local_planner_carter/[日期]/model_2999.pt
```

**繼續訓練**：

```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 48 \
    --resume \
    --load_run logs/rsl_rl/local_planner_carter/[日期]
```

---

## 🎯 總結

### 訓練架構核心要點

| 組件 | 配置 | 文件位置 |
|------|------|---------|
| **觀測空間** | 369 維 (LiDAR + 速度 + 目標) | `observations.py` |
| **動作空間** | 2 維 (線速度 + 角速度) | `local_planner_env_cfg.py` 第 142-156 行 |
| **獎勵函數** | 6 項組合（接近+到達-碰撞-...） | `local_planner_env_cfg.py` 第 219-261 行 |
| **神經網路** | Actor-Critic [256,256,128] | `agents/rsl_rl_ppo_cfg.py` 第 34-39 行 |
| **演算法** | PPO (lr=1e-3, clip=0.2) | `agents/rsl_rl_ppo_cfg.py` 第 17-29 行 |
| **訓練參數** | 48 envs × 24 steps × 3000 iter | `agents/rsl_rl_ppo_cfg.py` 第 43-55 行 |

### 關鍵設計決策

1. **密集獎勵**：每一步都給予反饋，加速學習
2. **機器人座標系**：目標用相對位置，提升泛化性
3. **2D LiDAR**：360° 水平掃描，簡化感知
4. **差速驅動**：2 維動作，簡單直觀
5. **並行訓練**：48 個環境，大幅加速

### 觀察訓練的最佳方式

1. **實時監控**：終端輸出查看關鍵指標
2. **深入分析**：TensorBoard 查看詳細曲線
3. **直觀評估**：Play 模式測試實際表現

---

**本文檔涵蓋了訓練系統的所有核心設計！** 🎯

