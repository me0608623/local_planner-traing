# 🚀 v3 訓練配置與實驗記錄

> **版本**：v3（強化 Progress 驅動力）  
> **日期**：2025-10-30  
> **基於**：v2 結果分析與用戶操作清單

---

## 📋 v3 調整總結

### 基於 v2 結果的問題診斷

**v2 訓練結果（5000 iterations）**：
- ✅ Progress 變正：0.0425（但太小）
- ✅ Heading 大幅降低：0.34（修正成功）
- ✅ 符號正確：Standstill -0.0057
- ⚠️ Position Error：4.19m（改善不足）
- ❌ Success Rate：0%
- ❌ Timeout Rate：100%

**根本問題**：
1. **Progress 權重（30）vs Time Penalty（-30）相互抵消**
2. **Near Goal 範圍太小**（1.5m），從未生效
3. **前進太慢**：每步只接近 0.04m

---

## ⚙️ v3 修改清單

### 操作 1：增加 Progress 權重 + 減少 Time Penalty ⭐

```python
# local_planner_env_cfg_min.py

progress_to_goal:
  v2: weight=30.0
  v3: weight=60.0 ✅
  理由：強力推動接近目標，壓倒 time_penalty

time_penalty:
  v2: weight=0.02（每步 -0.02，累積 -30）
  v3: weight=0.01 ✅（每步 -0.01，累積 -15）
  理由：減輕時間壓力，讓 progress 成為主導信號
```

**預期效果**：
```
v2 獎勵平衡：progress × 30 ≈ 1.2, time ≈ -30 → 被壓制
v3 獎勵平衡：progress × 60 ≈ 2.4, time ≈ -15 → progress 突出
```

### 操作 2：擴大 Near Goal 範圍並提高權重 ⭐

```python
near_goal_shaping:
  v2: radius=1.5m, weight=10.0
  v3: radius=3.0m, weight=20.0 ✅
  理由：
    - v2 從未進入 1.5m（結果為 0）
    - 擴大到 3.0m 讓 shaping 更早生效
    - 提升權重增強吸引力
```

**預期效果**：
- Agent 在 3-4m 範圍時開始獲得 shaping 獎勵
- 形成「遠距離 progress + 中距離 shaping」梯度
- near_goal_shaping 從 0 → > 0.3

### 操作 3：延長訓練 ⭐

```python
# agents/rsl_rl_ppo_cfg.py

max_iterations:
  v2: 5000
  v3: 10000 ✅
  理由：給予更多時間學習與探索
```

### 操作 4：保持 v2 其他修正

```python
# 以下參數保持不變（v2 穩定化已證明有效）

heading_alignment: 1.0（條件式，需前進才給分）
standstill: 4.0（符號已修正）
anti_idle: 2.0
spin_penalty: 0.5
entropy_coef: 0.001
clip_param: 0.1
num_learning_epochs: 3
```

---

## 📊 v3 完整獎勵權重

### 當前配置
```python
progress_to_goal: 60.0      # ↑↑ v3: 主要驅動力
near_goal_shaping: 20.0     # ↑ v3: 擴大影響（radius=3.0m）
heading_alignment: 1.0      # v2: 條件式
reached_goal: 200.0         # v2: 保持
standstill: 4.0             # v2: 符號修正
anti_idle: 2.0              # v2: 反閒置
spin_penalty: 0.5           # v2: 反旋轉
time_penalty: 0.01          # ↓ v3: 減輕壓力
```

### 預期獎勵組成（理想狀態）
```
假設 Agent 每步接近 0.1m：

progress × 60 = 0.1 × 60 = 6.0
near_goal × 20 = 0.3 × 20 = 6.0（進入 3m 範圍）
heading × 1 = 0.5 × 1 = 0.5
time × 0.01 × 1500 = -15.0
standstill ≈ -0.02 × 4 = -0.08

每 episode 總獎勵 ≈ 6 + 6 + 0.5 - 15 - 0.08 ≈ -2.5

若成功到達（0.8m 內）：
  + reached_goal × 200 = 200
  總獎勵 ≈ 200 - 2.5 = 197.5 ✅
```

---

## 🎯 v3 驗收標準

### 主要目標（必須改善）

| 指標 | v2 結果 | v3 目標 | 判斷標準 |
|------|---------|---------|----------|
| **Progress to Goal** | 0.0425 | **> 0.15** | ⭐⭐⭐ 關鍵 |
| **Position Error** | 4.19m | **< 3.0m** | ⭐⭐⭐ 關鍵 |
| **Near Goal Shaping** | 0.0000 | **> 0.2** | ⭐⭐ 重要 |
| **Success Rate** | 0% | **> 5%** | ⭐⭐ 重要 |
| **Timeout Rate** | 100% | **< 85%** | ⭐ 參考 |
| **Mean Reward** | -22.86 | **> 0** | ⭐ 參考 |

### 次要目標（監控用）

| 指標 | v2 結果 | v3 預期 |
|------|---------|---------|
| Heading Alignment | 0.34 | 0.3-0.8（保持低值）|
| Standstill Penalty | -0.0057 | < -0.01（確保負值）|
| Anti-idle | 0.0000 | 0（Agent 有在動）|
| Time Penalty | -0.02 | -0.01（減半）|

---

## 📈 成功判斷邏輯

### ✅ v3 成功標準（同時滿足）
1. **Progress to Goal > 0.15**（前進速度提升 3 倍）
2. **Position Error 下降**（< 3.0m，改善 >1m）
3. **Near Goal Shaping > 0**（開始進入 3m 範圍）

**如果滿足 → v3 修正成功！**

### ⚠️ v3 部分成功（滿足 1 或 2）
- Progress 提升但 Position Error 停滯
  → 繼續延長訓練（15000-20000 iter）
- Position Error 下降但 Progress 仍小
  → 進一步提升 progress 權重到 100

### ❌ v3 失敗（都不滿足）
- Progress 仍然 < 0.1
- Position Error 仍然 > 4m
  → 需要更激進調整：
    1. 移除 time_penalty
    2. Progress 權重 → 150
    3. 或檢查觀測/動作是否有問題

---

## 🔍 操作 3：移動路徑分析（進階調試）

### 檢查是否有前進-後退/繞圈行為

**方法 1：TensorBoard 曲線觀察**
- 看 `Episode_Reward/progress_to_goal` 的波動
- 如果劇烈震盪（-1 ~ +1）→ 前進後退
- 如果平穩接近 0 → 緩慢漂移或繞圈

**方法 2：Play 模式可視化**（如果能運行）
```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
    --task Isaac-Navigation-LocalPlanner-Min-v0 \
    --num_envs 1 \
    --checkpoint logs/rsl_rl/local_planner_carter/2025-10-30_14-26-01/model_4999.pt
```

觀察機器人運動模式：
- 前進 → 後退 → 前進？（來回震盪）
- 繞著目標轉圈？（角度控制問題）
- 緩慢漂移？（動作太小）

### 如果發現前進-後退行為

**新增「後退懲罰」**（可選）：
```python
# rewards.py

def backward_penalty(env: ManagerBasedRLEnv) -> torch.Tensor:
    """懲罰後退行為"""
    robot = env.scene["robot"]
    v_x = robot.data.root_lin_vel_b[:, 0]  # 前進方向速度
    
    # v_x < 0 表示後退
    is_backward = v_x < -0.05
    penalty = torch.where(is_backward, torch.ones_like(v_x) * -1.0, torch.zeros_like(v_x))
    
    return penalty
```

---

## 🔍 操作 4：驗證觀測與動作

### 檢查清單

#### 1. 目標位置計算正確性
**檔案**：`mdp/observations.py` 第 89-111 行

**驗證方法**：
```python
# 在訓練時添加 debug print（臨時）
goal_pos_rel_b = math_utils.quat_apply_inverse(robot_quat_w, goal_pos_rel_w)
print(f"Goal in robot frame: {goal_pos_rel_b[0, :2]}")  # 第一個環境
print(f"Distance: {torch.norm(goal_pos_rel_b[0, :2])}")
```

**預期**：
- 目標在前方 → dx > 0
- 距離應該在 2-6m 範圍
- 隨著訓練，距離應該逐步縮小

#### 2. LiDAR 數據正確性
**檔案**：`mdp/observations.py` 第 21-66 行

**檢查**：
- `ray_hits_w` API 是否正確使用（第 35-44 行）
- 距離標準化是否正確（÷ max_distance）
- 是否有 NaN 或 Inf 值

#### 3. 動作範圍是否合理
**檔案**：`local_planner_env_cfg_min.py` 第 97-110 行

**當前設定**：
```python
max_linear_speed: 0.8 m/s
max_angular_speed: 0.8 rad/s
```

**驗證**：
- 0.8 m/s × 30s = 24m（理論最大移動距離）
- 目標範圍 2-6m → 理論上足夠
- 如果 Agent 仍然太慢，可能需要提升到 1.2 m/s

---

## 🚀 v3 訓練指令

### 啟動訓練（10000 iterations）

```bash
cd /home/aa/IsaacLab

./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Min-v0 \
    --num_envs 24 \
    --max_iterations 10000 \
    --headless
```

**訓練時間**：約 3-3.5 小時（RTX 3090/4090）

### 監控訓練（TensorBoard）

```bash
# 在另一個終端
tensorboard --logdir logs/rsl_rl/local_planner_carter/
```

**重點曲線**（每 1000 iter 檢查一次）：

| Iteration | Progress 目標 | Position Error 目標 | Near Goal 目標 |
|-----------|--------------|-------------------|---------------|
| 1000 | > 0.08 | < 4.0m | > 0 |
| 3000 | > 0.12 | < 3.5m | > 0.1 |
| 5000 | > 0.15 | < 3.0m | > 0.2 |
| 10000 | > 0.20 | < 2.5m | > 0.3 |

### 中途檢查點（3000 iter）

**如果表現良好**（Progress >0.12, Position Error <3.5m）：
- ✅ 繼續訓練到 10000
- 記錄成功案例

**如果仍然不佳**（Progress <0.08, Position Error >4m）：
- ⚠️ 停止訓練
- 套用更激進調整（progress=100, 移除 time_penalty）

---

## 📐 預期獎勵分析

### v3 理論計算

**假設 Agent 每步接近 0.15m**：

```
progress × 60 = 0.15 × 60 = 9.0
near_goal × 20 = 0.3 × 20 = 6.0（假設進入 3m 範圍）
heading × 1 = 0.5 × 1 = 0.5
time × 0.01 × 1500 = -15.0
standstill ≈ -0.02 × 4 = -0.08

每 episode 總獎勵 ≈ 9 + 6 + 0.5 - 15 - 0.08 ≈ 0.42（正值！）

若成功到達：
  + reached_goal × 200 = 200
  總獎勵 ≈ 200 + 0.42 = 200.42 ✅
```

**對比 v2**：
```
v2: progress × 30 ≈ 1.2, time -30 → 總獎勵 -28
v3: progress × 60 ≈ 9.0, time -15 → 總獎勵 +0.4（如果進步夠快）
```

---

## 🔬 進階調試（操作 3 & 4）

### 如果 v3 仍然 Progress < 0.1

**可能問題**：
1. **觀測問題**：goal_position 計算錯誤
2. **動作問題**：差速驅動轉換錯誤
3. **物理問題**：機器人卡住或滑動

**調試步驟**：

#### 步驟 1：驗證觀測
```python
# 在 observations.py 添加臨時 debug
def goal_position_in_robot_frame(env, command_name):
    ...
    goal_pos_rel_b = math_utils.quat_apply_inverse(robot_quat_w, goal_pos_rel_w)
    
    # Debug（僅第一個環境）
    if env.episode_length_buf[0] % 100 == 0:
        print(f"[Env 0] Goal in robot frame: {goal_pos_rel_b[0, :2]}")
        print(f"[Env 0] Distance: {torch.norm(goal_pos_rel_b[0, :2]).item():.2f}m")
    
    return goal_pos_rel_b[:, :2]
```

**預期輸出**：
- 目標應該在前方（dx > 0）
- 距離應該逐步縮小

#### 步驟 2：驗證動作
```python
# 在 actions.py 添加臨時 debug
# 檢查輪速度是否合理
```

#### 步驟 3：檢查 max_linear_speed

**如果 0.8 m/s 太慢**：
```python
# local_planner_env_cfg_min.py
max_linear_speed: 0.8 → 1.2
max_angular_speed: 0.8 → 1.0
```

---

## 📋 訓練執行計畫

### Phase 1：啟動 v3 訓練（立即）
```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Min-v0 \
    --num_envs 24 \
    --max_iterations 10000 \
    --headless
```

### Phase 2：監控與檢查點（訓練中）

**1000 iter 檢查**：
- Progress > 0.08？
- Position Error 開始下降？
- 如果否 → 考慮停止並調整

**3000 iter 檢查**：
- Progress > 0.12？
- Position Error < 3.5m？
- Near Goal Shaping > 0？
- 如果否 → 考慮更激進調整

**5000 iter 檢查**：
- Success Rate > 0？
- 如果是 → 繼續到 10000
- 如果否 → 評估是否需要 v4

**10000 iter 完成**：
- 完整評估與記錄
- 決定是否需要 v4 或進階調整

### Phase 3：結果記錄（訓練後）

創建實驗記錄：
- v3 最終指標
- TensorBoard 曲線截圖
- 成功/失敗案例
- 下一步建議

---

## 💾 文件位置參考

### 需要修改的檔案（v3 已修改）
- ✅ `local_planner_env_cfg_min.py`（獎勵權重）
- ✅ `agents/rsl_rl_ppo_cfg.py`（max_iterations）

### 觀察與調試檔案（如需深入）
- `mdp/observations.py`（第 89-111 行：goal_position）
- `mdp/actions.py`（差速驅動轉換）
- `mdp/rewards.py`（所有獎勵函數實現）

### 文檔參考
- `md/AI交接手冊.md`（完整歷史）
- `md/當前訓練參數總覽.md`（參數速查）
- `md/v2訓練驗收清單.md`（v2 分析）
- 本文檔（v3 配置）

---

## 🎯 v3 核心改進邏輯

### 問題
v2 的 Progress（30）被 Time Penalty（-30）抵消

### 解決
1. **提升 Progress**：30 → 60（增強正向驅動）
2. **減輕 Time**：0.02 → 0.01（減少負向壓制）
3. **擴大 Near Goal**：radius 1.5m → 3.0m（提早生效）
4. **結果**：progress × 60 壓倒 time × 0.01，形成正向梯度

### 理論基礎
- **Potential-based Shaping**：progress 提供主要梯度
- **Multi-scale Shaping**：near_goal 提供中距離梯度
- **Time Cost Balancing**：time 提供弱約束，不壓倒主信號

---

**v3 配置已完成！準備開始 10000 iterations 訓練。** 🚀

**預期訓練時間**：3-3.5 小時

