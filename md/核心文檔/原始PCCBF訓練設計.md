# ğŸ¯ åŸå§‹ PCCBF è¨“ç·´è¨­è¨ˆè©³è§£

> **PPO + PCCBF** Nova Carter å°èˆªè¨“ç·´çš„å®Œæ•´æ¶æ§‹èªªæ˜

---

## ğŸ“‹ ç›®éŒ„

1. [State (è§€æ¸¬ç©ºé–“)](#state-è§€æ¸¬ç©ºé–“)
2. [Action (å‹•ä½œç©ºé–“)](#action-å‹•ä½œç©ºé–“)
3. [Agent (ç¥ç¶“ç¶²è·¯æ¶æ§‹)](#agent-ç¥ç¶“ç¶²è·¯æ¶æ§‹)
4. [Reward (çå‹µå‡½æ•¸)](#reward-çå‹µå‡½æ•¸)
5. [PCCBF æ•´åˆ](#pccbf-æ•´åˆ)
6. [è¨“ç·´åƒæ•¸](#è¨“ç·´åƒæ•¸)

---

## ğŸ” State (è§€æ¸¬ç©ºé–“)

### è§€æ¸¬ç¶­åº¦ç¸½è¦½

**ç¸½ç¶­åº¦**ï¼š`548` = `360 (LiDAR) + 3 (ç·šé€Ÿåº¦) + 3 (è§’é€Ÿåº¦) + 2 (ç›®æ¨™ä½ç½®) + 1 (ç›®æ¨™è·é›¢) + 179 (é æ¸¬éšœç¤™ç‰©)`

**ä½†åœ¨ v4 æœ€å°é…ç½®ä¸­ç°¡åŒ–ç‚º**ï¼š`367` = `360 (LiDAR) + 3 + 3 + 2 + 1`

---

### 1ï¸âƒ£ LiDAR è·é›¢æƒæ

**é…ç½®**ï¼š
```python
lidar = RayCasterCfg(
    prim_path="/World/envs/.*/Robot/Robot/chassis_link/base_link",
    mesh_prim_paths=["/World/ground"],
    pattern_cfg=patterns.LidarPatternCfg(
        channels=1,              # å–®å±¤ 2D LiDAR
        vertical_fov_range=(0.0, 0.0),
        horizontal_fov_range=(-180.0, 180.0),  # 360Â° æƒæ
        horizontal_res=1.0,      # æ¯ 1Â° ä¸€æ¢å°„ç·š
    ),
    max_distance=10.0,           # æœ€å¤§æ¢æ¸¬è·é›¢ 10m
    drift_range=(0.0, 0.0),
    debug_vis=False,
)
```

**è¼¸å‡º**ï¼š
- **å½¢ç‹€**ï¼š`(num_envs, 360)`
- **æ•¸å€¼ç¯„åœ**ï¼š`[0, 1]`ï¼ˆå·²æ­£è¦åŒ–ï¼Œ0 = æœ€é  10mï¼Œ1 = æ¥è§¸ï¼‰
- **ç‰©ç†æ„ç¾©**ï¼š360Â° æ–¹å‘çš„éšœç¤™ç‰©è·é›¢
  - 0Â° = æ©Ÿå™¨äººå‰æ–¹
  - 90Â° = æ©Ÿå™¨äººå·¦å´
  - -90Â° = æ©Ÿå™¨äººå³å´
  - Â±180Â° = æ©Ÿå™¨äººå¾Œæ–¹

**å¯¦ä½œç´°ç¯€**ï¼š
```python
def lidar_obs(env, sensor_cfg):
    sensor = env.scene.sensors[sensor_cfg.name]
    data = sensor.data
    
    # Isaac Sim 5.0+ éœ€è¦æ‰‹å‹•è¨ˆç®—è·é›¢
    if hasattr(data, "ray_hits_w"):
        hit_points = data.ray_hits_w  # (num_envs, 360, 3)
        sensor_pos = data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
        distances = torch.norm(hit_points - sensor_pos, dim=-1)
    
    # æ­£è¦åŒ–åˆ° [0, 1]
    distances = distances / sensor.cfg.max_distance
    distances = torch.clamp(distances, 0.0, 1.0)
    
    return distances  # (num_envs, 360)
```

---

### 2ï¸âƒ£ æ©Ÿå™¨äººé€Ÿåº¦

#### ç·šé€Ÿåº¦ï¼ˆLinear Velocityï¼‰

**è¼¸å‡º**ï¼š
- **å½¢ç‹€**ï¼š`(num_envs, 3)`
- **åº§æ¨™ç³»**ï¼šæ©Ÿå™¨äººåº§æ¨™ç³»ï¼ˆBody Frameï¼‰
- **å…§å®¹**ï¼š`[vx, vy, vz]`
  - `vx`ï¼šå‰é€²é€Ÿåº¦ï¼ˆå‰ +ï¼Œå¾Œ -ï¼‰
  - `vy`ï¼šæ©«å‘é€Ÿåº¦ï¼ˆå·¦ +ï¼Œå³ -ï¼‰
  - `vz`ï¼šå‚ç›´é€Ÿåº¦ï¼ˆé€šå¸¸ç‚º 0ï¼‰

```python
def base_lin_vel(env, asset_cfg):
    asset = env.scene[asset_cfg.name]
    return asset.data.root_lin_vel_b  # (num_envs, 3)
```

#### è§’é€Ÿåº¦ï¼ˆAngular Velocityï¼‰

**è¼¸å‡º**ï¼š
- **å½¢ç‹€**ï¼š`(num_envs, 3)`
- **åº§æ¨™ç³»**ï¼šæ©Ÿå™¨äººåº§æ¨™ç³»
- **å…§å®¹**ï¼š`[wx, wy, wz]`
  - `wx`ï¼šç¹ X è»¸æ—‹è½‰ï¼ˆç¿»æ»¾ï¼Œé€šå¸¸ç‚º 0ï¼‰
  - `wy`ï¼šç¹ Y è»¸æ—‹è½‰ï¼ˆä¿¯ä»°ï¼Œé€šå¸¸ç‚º 0ï¼‰
  - `wz`ï¼šç¹ Z è»¸æ—‹è½‰ï¼ˆåèˆªï¼Œæ­£ = é€†æ™‚é‡ï¼‰

```python
def base_ang_vel(env, asset_cfg):
    asset = env.scene[asset_cfg.name]
    return asset.data.root_ang_vel_b  # (num_envs, 3)
```

---

### 3ï¸âƒ£ ç›®æ¨™è³‡è¨Š

#### ç›®æ¨™ç›¸å°ä½ç½®ï¼ˆRobot Frameï¼‰

**è¼¸å‡º**ï¼š
- **å½¢ç‹€**ï¼š`(num_envs, 2)`
- **åº§æ¨™ç³»**ï¼šæ©Ÿå™¨äººåº§æ¨™ç³»
- **å…§å®¹**ï¼š`[dx, dy]`
  - `dx`ï¼šå‰å¾Œæ–¹å‘ï¼ˆå‰ +ï¼Œå¾Œ -ï¼‰
  - `dy`ï¼šå·¦å³æ–¹å‘ï¼ˆå·¦ +ï¼Œå³ -ï¼‰

**è½‰æ›æµç¨‹**ï¼š
```python
def goal_position_in_robot_frame(env, command_name):
    # 1. ç²å–ç›®æ¨™ä¸–ç•Œåº§æ¨™
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]  # (num_envs, 3)
    
    # 2. ç²å–æ©Ÿå™¨äººä¸–ç•Œåº§æ¨™
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    robot_quat_w = robot.data.root_quat_w
    
    # 3. è¨ˆç®—ç›¸å°ä½ç½®ï¼ˆä¸–ç•Œåº§æ¨™ç³»ï¼‰
    goal_pos_rel_w = goal_pos_w - robot_pos_w
    
    # 4. è½‰æ›åˆ°æ©Ÿå™¨äººåº§æ¨™ç³»
    goal_pos_rel_b = quat_apply_inverse(robot_quat_w, goal_pos_rel_w)
    
    # 5. åªè¿”å› x, yï¼ˆå¿½ç•¥ zï¼‰
    return goal_pos_rel_b[:, :2]
```

#### ç›®æ¨™è·é›¢ï¼ˆScalarï¼‰

**è¼¸å‡º**ï¼š
- **å½¢ç‹€**ï¼š`(num_envs, 1)`
- **æ•¸å€¼ç¯„åœ**ï¼š`[0, âˆ)`ï¼ˆå–®ä½ï¼šç±³ï¼‰
- **è¨ˆç®—**ï¼š2D æ­æ°è·é›¢ï¼ˆå¿½ç•¥ Z è»¸ï¼‰

```python
def distance_to_goal(env, command_name):
    command = env.command_manager.get_command(command_name)
    goal_pos_w = command[:, :3]
    
    robot = env.scene["robot"]
    robot_pos_w = robot.data.root_pos_w
    
    # 2D è·é›¢ï¼ˆå¿½ç•¥ zï¼‰
    distance = torch.norm(
        goal_pos_w[:, :2] - robot_pos_w[:, :2], 
        dim=-1, 
        keepdim=True
    )
    
    return distance  # (num_envs, 1)
```

---

### 4ï¸âƒ£ PCCBF é æ¸¬è§€æ¸¬ï¼ˆå¯é¸ï¼‰

**æ³¨æ„**ï¼šç•¶å‰ v4 é…ç½®ä¸­**æœªå•Ÿç”¨**ï¼Œä½†ä»£ç¢¼ä¸­å·²å¯¦ä½œ

#### é æ¸¬éšœç¤™ç‰©è·é›¢

**åŠŸèƒ½**ï¼šé æ¸¬æœªä¾† N æ­¥çš„éšœç¤™ç‰©åˆ†å¸ƒ

```python
def predicted_obstacle_distances(
    env, 
    sensor_cfg,
    prediction_horizon: int = 3  # é æ¸¬æœªä¾† 3 æ­¥
):
    """
    åŸºæ–¼ PCCBF-MPC çš„ã€Œå‰ç»æ™‚åŸŸåœ°åœ–ï¼ˆFTD Mapï¼‰ã€æ¦‚å¿µ
    é æ¸¬æœªä¾† N æ­¥æ©Ÿå™¨äººå‘¨åœçš„éšœç¤™ç‰©åˆ†å¸ƒ
    """
    sensor = env.scene.sensors[sensor_cfg.name]
    robot = env.scene["robot"]
    
    # ç•¶å‰ LiDAR æ•¸æ“š
    current_distances = lidar_obs(env, sensor_cfg)
    
    # æ©Ÿå™¨äººç•¶å‰é€Ÿåº¦
    lin_vel = robot.data.root_lin_vel_b
    ang_vel = robot.data.root_ang_vel_b
    
    # ç°¡åŒ–ï¼šç·šæ€§é æ¸¬ï¼ˆç­‰é€Ÿæ¨¡å‹ï¼‰
    # çœŸå¯¦ PCCBF ä½¿ç”¨å¡çˆ¾æ›¼æ¿¾æ³¢å™¨
    dt = 0.1  # æ™‚é–“æ­¥é•·
    predicted_positions = []
    
    for t in range(1, prediction_horizon + 1):
        # é æ¸¬æ©Ÿå™¨äººä½ç½®
        pred_x = lin_vel[:, 0] * t * dt
        pred_y = lin_vel[:, 1] * t * dt
        pred_theta = ang_vel[:, 2] * t * dt
        
        # è½‰æ› LiDAR åˆ°é æ¸¬ä½ç½®
        # ... (ç°¡åŒ–å¯¦ä½œ)
        
    return min_predicted_distances
```

---

### ğŸ“Š è§€æ¸¬ç©ºé–“ç¸½çµ

```python
@configclass
class ObservationsCfg:
    @configclass
    class PolicyCfg(ObsGroup):
        # 1. LiDAR æƒæ (360)
        lidar_distances = ObsTerm(
            func=mdp.lidar_obs,
            params={"sensor_cfg": SceneEntityCfg("lidar")},
        )
        
        # 2. ç·šé€Ÿåº¦ (3)
        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
        
        # 3. è§’é€Ÿåº¦ (3)
        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
        
        # 4. ç›®æ¨™ç›¸å°ä½ç½® (2)
        goal_position = ObsTerm(
            func=mdp.goal_position_in_robot_frame,
            params={"command_name": "goal_command"},
        )
        
        # 5. ç›®æ¨™è·é›¢ (1)
        goal_distance = ObsTerm(
            func=mdp.distance_to_goal,
            params={"command_name": "goal_command"},
        )
        
        def __post_init__(self):
            self.enable_corruption = False
            self.concatenate_terms = True  # ä¸²æ¥æ‰€æœ‰è§€æ¸¬

    policy: PolicyCfg = PolicyCfg()
```

**ç¸½ç¶­åº¦**ï¼š360 + 3 + 3 + 2 + 1 = **369**

---

## ğŸ® Action (å‹•ä½œç©ºé–“)

### å·®é€Ÿé©…å‹•æ§åˆ¶ï¼ˆDifferential Driveï¼‰

**ç¶­åº¦**ï¼š`2` = `[ç·šé€Ÿåº¦æŒ‡ä»¤, è§’é€Ÿåº¦æŒ‡ä»¤]`

**é…ç½®**ï¼š
```python
@configclass
class ActionsCfg:
    base_velocity = mdp.DifferentialDriveActionCfg(
        asset_name="robot",
        left_wheel_joint_names=["joint_wheel_left"],
        right_wheel_joint_names=["joint_wheel_right"],
        wheel_radius=0.125,       # è¼ªå­åŠå¾‘ 12.5cm
        wheel_base=0.413,         # è¼ªè· 41.3cm
        max_linear_speed=0.8,     # æœ€å¤§ç·šé€Ÿåº¦ 0.8 m/s
        max_angular_speed=0.8,    # æœ€å¤§è§’é€Ÿåº¦ 0.8 rad/s
    )
```

### å‹•ä½œè½‰æ›æµç¨‹

```
ç¥ç¶“ç¶²è·¯è¼¸å‡º (NN Output)
  â†“
[v_cmd, w_cmd]  â† é€£çºŒå€¼åœ¨ [-1, 1] ä¹‹é–“
  â†“
ç¸®æ”¾åˆ°ç‰©ç†ç¯„åœ (Scaling)
  â†“
v = v_cmd * max_linear_speed   # [-0.8, 0.8] m/s
w = w_cmd * max_angular_speed  # [-0.8, 0.8] rad/s
  â†“
å·®é€Ÿé©…å‹•é‹å‹•å­¸ (Differential Drive Kinematics)
  â†“
v_left  = (v - w * wheel_base / 2) / wheel_radius
v_right = (v + w * wheel_base / 2) / wheel_radius
  â†“
è¼ªå­é€Ÿåº¦æŒ‡ä»¤ç™¼é€åˆ°æ¨¡æ“¬å™¨
```

### å‹•ä½œç¯„åœ

| å‹•ä½œ | ç¥ç¶“ç¶²è·¯è¼¸å‡º | ç‰©ç†ç¯„åœ | ç‰©ç†æ„ç¾© |
|------|-------------|----------|----------|
| **ç·šé€Ÿåº¦** | `[-1, 1]` | `[-0.8, 0.8]` m/s | å‰é€²ï¼ˆ+ï¼‰/ å¾Œé€€ï¼ˆ-ï¼‰ |
| **è§’é€Ÿåº¦** | `[-1, 1]` | `[-0.8, 0.8]` rad/s | é€†æ™‚é‡ï¼ˆ+ï¼‰/ é †æ™‚é‡ï¼ˆ-ï¼‰ |

**å…¸å‹å‹•ä½œ**ï¼š
- `[1.0, 0.0]`ï¼šæœ€å¿«å‰é€²
- `[-1.0, 0.0]`ï¼šæœ€å¿«å¾Œé€€
- `[0.0, 1.0]`ï¼šåŸåœ°é€†æ™‚é‡æ—‹è½‰
- `[0.0, -1.0]`ï¼šåŸåœ°é †æ™‚é‡æ—‹è½‰
- `[0.5, 0.5]`ï¼šå‰é€² + å·¦è½‰

---

## ğŸ§  Agent (ç¥ç¶“ç¶²è·¯æ¶æ§‹)

### PPO Actor-Critic æ¶æ§‹

**é…ç½®**ï¼š
```python
policy = RslRlPpoActorCriticCfg(
    init_noise_std=0.5,          # åˆå§‹æ¢ç´¢å™ªéŸ³
    actor_hidden_dims=[256, 256, 128],
    critic_hidden_dims=[256, 256, 128],
    activation="elu",            # ELU æ¿€æ´»å‡½æ•¸
)
```

---

### 1ï¸âƒ£ Actor Networkï¼ˆç­–ç•¥ç¶²è·¯ï¼‰

**åŠŸèƒ½**ï¼šè¼¸å‡ºå‹•ä½œåˆ†å¸ƒ

```
Input: State (369)
  â†“
Dense(369 â†’ 256) + ELU
  â†“
Dense(256 â†’ 256) + ELU
  â†“
Dense(256 â†’ 128) + ELU
  â†“
Dense(128 â†’ 2)  # å‡å€¼ Î¼
  â†“
Output: Gaussian(Î¼, Ïƒ)
        Ïƒ åˆå§‹ç‚º 0.5ï¼Œè¨“ç·´ä¸­é€æ¼¸é™ä½
  â†“
Sample: action ~ N(Î¼, ÏƒÂ²)
  â†“
Clip: action âˆˆ [-1, 1]
```

**åƒæ•¸é‡**ï¼š
```
Layer 1: 369 Ã— 256 + 256 = 94,720
Layer 2: 256 Ã— 256 + 256 = 65,792
Layer 3: 256 Ã— 128 + 128 = 32,896
Output:  128 Ã— 2 + 2 = 258
Total: ~193k åƒæ•¸
```

---

### 2ï¸âƒ£ Critic Networkï¼ˆåƒ¹å€¼ç¶²è·¯ï¼‰

**åŠŸèƒ½**ï¼šä¼°è¨ˆç‹€æ…‹åƒ¹å€¼ V(s)

```
Input: State (369)
  â†“
Dense(369 â†’ 256) + ELU
  â†“
Dense(256 â†’ 256) + ELU
  â†“
Dense(256 â†’ 128) + ELU
  â†“
Dense(128 â†’ 1)
  â†“
Output: V(s)  # ç‹€æ…‹åƒ¹å€¼ï¼ˆæ¨™é‡ï¼‰
```

**åƒæ•¸é‡**ï¼š
```
Layer 1: 369 Ã— 256 + 256 = 94,720
Layer 2: 256 Ã— 256 + 256 = 65,792
Layer 3: 256 Ã— 128 + 128 = 32,896
Output:  128 Ã— 1 + 1 = 129
Total: ~193k åƒæ•¸
```

---

### ğŸ”§ ç¶²è·¯è¨­è¨ˆç‰¹é»

**1. ELU æ¿€æ´»å‡½æ•¸**
```python
ELU(x) = {
    x,              if x > 0
    Î±(e^x - 1),     if x â‰¤ 0
}
```
- å„ªé»ï¼šå…è¨±è² å€¼è¼¸å‡ºï¼Œæ”¶æ–‚æ›´å¿«
- vs ReLUï¼šä¸æœƒæœ‰"æ­»ç¥ç¶“å…ƒ"å•é¡Œ

**2. é€æ­¥è¡°æ¸›çš„æ¢ç´¢**
```python
init_noise_std = 0.5  # åˆå§‹æ¢ç´¢

# è¨“ç·´éç¨‹ä¸­ï¼š
Ïƒ(t) = Ïƒâ‚€ Ã— decay_factor^t
# é€æ¼¸å¾ 0.5 é™åˆ° ~0.1
```

**3. å…±äº«ç‰¹å¾µæå–**
- Actor å’Œ Critic ä½¿ç”¨ç›¸åŒæ¶æ§‹
- å¯é¸ï¼šå…±äº«å‰å¹¾å±¤ï¼ˆç•¶å‰æœªå…±äº«ï¼‰

---

## ğŸ Reward (çå‹µå‡½æ•¸)

### v4 çå‹µé…ç½®ï¼ˆç•¶å‰ç‰ˆæœ¬ï¼‰

**ç¸½çå‹µé …**ï¼š8 é …ï¼ˆæ­£å‘ 4 é … + è² å‘ 4 é …ï¼‰

---

### 1ï¸âƒ£ æ­£å‘çå‹µï¼ˆPositive Rewardsï¼‰

#### progress_to_goalï¼ˆæ¥è¿‘ç›®æ¨™ï¼‰

**æ¬Šé‡**ï¼š`60.0`ï¼ˆæœ€é‡è¦ï¼‰

**è¨ˆç®—**ï¼š
```python
def progress_to_goal_reward(env, command_name):
    # ç•¶å‰è·é›¢
    current_distance = ||goal_pos - robot_pos||â‚‚
    
    # é€²åº¦ = ä¸Šä¸€æ­¥è·é›¢ - ç•¶å‰è·é›¢
    progress = prev_distance - current_distance
    
    # æ­£å€¼ï¼šæ¥è¿‘ç›®æ¨™
    # è² å€¼ï¼šé é›¢ç›®æ¨™
    # é›¶å€¼ï¼šè·é›¢ä¸è®Š
    
    return progress
```

**ç‰©ç†æ„ç¾©**ï¼š
- æ¯æ­¥æ¥è¿‘ç›®æ¨™ 0.1m â†’ çå‹µ = 0.1 Ã— 60 = 6.0
- æ¯æ­¥é é›¢ç›®æ¨™ 0.05m â†’ çå‹µ = -0.05 Ã— 60 = -3.0

---

#### reached_goalï¼ˆåˆ°é”ç›®æ¨™ï¼‰

**æ¬Šé‡**ï¼š`200.0`

**è¨ˆç®—**ï¼š
```python
def reached_goal_reward(env, command_name, threshold=0.8):
    distance = ||goal_pos - robot_pos||â‚‚
    reward = 1.0 if distance < 0.8m else 0.0
    return reward
```

**ç‰©ç†æ„ç¾©**ï¼š
- åˆ°é”ç›®æ¨™ï¼ˆ< 0.8mï¼‰â†’ çå‹µ = 1.0 Ã— 200 = 200
- æœªåˆ°é” â†’ çå‹µ = 0

---

#### near_goal_shapingï¼ˆè¿‘ç›®æ¨™å¡‘å½¢ï¼‰

**æ¬Šé‡**ï¼š`20.0`

**è¨ˆç®—**ï¼š
```python
def near_goal_shaping(env, command_name, radius=3.0):
    distance = ||goal_pos - robot_pos||â‚‚
    
    if distance < radius:
        # æŒ‡æ•¸è¡°æ¸›çå‹µ
        reward = exp(-distance / radius)
    else:
        reward = 0.0
    
    return reward
```

**ç‰©ç†æ„ç¾©**ï¼š
- è·é›¢ 0m â†’ çå‹µ = 1.0 Ã— 20 = 20
- è·é›¢ 1.5m â†’ çå‹µ = exp(-0.5) Ã— 20 â‰ˆ 12
- è·é›¢ 3m â†’ çå‹µ = exp(-1) Ã— 20 â‰ˆ 7.4
- è·é›¢ > 3m â†’ çå‹µ = 0

---

#### heading_alignmentï¼ˆæœå‘å°é½Šï¼‰

**æ¬Šé‡**ï¼š`1.0`ï¼ˆæ¢ä»¶å¼ï¼‰

**è¨ˆç®—**ï¼š
```python
def heading_alignment_reward(env, command_name, v_min=0.1):
    # è¨ˆç®—æœå‘èª¤å·®
    goal_dir = goal_pos - robot_pos
    robot_heading = robot_quaternion_to_heading()
    
    heading_error = angle_between(robot_heading, goal_dir)
    
    # æ¢ä»¶ï¼šå¿…é ˆåœ¨å‰é€²æ™‚æ‰çå‹µ
    is_moving = ||lin_vel|| > v_min
    
    if is_moving:
        reward = cos(heading_error)  # [1, -1]
    else:
        reward = 0.0
    
    return reward
```

**ç‰©ç†æ„ç¾©**ï¼š
- å®Œç¾æœå‘ï¼ˆ0Â°ï¼‰ä¸”å‰é€² â†’ çå‹µ = 1.0 Ã— 1 = 1.0
- å‚ç›´æœå‘ï¼ˆ90Â°ï¼‰ä¸”å‰é€² â†’ çå‹µ = 0.0
- åå‘æœå‘ï¼ˆ180Â°ï¼‰ä¸”å‰é€² â†’ çå‹µ = -1.0
- éœæ­¢ä¸å‹• â†’ çå‹µ = 0ï¼ˆé¿å…åŸåœ°è½‰åœˆï¼‰

---

### 2ï¸âƒ£ è² å‘æ‡²ç½°ï¼ˆPenaltiesï¼‰

#### standstill_penaltyï¼ˆéœæ­¢æ‡²ç½°ï¼‰

**æ¬Šé‡**ï¼š`1.0`ï¼ˆv4 é™ä½ï¼‰

**è¨ˆç®—**ï¼š
```python
def standstill_penalty(env):
    speed = ||lin_vel||â‚‚
    
    # é€Ÿåº¦è¶Šä½ï¼Œæ‡²ç½°è¶Šå¤§
    penalty = -exp(-speed / 0.1)
    
    return penalty  # ç¯„åœ [-1, 0]
```

**ç‰©ç†æ„ç¾©**ï¼š
- é€Ÿåº¦ 0 m/s â†’ æ‡²ç½° = -1.0 Ã— 1.0 = -1.0
- é€Ÿåº¦ 0.1 m/s â†’ æ‡²ç½° â‰ˆ -0.37
- é€Ÿåº¦ > 0.5 m/s â†’ æ‡²ç½° â‰ˆ 0

---

#### anti_idle_penaltyï¼ˆåé–’ç½®æ‡²ç½°ï¼‰

**æ¬Šé‡**ï¼š`0.5`ï¼ˆv4 é™ä½ï¼‰

**è¨ˆç®—**ï¼š
```python
def anti_idle_penalty(env, v_threshold=0.05):
    speed = ||lin_vel||â‚‚
    
    if speed < v_threshold:
        penalty = -1.0
    else:
        penalty = 0.0
    
    return penalty
```

**ç‰©ç†æ„ç¾©**ï¼š
- é€Ÿåº¦ < 0.05 m/s â†’ æ‡²ç½° = -1.0 Ã— 0.5 = -0.5
- é€Ÿåº¦ â‰¥ 0.05 m/s â†’ æ‡²ç½° = 0

---

#### spin_penaltyï¼ˆæ—‹è½‰æ‡²ç½°ï¼‰

**æ¬Šé‡**ï¼š`0.1`ï¼ˆv4 é™ä½ï¼‰

**è¨ˆç®—**ï¼š
```python
def spin_penalty(env, w_threshold=0.5, v_threshold=0.1):
    ang_speed = |ang_vel_z|
    lin_speed = ||lin_vel||â‚‚
    
    # é«˜è§’é€Ÿåº¦ + ä½ç·šé€Ÿåº¦ = åŸåœ°æ‰“è½‰
    if ang_speed > w_threshold and lin_speed < v_threshold:
        penalty = -ang_speed
    else:
        penalty = 0.0
    
    return penalty
```

**ç‰©ç†æ„ç¾©**ï¼š
- åŸåœ°å¿«é€Ÿæ—‹è½‰ï¼ˆw=0.8, v=0.05ï¼‰â†’ æ‡²ç½° = -0.8 Ã— 0.1 = -0.08
- å‰é€²ä¸­è½‰å½ï¼ˆw=0.3, v=0.5ï¼‰â†’ æ‡²ç½° = 0

---

#### time_penaltyï¼ˆæ™‚é–“æ‡²ç½°ï¼‰

**æ¬Šé‡**ï¼š`0.005`ï¼ˆv4 é™ä½ï¼‰

**è¨ˆç®—**ï¼š
```python
def time_penalty(env):
    return -1.0  # æ¯æ­¥å›ºå®šæ‡²ç½°
```

**ç‰©ç†æ„ç¾©**ï¼š
- æ¯æ­¥ â†’ æ‡²ç½° = -1.0 Ã— 0.005 = -0.005
- æ¯ episodeï¼ˆ1500 æ­¥ï¼‰â†’ ç¸½æ‡²ç½° = -7.5

---

### ğŸ“Š çå‹µæ¬Šé‡ç¸½è¦½ï¼ˆv4 é…ç½®ï¼‰

| çå‹µé … | æ¬Šé‡ | é¡å‹ | ç¯„åœ | ç›®çš„ |
|--------|------|------|------|------|
| **progress_to_goal** | 60.0 | æ­£å‘ | (-âˆ, +âˆ) | ä¸»è¦é©…å‹•åŠ› |
| **reached_goal** | 200.0 | æ­£å‘ | {0, 200} | æˆåŠŸçå‹µ |
| **near_goal_shaping** | 20.0 | æ­£å‘ | [0, 20] | è¿‘è·é›¢å¼•å° |
| **heading_alignment** | 1.0 | æ­£å‘ | [-1, 1] | æ–¹å‘è¼”åŠ© |
| **standstill_penalty** | 1.0 | è² å‘ | [-1, 0] | é˜²æ­¢éœæ­¢ |
| **anti_idle** | 0.5 | è² å‘ | [-0.5, 0] | é˜²æ­¢é–’ç½® |
| **spin_penalty** | 0.1 | è² å‘ | [-0.08, 0] | é˜²æ­¢æ‰“è½‰ |
| **time_penalty** | 0.005 | è² å‘ | -0.005 | é¼“å‹µå¿«é€Ÿ |

---

### ğŸ¯ çå‹µè¨­è¨ˆç†å¿µï¼ˆv4ï¼‰

**æ­£å‘ä¸»å°**ï¼š
```
ç†æƒ³æƒ…æ³ï¼ˆå‰é€² 0.1m/æ­¥ï¼‰ï¼š
  progress: 0.1 Ã— 60 = 6.0
  near_goal: ~0.3 Ã— 20 = 6.0
  heading: ~0.5 Ã— 1 = 0.5
  Total positive: ~12.5

è² å‘ç´„æŸï¼š
  time: -0.005
  standstill: ~0 (åœ¨å‹•)
  Total negative: ~-0.005

æ·¨çå‹µ: ~12.5 âœ…ï¼ˆå¼·æ­£å‘ï¼‰
```

**vs v3ï¼ˆéåº¦æ‡²ç½°ï¼‰**ï¼š
```
v3 é…ç½®ï¼š
  positive: ~12.5
  negative: standstill(4.0) + anti_idle(2.0) + time(0.01) â‰ˆ -6
  æ·¨çå‹µ: ~6.5ï¼ˆæ­£å‘è¢«å£“åˆ¶ï¼‰
```

---

## ğŸ›¡ï¸ PCCBF æ•´åˆ

### PCCBF æ¦‚å¿µ

**PCCBF** = **Perception-aware Control Barrier Functions**ï¼ˆæ„ŸçŸ¥æ„ŸçŸ¥æ§åˆ¶éšœç¤™å‡½æ•¸ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. **é æ¸¬å®‰å…¨ç´„æŸ**ï¼šé æ¸¬æœªä¾† N æ­¥çš„éšœç¤™ç‰©åˆ†å¸ƒ
2. **è»Ÿç´„æŸ**ï¼šä¸æ˜¯ç¡¬æ€§é˜»æ­¢å‹•ä½œï¼Œè€Œæ˜¯é€šéçå‹µå¼•å°
3. **æ„ŸçŸ¥èåˆ**ï¼šçµåˆ LiDAR å’Œé‹å‹•æ¨¡å‹

---

### ç•¶å‰å¯¦ä½œç‹€æ…‹

**âš ï¸ æ³¨æ„**ï¼šv4 é…ç½®ä¸­ PCCBF **æœªå®Œå…¨å•Ÿç”¨**

**å·²å¯¦ä½œ**ï¼š
- âœ… é æ¸¬éšœç¤™ç‰©è·é›¢å‡½æ•¸ï¼ˆ`predicted_obstacle_distances`ï¼‰
- âœ… PCCBF å®‰å…¨çå‹µå‡½æ•¸ï¼ˆ`pccbf_safety_reward`ï¼‰
- âœ… PCCBF é•åæ‡²ç½°å‡½æ•¸ï¼ˆ`pccbf_violation_penalty`ï¼‰

**æœªå•Ÿç”¨**ï¼š
- âŒ æœªåŠ å…¥è§€æ¸¬ç©ºé–“ï¼ˆåªç”¨ç•¶å‰ LiDARï¼‰
- âŒ æœªåŠ å…¥çå‹µé…ç½®ï¼ˆåªç”¨åŸºæœ¬çå‹µï¼‰

---

### PCCBF çå‹µå‡½æ•¸ï¼ˆä»£ç¢¼ä¸­å·²å¯¦ä½œï¼‰

```python
def pccbf_safety_reward(
    env,
    sensor_cfg,
    prediction_horizon: int = 3,
    safe_distance: float = 1.0,
) -> torch.Tensor:
    """PCCBF å•Ÿç™¼çš„å®‰å…¨çå‹µ
    
    åŸç†ï¼š
    - é æ¸¬æœªä¾† N æ­¥çš„éšœç¤™ç‰©åˆ†å¸ƒ
    - å¦‚æœé æ¸¬è·¯å¾‘å®‰å…¨ï¼ˆè·é›¢ > safe_distanceï¼‰â†’ æ­£çå‹µ
    - å¦‚æœé æ¸¬è·¯å¾‘å±éšª â†’ è² æ‡²ç½°
    """
    # é æ¸¬æœªä¾†éšœç¤™ç‰©è·é›¢
    pred_distances = predicted_obstacle_distances(
        env, sensor_cfg, prediction_horizon
    )
    
    # è¨ˆç®—å®‰å…¨è£•åº¦
    safety_margin = pred_distances - safe_distance
    
    # å®‰å…¨ï¼šæ­£çå‹µï¼›å±éšªï¼šè² æ‡²ç½°
    reward = torch.tanh(safety_margin)
    
    return reward
```

---

### å¦‚ä½•å®Œæ•´å•Ÿç”¨ PCCBF

**æ­¥é©Ÿ 1ï¼šåŠ å…¥è§€æ¸¬**
```python
@configclass
class ObservationsCfg:
    @configclass
    class PolicyCfg(ObsGroup):
        # ... ç¾æœ‰è§€æ¸¬ ...
        
        # æ–°å¢ï¼šPCCBF é æ¸¬
        predicted_obstacles = ObsTerm(
            func=mdp.predicted_obstacle_distances,
            params={
                "sensor_cfg": SceneEntityCfg("lidar"),
                "prediction_horizon": 3,
            },
        )
```

**æ­¥é©Ÿ 2ï¼šåŠ å…¥çå‹µ**
```python
@configclass
class RewardsCfg:
    # ... ç¾æœ‰çå‹µ ...
    
    # æ–°å¢ï¼šPCCBF å®‰å…¨çå‹µ
    pccbf_safety = RewTerm(
        func=mdp.pccbf_safety_reward,
        weight=5.0,
        params={
            "sensor_cfg": SceneEntityCfg("lidar"),
            "prediction_horizon": 3,
            "safe_distance": 1.0,
        },
    )
```

**æ­¥é©Ÿ 3ï¼šèª¿æ•´ç¶²è·¯**
```python
# è§€æ¸¬ç¶­åº¦å¢åŠ 
# 369 â†’ 369 + (prediction_horizon * num_angles)
# 369 â†’ 369 + (3 * 360) = 1449

policy = RslRlPpoActorCriticCfg(
    actor_hidden_dims=[512, 512, 256],  # å¢å¤§ç¶²è·¯
    critic_hidden_dims=[512, 512, 256],
)
```

---

## âš™ï¸ è¨“ç·´åƒæ•¸

### PPO ç®—æ³•åƒæ•¸

```python
algorithm = RslRlPpoAlgorithmCfg(
    # æå¤±å‡½æ•¸ä¿‚æ•¸
    value_loss_coef=1.0,              # Value loss æ¬Šé‡
    use_clipped_value_loss=True,      # ä½¿ç”¨ Clipped Value Loss
    
    # PPO æ ¸å¿ƒåƒæ•¸
    clip_param=0.1,                   # PPO clip ç¯„åœï¼ˆv4 é™ä½ç©©å®šï¼‰
    entropy_coef=0.001,               # ç†µä¿‚æ•¸ï¼ˆv4 é™ä½æ”¶æ–‚ï¼‰
    
    # å­¸ç¿’è¨­ç½®
    num_learning_epochs=3,            # æ¯æ¬¡æ›´æ–°çš„ epoch æ•¸ï¼ˆv4 é™ä½ï¼‰
    num_mini_batches=4,               # Mini-batch æ•¸é‡
    learning_rate=3e-4,               # å­¸ç¿’ç‡ï¼ˆv4 é™ä½ï¼‰
    schedule="adaptive",              # è‡ªé©æ‡‰å­¸ç¿’ç‡
    
    # æŠ˜æ‰£å› å­
    gamma=0.99,                       # æŠ˜æ‰£å› å­
    lam=0.95,                         # GAE lambda
    
    # ç´„æŸ
    desired_kl=0.01,                  # ç›®æ¨™ KL æ•£åº¦
    max_grad_norm=1.0,                # æ¢¯åº¦è£å‰ª
)
```

---

### è¨“ç·´æµç¨‹åƒæ•¸

```python
runner = LocalPlannerPPORunnerCfg(
    # åŸºæœ¬è¨­ç½®
    seed=42,                          # éš¨æ©Ÿç¨®å­
    device="cuda:0",                  # è¨“ç·´è¨­å‚™
    
    # æ•¸æ“šæ”¶é›†
    num_steps_per_env=24,             # æ¯å€‹ç’°å¢ƒæ”¶é›† 24 æ­¥
    # ç¸½æ¨£æœ¬æ•¸ = num_envs(24) Ã— num_steps_per_env(24) = 576
    
    # è¨“ç·´é•·åº¦
    max_iterations=10000,             # æœ€å¤§è¿­ä»£æ•¸ï¼ˆv4ï¼‰
    save_interval=100,                # æ¯ 100 iter ä¿å­˜
    
    # æ—¥èªŒ
    experiment_name="local_planner_carter",
    run_name="v4_balanced_penalties",
    logger="wandb",
    wandb_project="nova-carter-navigation",
)
```

---

### ç’°å¢ƒåƒæ•¸

```python
env_cfg = LocalPlannerEnvCfgMin(
    # å ´æ™¯è¨­ç½®
    scene=LocalPlannerSceneCfgMin(
        num_envs=24,                  # ä¸¦è¡Œç’°å¢ƒæ•¸
        env_spacing=10.0,             # ç’°å¢ƒé–“éš” 10m
    ),
    
    # æ™‚é–“è¨­ç½®
    decimation=2,                     # æ§åˆ¶é »ç‡ä¸‹æ¡æ¨£
    episode_length_s=30.0,            # Episode é•·åº¦ 30 ç§’
    sim.dt=0.01,                      # æ¨¡æ“¬æ™‚é–“æ­¥ 0.01s
    sim.render_interval=2,            # æ¸²æŸ“é–“éš”ï¼ˆdecimationï¼‰
    
    # è¨­å‚™
    sim.device="cuda:0",
)
```

**æ™‚é–“è¨ˆç®—**ï¼š
```
ç‰©ç†æ™‚é–“æ­¥ dt = 0.01s
æ§åˆ¶é »ç‡ = 1 / (dt Ã— decimation) = 1 / 0.02 = 50 Hz
Episode æ­¥æ•¸ = episode_length_s / (dt Ã— decimation) = 30 / 0.02 = 1500 æ­¥
```

---

### ç›®æ¨™ç”Ÿæˆåƒæ•¸

```python
goal_command = UniformPoseCommandCfg(
    resampling_time_range=(10.0, 10.0),  # æ¯ 10 ç§’é‡æ–°ç”Ÿæˆç›®æ¨™
    ranges=Ranges(
        pos_x=(2.0, 6.0),                # X ç¯„åœ 2-6m
        pos_y=(-3.0, 3.0),               # Y ç¯„åœ -3-3m
        pos_z=(0.0, 0.0),                # Z å›ºå®šç‚º 0
        yaw=(0.0, 0.0),                  # ä¸é™åˆ¶æœå‘
    ),
)
```

---

## ğŸ“Š å®Œæ•´æ•¸æ“šæµ

```
è¨“ç·´å¾ªç’°ï¼ˆæ¯ Iterationï¼‰:

1. æ•¸æ“šæ”¶é›† (Rollout)
   â”œâ”€ 24 å€‹ç’°å¢ƒä¸¦è¡Œ
   â”œâ”€ æ¯å€‹ç’°å¢ƒæ”¶é›† 24 æ­¥
   â””â”€ ç¸½å…± 576 å€‹ (s, a, r, s') æ¨£æœ¬

2. å„ªå‹¢ä¼°è¨ˆ (GAE)
   â”œâ”€ ä½¿ç”¨ Critic ä¼°è¨ˆ V(s)
   â”œâ”€ è¨ˆç®— TD error
   â””â”€ è¨ˆç®— Advantage A(s,a)

3. PPO æ›´æ–°
   â”œâ”€ 3 å€‹ Epoch
   â”œâ”€ æ¯å€‹ Epoch åˆ† 4 å€‹ Mini-batch
   â”‚   â”œâ”€ Mini-batch size = 576 / 4 = 144
   â”‚   â”œâ”€ Actor loss (PPO clip)
   â”‚   â”œâ”€ Critic loss (Value prediction)
   â”‚   â””â”€ Entropy loss (æ¢ç´¢)
   â””â”€ æ¢¯åº¦è£å‰ª + Adam å„ªåŒ–

4. è¨˜éŒ„ & ä¿å­˜
   â”œâ”€ WandB è¨˜éŒ„æ›²ç·š
   â”œâ”€ æ¯ 100 iter ä¿å­˜æ¨¡å‹
   â””â”€ æ›´æ–°å­¸ç¿’ç‡ï¼ˆAdaptiveï¼‰
```

---

## ğŸ¯ é—œéµè¨­è¨ˆæ±ºç­–

### 1ï¸âƒ£ ç‚ºä»€éº¼ç”¨ PPO è€Œä¸æ˜¯ TD3ï¼Ÿ

| é …ç›® | PPO | TD3 | é¸æ“‡ |
|------|-----|-----|------|
| **è¨“ç·´ç©©å®šæ€§** | âœ… é«˜ï¼ˆOn-Policy + Clipï¼‰ | âš ï¸ ä¸­ï¼ˆOff-Policyï¼‰ | PPO |
| **æ¨£æœ¬æ•ˆç‡** | âš ï¸ ä½ï¼ˆç”¨å®Œå³ä¸Ÿï¼‰ | âœ… é«˜ï¼ˆReplay Bufferï¼‰ | TD3 |
| **ä¸¦è¡Œè¨“ç·´** | âœ… å¤©ç”Ÿæ”¯æŒ | âš ï¸ éœ€è¦é¡å¤–è¨­è¨ˆ | PPO |
| **å¯¦ä½œè¤‡é›œåº¦** | âœ… ç°¡å–® | âš ï¸ è¤‡é›œ | PPO |

**çµè«–**ï¼šPPO æ›´é©åˆ Isaac Lab çš„ä¸¦è¡Œç’°å¢ƒæ¶æ§‹

---

### 2ï¸âƒ£ ç‚ºä»€éº¼è§€æ¸¬ç©ºé–“é€™éº¼å¤§ï¼ˆ369 ç¶­ï¼‰ï¼Ÿ

**LiDAR 360 ç¶­**ï¼š
- å„ªé»ï¼šå®Œæ•´çš„ 360Â° æ„ŸçŸ¥
- ç¼ºé»ï¼šç¶­åº¦é«˜ï¼Œè¨“ç·´æ…¢
- æ›¿ä»£æ–¹æ¡ˆï¼šé™æ¡æ¨£åˆ° 72 ç¶­ï¼ˆæ¯ 5Â°ï¼‰

**ç•¶å‰ä¿ç•™åŸå› **ï¼š
- é«˜è§£æåº¦é¿éšœ
- è«–æ–‡ä¸­ä½¿ç”¨ç›¸åŒè¨­ç½®
- GPU è¨“ç·´å¯è™•ç†

---

### 3ï¸âƒ£ ç‚ºä»€éº¼ v4 å¤§å¹…é™ä½æ‡²ç½°æ¬Šé‡ï¼Ÿ

**v3 å•é¡Œ**ï¼š
```
é€²åº¦çå‹µï¼š+6.0
æ‡²ç½°ç¸½å’Œï¼š-6.0
æ·¨çå‹µï¼šâ‰ˆ0ï¼ˆäº’ç›¸æŠµæ¶ˆï¼‰
çµæœï¼šAgent è¢«å£“åˆ¶ï¼Œä¸æ•¢å‹•
```

**v4 ä¿®æ­£**ï¼š
```
é€²åº¦çå‹µï¼š+6.0
æ‡²ç½°ç¸½å’Œï¼š-0.5
æ·¨çå‹µï¼š+5.5ï¼ˆæ­£å‘ä¸»å°ï¼‰
çµæœï¼šAgent æ•¢æ–¼æ¢ç´¢
```

---

### 4ï¸âƒ£ ç‚ºä»€éº¼ PCCBF æœªå®Œå…¨å•Ÿç”¨ï¼Ÿ

**åŸå› **ï¼š
1. **è¤‡é›œåº¦**ï¼šé æ¸¬å¢åŠ è§€æ¸¬ç¶­åº¦å’Œè¨ˆç®—é‡
2. **éšæ®µæ€§**ï¼šå…ˆé©—è­‰åŸºæœ¬å°èˆªï¼Œå†åŠ å®‰å…¨ç´„æŸ
3. **èª¿è©¦å‹å¥½**ï¼šç°¡å–®çå‹µæ›´æ˜“åˆ†æ

**æœªä¾†è¨ˆåŠƒ**ï¼š
- v5ï¼šæ¥µç°¡çå‹µï¼ˆTD3 é¢¨æ ¼ï¼‰
- v6ï¼šèª²ç¨‹å­¸ç¿’
- v7ï¼šå®Œæ•´ PCCBF æ•´åˆ

---

## ğŸ“š ç¸½çµ

### æ ¸å¿ƒè¨­è¨ˆ

1. **State**ï¼š360Â° LiDAR + é€Ÿåº¦ + ç›®æ¨™è³‡è¨Šï¼ˆ369 ç¶­ï¼‰
2. **Action**ï¼šå·®é€Ÿé©…å‹•ï¼ˆç·šé€Ÿåº¦ + è§’é€Ÿåº¦ï¼Œ2 ç¶­ï¼‰
3. **Agent**ï¼š3 å±¤ MLPï¼ˆ256-256-128ï¼‰
4. **Reward**ï¼š8 é …ï¼ˆæ­£å‘ä¸»å°ï¼Œæ‡²ç½°ç‚ºè¼”ï¼‰

### é—œéµç‰¹é»

- âœ… ä¸¦è¡Œè¨“ç·´ï¼ˆ24 ç’°å¢ƒï¼‰
- âœ… é«˜é »æ§åˆ¶ï¼ˆ50 Hzï¼‰
- âœ… WandB è¨˜éŒ„
- âœ… æ¨¡å¡ŠåŒ–è¨­è¨ˆï¼ˆæ˜“æ–¼èª¿æ•´ï¼‰
- âš ï¸ PCCBF é ç•™ä½†æœªå•Ÿç”¨

### å„ªåŒ–æ–¹å‘

1. **çŸ­æœŸ**ï¼ˆv5ï¼‰ï¼šæ¥µç°¡çå‹µï¼ˆ3-4 é …ï¼‰
2. **ä¸­æœŸ**ï¼ˆv6-v7ï¼‰ï¼šèª²ç¨‹å­¸ç¿’ + ç’°å¢ƒéš¨æ©ŸåŒ–
3. **é•·æœŸ**ï¼ˆv8-v9ï¼‰ï¼šå®Œæ•´ PCCBF æ•´åˆ

---

**é€™å°±æ˜¯åŸå§‹ PCCBF è¨“ç·´çš„å®Œæ•´è¨­è¨ˆï¼** ğŸ¯

