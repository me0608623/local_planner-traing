# 📊 當前訓練參數與架構總覽

> **版本**：v2（修正 Reward Hacking 後）  
> **更新日期**：2025-10-30  
> **配置檔案**：`local_planner_env_cfg_min.py` + `rsl_rl_ppo_cfg.py`

---

## 🎯 環境配置

### 基本設定
```python
# 來源：local_planner_env_cfg_min.py

num_envs: 48                    # 並行環境數（指令可覆蓋為 24）
env_spacing: 10.0               # 環境間距（米）
episode_length_s: 30.0          # 每回合 30 秒
decimation: 2                   # 每 2 個物理步執行 1 次 RL 步
sim.dt: 0.01                    # 物理時間步 10ms
sim.device: "cuda:0"            # GPU 設備
```

**實際控制頻率**：
- 物理模擬：100 Hz（每 0.01 秒）
- RL 控制：50 Hz（每 0.02 秒，decimation=2）
- 每回合步數：30s ÷ 0.02s = **1500 步**

### 場景組件
```python
# 保留組件（最小環境）
✅ Terrain: 平面地形（無障礙物）
✅ Robot: Nova Carter（差速驅動）
   - USD: /home/aa/isaacsim/usd/nova_carter.usd
   - 驅動輪：joint_wheel_left、joint_wheel_right
✅ LiDAR: 360° 2D 掃描
   - 範圍：10 米
   - 解析度：每度 1 條射線（360 條）
   - mesh_prim_paths: ["/World/ground"]
✅ Goal: 隨機目標點
   - 範圍：x [2.0, 6.0]、y [-3.0, 3.0]
   - 重新生成：每 10 秒
   - 可視化：開啟（debug_vis=True）

# 移除組件（簡化訓練）
❌ 靜態障礙物
❌ 動態障礙物
❌ 事件系統
```

---

## 👁️ 觀測空間（369 維）

### 組成
```python
# 來源：local_planner_env_cfg_min.py + mdp/observations.py

1. LiDAR 距離 [360 維]
   - 函數：mdp.lidar_obs()
   - 範圍：[0, 1]（標準化）
   - API：ray_hits_w（Isaac Sim 5.0）

2. 機器人線速度 [3 維]
   - 函數：mdp.base_lin_vel()
   - 內容：[vx, vy, vz] 機器人座標系

3. 機器人角速度 [3 維]
   - 函數：mdp.base_ang_vel()
   - 內容：[wx, wy, wz] 機器人座標系

4. 目標相對位置 [2 維]
   - 函數：mdp.goal_position_in_robot_frame()
   - 內容：[dx, dy] 機器人座標系
   - 轉換：世界座標 → 機器人座標（quat_apply_inverse）

5. 目標距離 [1 維]
   - 函數：mdp.distance_to_goal()
   - 內容：歐幾里得距離（2D，忽略 z）

總計：360 + 3 + 3 + 2 + 1 = 369 維
```

---

## 🎮 動作空間（2 維）

### 組成
```python
# 來源：local_planner_env_cfg_min.py

1. 線速度指令 [1 維]
   - 範圍：[-0.8, +0.8] m/s（v2 降低，原為 2.0）
   - 正值 = 前進，負值 = 後退

2. 角速度指令 [1 維]
   - 範圍：[-0.8, +0.8] rad/s（v2 降低，原為 π）
   - 正值 = 左轉，負值 = 右轉

總計：2 維連續動作
```

### 差速驅動轉換
```python
# 來源：mdp/actions.py（DifferentialDriveActionCfg）

輪距（wheel_base）: 0.413 m
輪半徑（wheel_radius）: 0.125 m

左輪速度 = 線速度 - 角速度 × 輪距/2
右輪速度 = 線速度 + 角速度 × 輪距/2
```

---

## 🎁 獎勵函數（8 項）

### v2 配置（修正版）
```python
# 來源：local_planner_env_cfg_min.py

1. progress_to_goal
   - 函數：mdp.progress_to_goal_reward()
   - 權重：30.0
   - 計算：上一步距離 - 當前距離
   - 作用：鼓勵接近目標（主要驅動力）

2. near_goal_shaping
   - 函數：mdp.near_goal_shaping()
   - 權重：10.0
   - 參數：radius=1.5m
   - 計算：max(0, (1.5 - distance) / 1.5)
   - 作用：近距離額外獎勵

3. heading_alignment（條件式）
   - 函數：mdp.heading_alignment_reward()
   - 權重：1.0
   - 參數：v_min=0.1 m/s
   - 條件：速度 > 0.1 m/s 且 cos(heading) > 0
   - 作用：前進時鼓勵朝向正確

4. reached_goal
   - 函數：mdp.reached_goal_reward()
   - 權重：200.0
   - 參數：threshold=0.8m
   - 計算：distance < 0.8m → 1.0，否則 0
   - 作用：成功獎勵

5. standstill_penalty
   - 函數：mdp.standstill_penalty()
   - 權重：4.0（函數返回負值）
   - 計算：-exp(-10.0 × speed)
   - 作用：懲罰靜止

6. anti_idle
   - 函數：mdp.anti_idle_penalty()
   - 權重：2.0（函數返回負值）
   - 參數：v_threshold=0.05 m/s
   - 計算：速度 < 0.05 → -1.0
   - 作用：強制移動

7. spin_penalty
   - 函數：mdp.spin_penalty()
   - 權重：0.5（函數返回負值）
   - 參數：w_threshold=0.5, v_threshold=0.1
   - 計算：高角速度 + 低線速度 → 負值
   - 作用：抑制原地旋轉

8. time_penalty
   - 函數：mdp.time_penalty()
   - 權重：0.02
   - 計算：每步 -1.0（由權重縮放）
   - 作用：時間成本（每步 -0.02，累積 -30）
```

### 當前結果（v2, 5000 iter）
```
progress_to_goal: 0.0425（✅ 正值，但太小）
near_goal_shaping: 0.0000（從未進入 1.5m）
heading_alignment: 0.3446（✅ 大幅降低）
reached_goal: 0.0000（未成功）
standstill_penalty: -0.0057（✅ 符號正確，但太小）
anti_idle: 0.0000（Agent 有在動）
spin_penalty: 0.0000（沒有原地旋轉）
time_penalty: -0.0200（✅ 正常）

Mean Reward: -22.86（正常範圍）
```

---

## 🧠 神經網路架構

### Actor-Critic 配置
```python
# 來源：agents/rsl_rl_ppo_cfg.py

Actor Network（策略網路）:
  輸入: 觀測 [369 維]
    ↓
  FC Layer 1: [369] → [256]
    ↓ ELU 激活
  FC Layer 2: [256] → [256]
    ↓ ELU 激活
  FC Layer 3: [256] → [128]
    ↓ ELU 激活
  輸出: 動作均值 [2 維] + log_std
    ↓
  採樣: N(mean, std)

Critic Network（價值網路）:
  輸入: 觀測 [369 維]
    ↓
  FC Layer 1: [369] → [256]
    ↓ ELU 激活
  FC Layer 2: [256] → [256]
    ↓ ELU 激活
  FC Layer 3: [256] → [128]
    ↓ ELU 激活
  輸出: State Value [1 維]

總參數量: ~250K-300K
```

### 網路參數
```python
actor_hidden_dims: [256, 256, 128]
critic_hidden_dims: [256, 256, 128]
activation: "elu"
init_noise_std: 0.5（v2 降低，原為 1.0）
```

---

## ⚙️ PPO 超參數

### 演算法配置
```python
# 來源：agents/rsl_rl_ppo_cfg.py

learning_rate: 3e-4              # 學習率（0.0003）
clip_param: 0.1                  # PPO clip 範圍（v2 降低）
entropy_coef: 0.001              # 探索獎勵係數（v2 降低）
value_loss_coef: 1.0             # Value loss 權重
use_clipped_value_loss: True     # 使用 clipped value loss

num_learning_epochs: 3           # 每輪更新 3 個 epoch（v2 降低）
num_mini_batches: 4              # 每個 epoch 4 個 mini-batch

gamma: 0.99                      # 折扣因子
lam: 0.95                        # GAE lambda
desired_kl: 0.01                 # 目標 KL 散度
max_grad_norm: 1.0               # 梯度裁剪

schedule: "adaptive"             # 自適應學習率
```

### 訓練循環配置
```python
# 來源：agents/rsl_rl_ppo_cfg.py

num_steps_per_env: 24            # 每個環境收集 24 步
max_iterations: 5000             # 總迭代次數（指令設定）
save_interval: 100               # 每 100 次保存模型
seed: 42                         # 隨機種子
device: "cuda:0"                 # 訓練設備
```

### 訓練總量（24 envs × 5000 iter）
```
每輪收集步數: 24 envs × 24 steps = 576 步
總訓練步數: 576 × 5000 = 2,880,000 步
實際時間: 約 1.5 小時（RTX 3090/4090）
```

---

## 🏁 終止條件

### 配置
```python
# 來源：local_planner_env_cfg_min.py

1. time_out（超時）
   - 條件：episode 時間 > 30 秒
   - 標記：time_out=True（不算失敗，只是結束）

2. goal_reached（成功）
   - 條件：distance < 0.8m
   - 標記：成功終止

# 注意：最小環境無碰撞終止（沒有障礙物）
```

---

## 📈 完整參數對照表

### 環境參數
| 參數 | 值 | 位置 |
|------|-----|------|
| num_envs | 48（訓練時用 24）| env_cfg_min.py L228 |
| env_spacing | 10.0 m | env_cfg_min.py L228 |
| episode_length_s | 30.0 s | env_cfg_min.py L232 |
| decimation | 2 | env_cfg_min.py L231 |
| sim.dt | 0.01 s | env_cfg_min.py L233 |
| control_frequency | 50 Hz | 計算值 |
| steps_per_episode | 1500 | 計算值 |

### 動作參數
| 參數 | 值 | 位置 |
|------|-----|------|
| max_linear_speed | 0.8 m/s | env_cfg_min.py L106 |
| max_angular_speed | 0.8 rad/s | env_cfg_min.py L107 |
| wheel_radius | 0.125 m | env_cfg_min.py L104 |
| wheel_base | 0.413 m | env_cfg_min.py L105 |

### 目標參數
| 參數 | 值 | 位置 |
|------|-----|------|
| goal_x_range | [2.0, 6.0] m | env_cfg_min.py L142 |
| goal_y_range | [-3.0, 3.0] m | env_cfg_min.py L143 |
| goal_threshold | 0.8 m | env_cfg_min.py L179 |
| resampling_time | 10.0 s | env_cfg_min.py L137 |

### LiDAR 參數
| 參數 | 值 | 位置 |
|------|-----|------|
| channels | 1（2D）| env_cfg_min.py L80 |
| horizontal_fov | [-180°, 180°] | env_cfg_min.py L82 |
| horizontal_res | 1.0°（360 條射線）| env_cfg_min.py L83 |
| max_distance | 10.0 m | env_cfg_min.py L85 |
| prim_path | Robot/.../base_link | env_cfg_min.py L77 |

---

## 🎁 獎勵權重（v2）

### 詳細配置
```python
# 來源：local_planner_env_cfg_min.py L171-208

progress_to_goal:
  - weight: 30.0
  - 函數返回：距離差值（正=接近，負=遠離）
  - 預期範圍：[-1, +1]（每步接近 0-1 米）
  - 累積貢獻：30 × 0.04 = 1.2/episode

near_goal_shaping:
  - weight: 10.0
  - 函數返回：[0, 1]（距離越近越高）
  - 生效範圍：< 1.5m
  - v2 結果：0（從未進入）

heading_alignment（條件式）:
  - weight: 1.0
  - 函數返回：[-1, 1]（cos(heading_error)）
  - 條件：速度 > 0.1 m/s 且 cos > 0
  - v2 結果：0.34

reached_goal:
  - weight: 200.0
  - 函數返回：{0, 1}（二元）
  - 觸發條件：< 0.8m
  - v2 結果：0（未觸發）

standstill_penalty:
  - weight: 4.0
  - 函數返回：[-1, 0]（負值）
  - 計算：-exp(-10 × speed)
  - v2 結果：-0.0057（太小）

anti_idle:
  - weight: 2.0
  - 函數返回：{-1, 0}（二元）
  - 觸發條件：速度 < 0.05 m/s
  - v2 結果：0（Agent 有在動）

spin_penalty:
  - weight: 0.5
  - 函數返回：[-inf, 0]（負值）
  - 觸發條件：角速度 > 0.5 且線速度 < 0.1
  - v2 結果：0（無原地旋轉）

time_penalty:
  - weight: 0.02
  - 函數返回：-1.0（常數）
  - 每步貢獻：-0.02
  - 累積/episode：-0.02 × 1500 = -30.0
  - v2 結果：-0.02（正常）
```

### v2 實際獎勵組成（5000 iter）
```
Mean Reward: -22.86 = 
  + progress_to_goal × 30.0 = 0.0425 × 30 = 1.275
  + heading_alignment × 1.0 = 0.3446 × 1 = 0.345
  - standstill × 4.0 = -0.0057 × 4 = -0.023
  - time_penalty × 0.02 = -1.0 × 0.02 × 1500步 ≈ -30.0
  + 其他 ≈ 0
  
  總計 ≈ 1.28 + 0.35 - 0.02 - 30 = -28.39（接近 -22.86）
```

**觀察**：time_penalty 主導了總獎勵（-30），壓倒其他項

---

## 🧠 PPO 超參數（v2 穩定版）

### 核心參數
```python
# 來源：agents/rsl_rl_ppo_cfg.py

learning_rate: 3e-4              # 0.0003（穩定）
clip_param: 0.1                  # PPO clip（v2 降低）
entropy_coef: 0.001              # 探索係數（v2 降低 10 倍）
value_loss_coef: 1.0             # Value loss 權重

num_learning_epochs: 3           # 每輪 epoch 數（v2 降低）
num_mini_batches: 4              # Mini-batch 數量
num_steps_per_env: 24            # 每環境收集步數

gamma: 0.99                      # 折扣因子
lam: 0.95                        # GAE lambda
desired_kl: 0.01                 # 目標 KL 散度
max_grad_norm: 1.0               # 梯度裁剪

init_noise_std: 0.5              # 初始動作噪音（v2 降低）
schedule: "adaptive"             # 自適應學習率
```

### Batch Size 計算
```
Rollout Buffer Size = num_envs × num_steps_per_env
                    = 24 × 24 = 576 步

Mini-batch Size = 576 ÷ 4 = 144 步

每輪總更新次數 = num_epochs × num_mini_batches
                = 3 × 4 = 12 次梯度更新
```

---

## 📊 v2 訓練結果（5000 iterations）

### 性能指標
```
Mean Reward: -22.86              # 正常範圍（主要被 time_penalty 拉低）
Mean Episode Length: 1500.00     # 完整跑完 30 秒
Action Noise Std: 2.07           # 探索噪音（正常範圍）

Success Rate: 0%                 # 仍未成功
Timeout Rate: 100%               # 全部超時
Position Error: 4.19m            # 從 5.15m 改善到 4.19m
```

### 改善對比（v1 → v2）
| 指標 | v1（失敗）| v2（改善）| 變化 |
|------|----------|----------|------|
| Mean Reward | 190.02 | -22.86 | ✅ 回歸正常 |
| Progress | -0.01 | 0.0425 | ✅ 變正 |
| Position Error | 5.15m | 4.19m | ✅ -0.96m |
| Heading | 4.70 | 0.34 | ✅ -4.36 |
| Standstill | +1.90 | -0.0057 | ✅ 符號修正 |
| Success | 0% | 0% | ⚠️ 無改善 |

---

## 🎯 v3 建議調整

### 問題診斷
**Agent 有在動（anti_idle=0），也有前進（progress=0.0425），但前進太慢**

**根因**：
1. Progress 權重（30）相對 Time penalty（累積 -30）抵消
2. 前進誘因不夠強
3. Agent 可能在「緩慢漂移」而非「積極接近」

### 建議方案

#### 方案 A：大幅提升 Progress（推薦）⭐
```python
progress_to_goal: 30.0 → 100.0
time_penalty: 0.02 → 0.01（減輕時間壓力）
```

**預期**：
- Progress 貢獻從 1.2 → 4.0
- 足以抵抗 time_penalty
- Agent 被強力推動接近目標

#### 方案 B：移除 Time Penalty（測試用）
```python
# 暫時註釋掉 time_penalty
# 觀察 Agent 是否能學會接近目標
```

#### 方案 C：提升 Near Goal Shaping 半徑
```python
near_goal_shaping:
  radius: 1.5m → 3.0m（擴大生效範圍）
  weight: 10.0 → 20.0
```

**理由**：當前從未進入 1.5m，擴大到 3m 讓 shaping 更早生效

---

## 📋 當前代碼檔案清單

### 核心檔案（訓練必需）
```
✅ local_planner_env_cfg_min.py      # 最小環境配置（v2）
✅ agents/rsl_rl_ppo_cfg.py          # PPO 配置（v2 穩定化）
✅ mdp/observations.py               # 觀測實現（支援 ray_hits_w）
✅ mdp/actions.py                    # 動作轉換（差速驅動）
✅ mdp/rewards.py                    # 獎勵實現（v2，8 項）
✅ mdp/terminations.py               # 終止條件
✅ __init__.py                       # 環境註冊（3 個環境）
```

### 訓練腳本
```
✅ scripts/reinforcement_learning/rsl_rl/train.py
✅ scripts/reinforcement_learning/rsl_rl/play.py
```

### 文檔
```
✅ md/訓練架構完整說明.md           # 技術文檔
✅ md/指令快速參考.md               # 命令速查
✅ md/AI交接手冊.md                  # 交接手冊
✅ md/v2訓練驗收清單.md              # v2 驗收
✅ md/當前訓練參數總覽.md（本文檔）  # 參數總覽
✅ README.md                         # 專案主文檔
```

---

## 💾 訓練結果位置

### 最新訓練（v2）
```
logs/rsl_rl/local_planner_carter/2025-10-30_14-26-01/
├─ model_0.pt
├─ model_100.pt
├─ ...
├─ model_4900.pt
├─ model_4999.pt
├─ events.out.tfevents.*
└─ params/
```

### 歷史訓練
```
logs/rsl_rl/local_planner_carter/
├─ 2025-10-27_20-56-40/  # v1（10000 iter，Reward Hacking）
├─ 2025-10-30_14-26-01/  # v2（5000 iter，Progress 改善但仍慢）
└─ ...
```

---

## 🚀 下一步行動

### 立即可執行（v3 調整）

**修改配置**：
```python
# local_planner_env_cfg_min.py

progress_to_goal: weight=100.0  # 從 30.0 提升
time_penalty: weight=0.01       # 從 0.02 降低
```

**啟動訓練**：
```bash
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Min-v0 \
    --num_envs 24 \
    --max_iterations 5000 \
    --headless
```

**驗收目標（v3）**：
- Progress to Goal: 0.04 → **> 0.2**
- Position Error: 4.19m → **< 3.0m**
- Success Rate: 0% → **> 5%**

---

**所有當前參數已完整列出！** 📊

您想要：
1. 套用 v3 調整並重新訓練？
2. 先嘗試其他參數組合？
3. 查看更詳細的某個部分？
