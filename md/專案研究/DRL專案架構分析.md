# ğŸ¤– DRL-robot-navigation å°ˆæ¡ˆæ¶æ§‹åˆ†æ

> **ç‰ˆæœ¬**ï¼šv1.0  
> **åˆ†ææ—¥æœŸ**ï¼š2025-10-30  
> **å°ˆæ¡ˆä¾†æº**ï¼š[GitHub - reiniscimurs/DRL-robot-navigation](https://github.com/reiniscimurs/DRL-robot-navigation)  
> **è«–æ–‡**ï¼šICRA 2022 & IEEE RA-L - "Goal-Driven Autonomous Exploration Through Deep Reinforcement Learning"

---

## ğŸ“‹ ç›®éŒ„

1. [å°ˆæ¡ˆæ¦‚è¿°](#å°ˆæ¡ˆæ¦‚è¿°)
2. [æ ¸å¿ƒæ¶æ§‹](#æ ¸å¿ƒæ¶æ§‹)
3. [è¨“ç·´æµç¨‹](#è¨“ç·´æµç¨‹)
4. [èˆ‡ç•¶å‰ç³»çµ±å°æ¯”](#èˆ‡ç•¶å‰ç³»çµ±å°æ¯”)
5. [æ•´åˆç­–ç•¥](#æ•´åˆç­–ç•¥)
6. [å¯¦ä½œå»ºè­°](#å¯¦ä½œå»ºè­°)

---

## ğŸ“– å°ˆæ¡ˆæ¦‚è¿°

### ğŸ¯ å°ˆæ¡ˆç›®æ¨™

ä½¿ç”¨ **TD3 (Twin Delayed Deep Deterministic Policy Gradient)** ç®—æ³•ï¼Œè¨“ç·´ç§»å‹•æ©Ÿå™¨äººåœ¨æ¨¡æ“¬ç’°å¢ƒä¸­è‡ªä¸»å°èˆªåˆ°éš¨æ©Ÿç›®æ¨™é»ï¼ŒåŒæ™‚é¿éšœã€‚

### ğŸ› ï¸ æŠ€è¡“æ£§

| çµ„ä»¶ | æŠ€è¡“ | ç•¶å‰å°ˆæ¡ˆ |
|------|------|----------|
| **æ¨¡æ“¬å™¨** | ROS Gazebo | Isaac Sim |
| **RL ç®—æ³•** | TD3ï¼ˆActor-Criticï¼‰ | PPOï¼ˆActor-Criticï¼‰ |
| **æ„Ÿæ¸¬å™¨** | 3D Velodyne LiDAR | 3D LiDAR |
| **æ¡†æ¶** | PyTorchï¼ˆè‡ªå®šç¾©ï¼‰ | RSL-RLï¼ˆPPO åº«ï¼‰ |
| **è¨˜éŒ„å·¥å…·** | TensorBoard | TensorBoard / WandB |
| **é€šè¨Š** | ROS Topics | Isaac Lab API |
| **æ©Ÿå™¨äºº** | è‡ªå®šç¾©ï¼ˆr1ï¼‰ | Nova Carter |

### ğŸ“Š è¨“ç·´åƒæ•¸

```python
max_timesteps = 5e6        # 500 è¬æ­¥ï¼ˆvs ç•¶å‰ 240kï¼‰
buffer_size = 1e6          # 100 è¬ç¶“é©—ï¼ˆReplay Bufferï¼‰
batch_size = 40            # å°æ‰¹æ¬¡ï¼ˆvs ç•¶å‰ 576ï¼‰
discount = 0.99999         # å¹¾ä¹ç„¡æŠ˜æ‰£ï¼ˆvs 0.99ï¼‰
expl_noise = 1 â†’ 0.1       # æ¢ç´¢å™ªéŸ³è¡°æ¸›
learning_rate = Adam é è¨­   # æœªæ˜ç¢ºæŒ‡å®šï¼ˆvs 3e-4ï¼‰
```

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ§‹

### 1ï¸âƒ£ ç’°å¢ƒè¨­è¨ˆï¼ˆ`velodyne_env.py`ï¼‰

#### ğŸ“¡ è§€æ¸¬ç©ºé–“ï¼ˆState Spaceï¼‰

**ç¶­åº¦**ï¼š`24` = `20 (LiDAR) + 4 (Robot State)`

```python
state = [
    # 1. LiDAR æƒæï¼ˆ20 å€‹è§’åº¦ç¯„åœçš„æœ€å°è·é›¢ï¼‰
    velodyne_data[0:20],     # æ¯å€‹è§’åº¦ç¯„åœçš„æœ€å°éšœç¤™è·é›¢
    
    # 2. æ©Ÿå™¨äººç‹€æ…‹ï¼ˆ4 ç¶­ï¼‰
    distance,                # åˆ°ç›®æ¨™çš„è·é›¢ï¼ˆæ­æ°è·é›¢ï¼‰
    theta,                   # ç›¸å°ç›®æ¨™çš„è§’åº¦ï¼ˆ-Ï€ åˆ° Ï€ï¼‰
    action[0],               # ä¸Šä¸€æ­¥çš„ç·šé€Ÿåº¦
    action[1]                # ä¸Šä¸€æ­¥çš„è§’é€Ÿåº¦
]
```

**LiDAR è™•ç†é‚è¼¯**ï¼š
- æƒæç¯„åœï¼š`-Ï€/2` åˆ° `Ï€/2`ï¼ˆ180Â°å‰æ–¹ï¼‰
- åˆ†æˆ 20 å€‹è§’åº¦ç¯„åœï¼ˆæ¯å€‹ 9Â°ï¼‰
- éæ¿¾é«˜åº¦ > -0.2m çš„é»ï¼ˆå¿½ç•¥åœ°é¢ï¼‰
- æ¯å€‹ç¯„åœå–æœ€å°è·é›¢ï¼ˆæœ€è¿‘éšœç¤™ç‰©ï¼‰
- åˆå§‹å€¼ï¼š10mï¼ˆç„¡éšœç¤™ç‰©ï¼‰

#### ğŸ® å‹•ä½œç©ºé–“ï¼ˆAction Spaceï¼‰

**ç¶­åº¦**ï¼š`2` = `[linear_velocity, angular_velocity]`

```python
action = [
    linear_velocity,   # ç·šé€Ÿåº¦ï¼š[0, 1]ï¼ˆç¶“éè½‰æ›ï¼‰
    angular_velocity   # è§’é€Ÿåº¦ï¼š[-1, 1]
]

# å¯¦éš›åŸ·è¡Œæ™‚çš„è½‰æ›
a_in = [(action[0] + 1) / 2, action[1]]
```

**å‹•ä½œç‰¹æ€§**ï¼š
- è¼¸å‡ºç¯„åœï¼š`[-1, 1]`ï¼ˆActor ä½¿ç”¨ Tanh æ¿€æ´»ï¼‰
- ç·šé€Ÿåº¦è½‰æ›ï¼šå¾ `[-1, 1]` â†’ `[0, 1]`
- è§’é€Ÿåº¦ç›´æ¥ä½¿ç”¨

#### ğŸ çå‹µå‡½æ•¸ï¼ˆReward Functionï¼‰

```python
def get_reward(target, collision, action, min_laser):
    if target:
        return 100.0           # âœ… åˆ°é”ç›®æ¨™
    elif collision:
        return -100.0          # âŒ ç¢°æ’
    else:
        # æ­£å¸¸ç§»å‹•çå‹µ
        r3 = lambda x: 1 - x if x < 1 else 0.0
        return action[0] / 2           # é¼“å‹µå‰é€²ï¼ˆ+0.5 maxï¼‰
               - abs(action[1]) / 2    # æ‡²ç½°æ—‹è½‰ï¼ˆ-0.5 maxï¼‰
               - r3(min_laser) / 2     # æ‡²ç½°é è¿‘éšœç¤™ï¼ˆ-0.5 maxï¼‰
```

**çå‹µè¨­è¨ˆç‰¹é»**ï¼š
- âœ… **æ¥µç°¡è¨­è¨ˆ**ï¼šåªæœ‰ 3 é …ï¼ˆvs ç•¶å‰ 8 é …ï¼‰
- âœ… **ç¨€ç– + å¯†é›†çµåˆ**ï¼š
  - ç¨€ç–ï¼šç›®æ¨™ +100ã€ç¢°æ’ -100
  - å¯†é›†ï¼šé€Ÿåº¦é¼“å‹µã€æ—‹è½‰æ‡²ç½°ã€é¿éšœæ‡²ç½°
- âœ… **ç„¡æ™‚é–“æ‡²ç½°**ï¼šä¸æ‡²ç½°è€—æ™‚ï¼ˆdiscount æ¥è¿‘ 1ï¼‰

**èˆ‡ç•¶å‰ç³»çµ±å°æ¯”**ï¼š

| é …ç›® | DRL-robot-navigation | ç•¶å‰ç³»çµ±ï¼ˆv4ï¼‰ |
|------|---------------------|--------------|
| çå‹µé …æ•¸é‡ | 3 | 8 |
| Progress | `action[0] / 2` | `weight=60.0` |
| é¿éšœ | `-r3(min_laser) / 2` | ç„¡ç›´æ¥çå‹µ |
| æ—‹è½‰ | `-abs(action[1]) / 2` | `spin_penalty=0.1` |
| æ™‚é–“ | ç„¡ | `time_penalty=0.005` |
| æœå‘ | ç„¡ | `heading_alignment=1.0` |
| éœæ­¢ | ç„¡ | `standstill=1.0` |

#### ğŸ”„ çµ‚æ­¢æ¢ä»¶ï¼ˆTerminationï¼‰

```python
# 1. åˆ°é”ç›®æ¨™
if distance < GOAL_REACHED_DIST:  # 0.3m
    target = True
    done = True

# 2. ç¢°æ’
if min_laser < COLLISION_DIST:  # 0.35m
    collision = True
    done = True

# 3. è¶…æ™‚ï¼ˆè¨“ç·´è…³æœ¬ä¸­ï¼‰
if episode_timesteps >= max_ep:  # 500 æ­¥
    done = True
```

#### ğŸ¯ ç›®æ¨™ç”Ÿæˆï¼ˆGoal Generationï¼‰

```python
def change_goal(self):
    # èª²ç¨‹å­¸ç¿’ï¼šé€æ¼¸æ“´å¤§ç›®æ¨™ç¯„åœ
    if self.upper < 10:
        self.upper += 0.004     # å¾ 5.0 æ…¢æ…¢å¢åŠ 
    if self.lower > -10:
        self.lower -= 0.004     # å¾ -5.0 æ…¢æ…¢æ¸›å°‘
    
    # éš¨æ©Ÿç”Ÿæˆç›®æ¨™ï¼ˆç›¸å°æ–¼æ©Ÿå™¨äººç•¶å‰ä½ç½®ï¼‰
    self.goal_x = self.odom_x + random.uniform(self.upper, self.lower)
    self.goal_y = self.odom_y + random.uniform(self.upper, self.lower)
    
    # ç¢ºä¿ç›®æ¨™ä¸åœ¨éšœç¤™ç‰©ä¸Š
    goal_ok = check_pos(self.goal_x, self.goal_y)
```

**èª²ç¨‹å­¸ç¿’ç­–ç•¥**ï¼š
- åˆå§‹ç¯„åœï¼š`[-5, 5]`ï¼ˆ10m Ã— 10mï¼‰
- æœ€çµ‚ç¯„åœï¼š`[-10, 10]`ï¼ˆ20m Ã— 20mï¼‰
- æ¯æ¬¡ reset å¢åŠ  0.004ï¼ˆç´„ 10000 æ¬¡å¾Œé”åˆ°æœ€å¤§ï¼‰

#### ğŸ§± å‹•æ…‹éšœç¤™ç‰©ï¼ˆRandomizationï¼‰

```python
def random_box(self):
    # æ¯æ¬¡ reset éš¨æ©Ÿç§»å‹• 4 å€‹ç®±å­
    for i in range(4):
        x = np.random.uniform(-6, 6)
        y = np.random.uniform(-6, 6)
        # ç¢ºä¿ç®±å­ï¼š
        # 1. ä¸åœ¨å›ºå®šéšœç¤™ç‰©ä¸Š
        # 2. è·é›¢æ©Ÿå™¨äºº > 1.5m
        # 3. è·é›¢ç›®æ¨™ > 1.5m
```

**ç’°å¢ƒéš¨æ©ŸåŒ–**ï¼š
- æ¯å€‹ episode é‡ç½®æ©Ÿå™¨äººä½ç½®ï¼ˆéš¨æ©Ÿï¼‰
- æ¯å€‹ episode é‡ç½®æ©Ÿå™¨äººæœå‘ï¼ˆéš¨æ©Ÿï¼‰
- æ¯å€‹ episode é‡ç½®ç›®æ¨™ä½ç½®ï¼ˆèª²ç¨‹å­¸ç¿’ç¯„åœå…§ï¼‰
- æ¯å€‹ episode é‡ç½® 4 å€‹å‹•æ…‹ç®±å­ä½ç½®

---

### 2ï¸âƒ£ TD3 ç®—æ³•ï¼ˆ`train_velodyne_td3.py`ï¼‰

#### ğŸ›ï¸ ç¶²è·¯æ¶æ§‹

**Actor ç¶²è·¯**ï¼ˆç­–ç•¥ç¶²è·¯ï¼‰ï¼š
```python
Input: state (24)
  â†“
Layer 1: Linear(24, 800) + ReLU
  â†“
Layer 2: Linear(800, 600) + ReLU
  â†“
Layer 3: Linear(600, 2) + Tanh
  â†“
Output: action (2) in [-1, 1]
```

**Critic ç¶²è·¯**ï¼ˆé›™ Q ç¶²è·¯ï¼‰ï¼š
```python
# Q1 ç¶²è·¯
Input: state (24)
  â†“
Layer 1: Linear(24, 800) + ReLU
  â†“
Layer 2_s: Linear(800, 600)  â†â”€â”
Action: (2)                     â”‚ ç›¸åŠ 
  â†“                             â”‚
Layer 2_a: Linear(2, 600)  â”€â”€â”€â”€â”€â”˜ + ReLU
  â†“
Layer 3: Linear(600, 1)
  â†“
Output: Q1 value

# Q2 ç¶²è·¯ï¼ˆç›¸åŒæ¶æ§‹ï¼‰
...
Output: Q2 value
```

**åƒæ•¸é‡å°æ¯”**ï¼š

| ç¶²è·¯ | DRL-robot-navigation | ç•¶å‰ç³»çµ±ï¼ˆPPOï¼‰ |
|------|---------------------|----------------|
| Actor | 24â†’800â†’600â†’2 â‰ˆ 500k | 24â†’256â†’256â†’128â†’2 â‰ˆ 100k |
| Critic | 24â†’800â†’600â†’1 Ã— 2 â‰ˆ 1M | 24â†’256â†’256â†’128â†’1 â‰ˆ 100k |
| **ç¸½è¨ˆ** | **â‰ˆ 1.5M** | **â‰ˆ 200k** |

**é—œéµå·®ç•°**ï¼š
- TD3 ä½¿ç”¨æ›´å¤§çš„ç¶²è·¯ï¼ˆ800, 600 vs 256, 256, 128ï¼‰
- TD3 ä½¿ç”¨é›™ Criticï¼ˆæ¸›å°‘ Q å€¼éä¼°ï¼‰
- PPO ä½¿ç”¨å–® Criticï¼ˆValue Functionï¼‰

#### ğŸ”„ è¨“ç·´å¾ªç’°

```python
# ä¸»è¨“ç·´å¾ªç’°ï¼ˆ5M æ­¥ï¼‰
while timestep < max_timesteps:
    if done:
        # Episode çµæŸæ™‚è¨“ç·´
        network.train(replay_buffer, episode_timesteps, ...)
        
        # æ¯ 5000 æ­¥è©•ä¼°ä¸€æ¬¡
        if timesteps_since_eval >= eval_freq:
            evaluations.append(evaluate(...))
            network.save(...)
    
    # å‹•ä½œé¸æ“‡ï¼ˆåŠ å™ªéŸ³æ¢ç´¢ï¼‰
    action = network.get_action(state)
    action += np.random.normal(0, expl_noise, size=2)
    
    # è¿‘éšœç¤™ç‰©ç‰¹æ®Šç­–ç•¥
    if random_near_obstacle and min(state[4:-8]) < 0.6:
        if np.random.uniform(0, 1) > 0.85:
            # å¼·åˆ¶éš¨æ©Ÿå¾Œé€€
            action = random_action
            action[0] = -1
    
    # åŸ·è¡Œå‹•ä½œ
    next_state, reward, done, target = env.step(action)
    
    # å„²å­˜ç¶“é©—
    replay_buffer.add(state, action, reward, done, next_state)
```

#### ğŸ“ TD3 æ›´æ–°æ©Ÿåˆ¶

```python
def train(replay_buffer, iterations, batch_size=100, ...):
    for it in range(iterations):
        # 1. å¾ Replay Buffer æ¡æ¨£
        states, actions, rewards, dones, next_states = replay_buffer.sample_batch(batch_size)
        
        # 2. è¨ˆç®— Target Qï¼ˆä½¿ç”¨ Target Networksï¼‰
        next_action = actor_target(next_states)
        next_action += noise.clamp(-noise_clip, noise_clip)  # Smoothing
        target_Q1, target_Q2 = critic_target(next_states, next_action)
        target_Q = min(target_Q1, target_Q2)  # å–æœ€å°ï¼ˆæ¸›å°‘éä¼°ï¼‰
        target_Q = rewards + discount * target_Q
        
        # 3. æ›´æ–° Critic
        current_Q1, current_Q2 = critic(states, actions)
        loss = MSE(current_Q1, target_Q) + MSE(current_Q2, target_Q)
        critic_optimizer.step()
        
        # 4. å»¶é²æ›´æ–° Actorï¼ˆæ¯ 2 æ­¥ï¼‰
        if it % policy_freq == 0:
            actor_loss = -critic(states, actor(states)).mean()  # Maximize Q
            actor_optimizer.step()
            
            # 5. Soft Update Target Networks
            target_params = Ï„ * params + (1 - Ï„) * target_params
```

**TD3 æ ¸å¿ƒæŠ€å·§**ï¼š
1. **é›™ Critic**ï¼šæ¸›å°‘ Q å€¼éä¼°ï¼ˆå– minï¼‰
2. **Delayed Policy Update**ï¼šActor æ›´æ–°é »ç‡ < Criticï¼ˆé¿å…ä¸ç©©å®šï¼‰
3. **Target Policy Smoothing**ï¼šçµ¦ target action åŠ å™ªéŸ³ï¼ˆæ­£å‰‡åŒ–ï¼‰
4. **Soft Target Update**ï¼šç›®æ¨™ç¶²è·¯ç·©æ…¢è·Ÿéš¨ï¼ˆÏ„=0.005ï¼‰

**vs PPO**ï¼š
- PPOï¼šOn-Policyï¼ˆä½¿ç”¨ç•¶å‰ç­–ç•¥çš„æ•¸æ“šï¼‰
- TD3ï¼šOff-Policyï¼ˆä½¿ç”¨ Replay Buffer çš„èˆŠæ•¸æ“šï¼‰
- PPOï¼šç­–ç•¥æ¢¯åº¦ + Clipping
- TD3ï¼šQ-Learning + Actor-Critic

---

### 3ï¸âƒ£ ç¶“é©—å›æ”¾ï¼ˆ`replay_buffer.py`ï¼‰

```python
class ReplayBuffer:
    def __init__(self, buffer_size=1e6):
        self.buffer = deque(maxlen=buffer_size)
    
    def add(self, state, action, reward, done, next_state):
        self.buffer.append((state, action, reward, done, next_state))
    
    def sample_batch(self, batch_size=40):
        batch = random.sample(self.buffer, batch_size)
        return states, actions, rewards, dones, next_states
```

**é—œéµç‰¹æ€§**ï¼š
- **å®¹é‡**ï¼š100 è¬ç­†ç¶“é©—ï¼ˆvs PPO ç„¡ Bufferï¼Œç›´æ¥ç”¨å®Œå³ä¸Ÿï¼‰
- **æ¡æ¨£**ï¼šéš¨æ©Ÿæ¡æ¨£ï¼ˆæ‰“ç ´æ™‚é–“ç›¸é—œæ€§ï¼‰
- **è³‡æ–™åˆ©ç”¨ç‡**ï¼šé«˜ï¼ˆå¯é‡è¤‡ä½¿ç”¨èˆŠç¶“é©—ï¼‰

**vs PPO**ï¼š
- PPOï¼šæ”¶é›† `num_envs Ã— num_steps_per_env` æ­¥å¾Œç«‹å³è¨“ç·´ä¸¦ä¸Ÿæ£„
- TD3ï¼šæŒçºŒç´¯ç©ç¶“é©—ï¼Œéš¨æ©Ÿæ¡æ¨£è¨“ç·´ï¼ˆæ›´ç©©å®šä½†éœ€æ›´å¤šè¨˜æ†¶é«”ï¼‰

---

### 4ï¸âƒ£ æ¢ç´¢ç­–ç•¥

#### ğŸ“‰ å™ªéŸ³è¡°æ¸›

```python
expl_noise = 1.0           # åˆå§‹ï¼ˆ100% æ¢ç´¢ï¼‰
expl_min = 0.1             # æœ€çµ‚ï¼ˆ10% æ¢ç´¢ï¼‰
expl_decay_steps = 500000  # è¡°æ¸›é€±æœŸ

# æ¯æ­¥è¡°æ¸›
expl_noise -= (1 - expl_min) / expl_decay_steps
```

**è¡°æ¸›æ›²ç·š**ï¼š
```
Step 0:      expl_noise = 1.0
Step 250k:   expl_noise = 0.55
Step 500k:   expl_noise = 0.1
Step 500k+:  expl_noise = 0.1ï¼ˆä¿æŒï¼‰
```

#### ğŸš§ è¿‘éšœç¤™ç‰©ç­–ç•¥ï¼ˆRandom Near Obstacleï¼‰

```python
if random_near_obstacle:
    # å¦‚æœé›·å°„æƒæ < 0.6m ä¸” 15% æ©Ÿç‡
    if np.random.uniform(0, 1) > 0.85 and min(state[4:-8]) < 0.6:
        # å¼·åˆ¶å¾Œé€€ 8-15 æ­¥
        count_rand_actions = np.random.randint(8, 15)
        random_action = np.random.uniform(-1, 1, 2)
        action = random_action
        action[0] = -1  # å¼·åˆ¶å¾Œé€€
```

**ç›®çš„**ï¼š
- å¢åŠ è¿‘éšœç¤™ç‰©çš„æ¢ç´¢
- é¿å… Agent å­¸æœƒã€Œå¡åœ¨ç‰†é‚Šã€
- æ‰“ç ´å±€éƒ¨æœ€å„ªï¼ˆé¡ä¼¼ Epsilon-Greedyï¼‰

---

## ğŸ“Š èˆ‡ç•¶å‰ç³»çµ±å°æ¯”

### å®Œæ•´å°æ¯”è¡¨

| é …ç›® | DRL-robot-navigation | ç•¶å‰å°ˆæ¡ˆï¼ˆNova Carterï¼‰ |
|------|---------------------|------------------------|
| **ç®—æ³•** | TD3ï¼ˆOff-Policyï¼‰ | PPOï¼ˆOn-Policyï¼‰ |
| **æ¨¡æ“¬å™¨** | ROS Gazebo | Isaac Sim |
| **æ¡†æ¶** | PyTorch è‡ªå®šç¾© | RSL-RL åº« |
| **è§€æ¸¬ç¶­åº¦** | 24 | 548ï¼ˆ20 LiDAR + 528 å…¶ä»–ï¼‰ |
| **å‹•ä½œç¶­åº¦** | 2 | 2 |
| **çå‹µé …æ•¸é‡** | 3ï¼ˆæ¥µç°¡ï¼‰ | 8ï¼ˆè¤‡é›œï¼‰ |
| **ç¶²è·¯å¤§å°** | 1.5M åƒæ•¸ | 200k åƒæ•¸ |
| **Batch Size** | 40 | 576 (24 envs Ã— 24 steps) |
| **è¨“ç·´æ­¥æ•¸** | 5M | 240kï¼ˆ10000 iterï¼‰ |
| **Replay Buffer** | 1M ç¶“é©— | ç„¡ï¼ˆOn-Policyï¼‰ |
| **æ¢ç´¢ç­–ç•¥** | å™ªéŸ³è¡°æ¸› + è¿‘éšœç¤™éš¨æ©Ÿ | å‹•ä½œå™ªéŸ³ï¼ˆå›ºå®š stdï¼‰ |
| **èª²ç¨‹å­¸ç¿’** | ç›®æ¨™è·é›¢é€æ¼¸å¢åŠ  | å›ºå®šç¯„åœ |
| **ç’°å¢ƒéš¨æ©ŸåŒ–** | å‹•æ…‹ç®±å­ + æ©Ÿå™¨äººä½ç½® | å›ºå®šå ´æ™¯ |
| **æ™‚é–“æ‡²ç½°** | ç„¡ | æœ‰ï¼ˆ0.005ï¼‰ |
| **ä¸¦è¡Œç’°å¢ƒ** | 1 | 24 |
| **è©•ä¼°é »ç‡** | æ¯ 5000 æ­¥ | æ¯ 100 iter |

---

## ğŸ”„ æ•´åˆç­–ç•¥

### æ–¹æ¡ˆ Aï¼šç›´æ¥ç§»æ¤ TD3 åˆ° Isaac Lab

**ç›®æ¨™**ï¼šç”¨ TD3 æ›¿æ› PPOï¼Œä¿æŒ Isaac Lab ç’°å¢ƒ

#### å„ªé»
- âœ… Off-Policy å­¸ç¿’ï¼ˆæ›´ç©©å®šï¼‰
- âœ… æ›´å¥½çš„æ¨£æœ¬æ•ˆç‡ï¼ˆReplay Bufferï¼‰
- âœ… é©åˆé€£çºŒå‹•ä½œç©ºé–“ï¼ˆvs PPOï¼‰
- âœ… è«–æ–‡å¯¦è­‰æˆåŠŸï¼ˆICRA 2022ï¼‰

#### ç¼ºé»
- âŒ éœ€è¦å¤§é‡è¨˜æ†¶é«”ï¼ˆ1M Bufferï¼‰
- âŒ å–®ç’°å¢ƒè¨“ç·´ï¼ˆvs ç•¶å‰ 24 ä¸¦è¡Œï¼‰
- âŒ è¨“ç·´æ™‚é–“æ›´é•·ï¼ˆ5M vs 240k æ­¥ï¼‰

#### å¯¦ä½œæ­¥é©Ÿ

```python
# 1. å‰µå»º TD3 Agent
td3_agent = TD3Agent(
    state_dim=24,          # 20 LiDAR + 4 robot state
    action_dim=2,
    actor_hidden=[800, 600],
    critic_hidden=[800, 600],
    max_action=1.0,
)

# 2. ç°¡åŒ–è§€æ¸¬ç©ºé–“ï¼ˆç•¶å‰ 548 â†’ 24ï¼‰
@configclass
class ObservationsCfg:
    policy = ObsTerm(func=observe_state)
    
    def observe_state(env):
        # LiDAR: 20 è§’åº¦ç¯„åœçš„æœ€å°è·é›¢
        lidar_min_distances = process_lidar(env.scene["lidar"])
        
        # Robot state
        goal_distance = compute_distance_to_goal(env)
        goal_theta = compute_relative_angle_to_goal(env)
        last_linear_vel = env.action_manager.prev_actions[:, 0]
        last_angular_vel = env.action_manager.prev_actions[:, 1]
        
        return torch.cat([
            lidar_min_distances,    # (20,)
            goal_distance,          # (1,)
            goal_theta,             # (1,)
            last_linear_vel,        # (1,)
            last_angular_vel,       # (1,)
        ], dim=-1)  # Total: 24

# 3. ç°¡åŒ–çå‹µå‡½æ•¸ï¼ˆ8 é … â†’ 3 é …ï¼‰
@configclass
class RewardsCfg:
    # 1. å‰é€²é¼“å‹µ
    forward_velocity = RewTerm(
        func=lambda env: env.action_manager.action[:, 0] / 2,
        weight=1.0,
    )
    
    # 2. æ—‹è½‰æ‡²ç½°
    angular_penalty = RewTerm(
        func=lambda env: -torch.abs(env.action_manager.action[:, 1]) / 2,
        weight=1.0,
    )
    
    # 3. é¿éšœæ‡²ç½°
    obstacle_penalty = RewTerm(
        func=lambda env: -(1 - torch.min(env.scene["lidar"].data.ray_hits_w, dim=-1).values).clamp(0, 1) / 2,
        weight=1.0,
    )
    
    # ç¨€ç–çå‹µ
    reached_goal = RewTerm(
        func=mdp.reached_goal_reward,
        weight=100.0,
    )
    
    collision = RewTerm(
        func=mdp.collision_penalty,
        weight=-100.0,
    )

# 4. TD3 è¨“ç·´å¾ªç’°
replay_buffer = ReplayBuffer(1000000)

for timestep in range(5_000_000):
    if done:
        state = env.reset()
    
    # é¸æ“‡å‹•ä½œï¼ˆåŠ å™ªéŸ³ï¼‰
    action = td3_agent.get_action(state)
    action += np.random.normal(0, expl_noise, size=2)
    
    # åŸ·è¡Œ
    next_state, reward, done, info = env.step(action)
    
    # å„²å­˜ç¶“é©—
    replay_buffer.add(state, action, reward, done, next_state)
    
    # è¨“ç·´ï¼ˆå¾ Buffer æ¡æ¨£ï¼‰
    if replay_buffer.size() > batch_size:
        td3_agent.train(replay_buffer, iterations=episode_timesteps)
```

---

### æ–¹æ¡ˆ Bï¼šå€Ÿé‘‘ TD3 è¨­è¨ˆæ”¹é€² PPO

**ç›®æ¨™**ï¼šä¿æŒ PPO ç®—æ³•ï¼Œä½†å€Ÿé‘‘ TD3 çš„æˆåŠŸç¶“é©—

#### å¯å€Ÿé‘‘çš„è¨­è¨ˆ

**1. æ¥µç°¡çå‹µå‡½æ•¸**
```python
# å¾ 8 é …æ¸›å°‘åˆ° 3-4 é …
@configclass
class RewardsCfg:
    # æ ¸å¿ƒï¼šåªä¿ç•™æœ€é‡è¦çš„
    progress = RewTerm(weight=1.0)         # å‰é€²
    rotation = RewTerm(weight=-0.5)        # æ—‹è½‰æ‡²ç½°
    obstacle = RewTerm(weight=-0.5)        # é¿éšœ
    reached_goal = RewTerm(weight=100.0)   # æˆåŠŸ
    # åˆªé™¤ï¼šstandstill, anti_idle, time, heading, near_goal
```

**2. èª²ç¨‹å­¸ç¿’**
```python
@configclass
class CommandsCfg:
    goal_command = UniformVelocityCommand2dCfg(
        resampling_time_range=(5.0, 5.0),
        ranges=GoalCommandRanges(
            # åˆå§‹ç¯„åœï¼š5m
            pos_x=(self.curriculum_distance, self.curriculum_distance),
            pos_y=(self.curriculum_distance, self.curriculum_distance),
        ),
    )
    
    # æ¯ 10000 iter å¢åŠ  0.5m
    def update_curriculum(self, iteration):
        if iteration % 10000 == 0:
            self.curriculum_distance = min(self.curriculum_distance + 0.5, 10.0)
```

**3. ç’°å¢ƒéš¨æ©ŸåŒ–**
```python
@configclass
class LocalPlannerSceneCfg:
    # æ¯æ¬¡ reset éš¨æ©Ÿç§»å‹•éšœç¤™ç‰©
    dynamic_obstacles = AssetBaseCfg(
        prim_path="/World/Obstacles",
        spawn=sim_utils.CuboidCfg(size=(0.5, 0.5, 0.5)),
        init_state=AssetInitialStateCfg(
            pos=RandomUniformDistribution(min=(-5, -5, 0), max=(5, 5, 0)),
        ),
    )
```

**4. è¿‘éšœç¤™ç‰©æ¢ç´¢**
```python
def compute_actions(self, obs):
    actions = self.policy(obs)
    
    # å¦‚æœé›·å°„æƒæ < 0.6mï¼Œ15% æ©Ÿç‡å¼·åˆ¶å¾Œé€€
    min_lidar = torch.min(obs[:, :20], dim=-1).values
    near_obstacle = min_lidar < 0.6
    force_backward = torch.rand(len(obs)) > 0.85
    
    actions[near_obstacle & force_backward, 0] = -1.0  # å¾Œé€€
    
    return actions
```

**5. æ›´å¤§çš„ç¶²è·¯**
```python
policy = RslRlPpoActorCriticCfg(
    actor_hidden_dims=[800, 600],     # vs ç•¶å‰ [256, 256, 128]
    critic_hidden_dims=[800, 600],    # vs ç•¶å‰ [256, 256, 128]
    activation="relu",                # vs ç•¶å‰ "elu"
)
```

---

### æ–¹æ¡ˆ Cï¼šæ··åˆæ¶æ§‹ï¼ˆæœ€æ¨è–¦ï¼‰

**æ ¸å¿ƒç†å¿µ**ï¼šTD3 çš„ç’°å¢ƒè¨­è¨ˆ + PPO çš„è¨“ç·´æ•ˆç‡

#### æ··åˆé…ç½®

```python
# ç’°å¢ƒè¨­è¨ˆï¼ˆå€Ÿé‘‘ TD3ï¼‰
1. è§€æ¸¬ç©ºé–“ï¼š24 ç¶­ï¼ˆ20 LiDAR + 4 robotï¼‰
2. çå‹µå‡½æ•¸ï¼š3-4 é …ï¼ˆæ¥µç°¡ï¼‰
3. èª²ç¨‹å­¸ç¿’ï¼šç›®æ¨™è·é›¢é€æ¼¸å¢åŠ 
4. ç’°å¢ƒéš¨æ©ŸåŒ–ï¼šå‹•æ…‹éšœç¤™ç‰© + æ©Ÿå™¨äººä½ç½®

# è¨“ç·´ç®—æ³•ï¼ˆä¿æŒ PPOï¼‰
1. ä¸¦è¡Œç’°å¢ƒï¼š24 å€‹ï¼ˆæ•ˆç‡ï¼‰
2. On-Policyï¼šä¸éœ€è¦å¤§ Bufferï¼ˆçœè¨˜æ†¶é«”ï¼‰
3. ç©©å®šæ€§ï¼šPPO Clippingï¼ˆè¨“ç·´ç©©å®šï¼‰
4. æˆç†Ÿå·¥å…·ï¼šRSL-RLï¼ˆæ˜“ç”¨ï¼‰
```

#### å…·é«”å¯¦ä½œ

**éšæ®µ 1ï¼šç°¡åŒ–çå‹µï¼ˆç«‹å³å¯åšï¼‰**
```python
# åˆªé™¤ v4 çš„è¤‡é›œæ‡²ç½°ï¼Œå›æ­¸ TD3 å¼æ¥µç°¡è¨­è¨ˆ
@configclass
class RewardsCfg:
    forward = RewTerm(
        func=lambda env: env.action_manager.action[:, 0] / 2,
        weight=1.0,
    )
    angular = RewTerm(
        func=lambda env: -torch.abs(env.action_manager.action[:, 1]) / 2,
        weight=1.0,
    )
    obstacle = RewTerm(
        func=mdp.obstacle_penalty,  # æ–°å¢ï¼šåŸºæ–¼ min_lidar çš„æ‡²ç½°
        weight=1.0,
    )
    reached_goal = RewTerm(
        func=mdp.reached_goal_reward,
        weight=200.0,  # vs TD3 çš„ 100
    )
```

**éšæ®µ 2ï¼šèª²ç¨‹å­¸ç¿’ï¼ˆä¸‹ä¸€ç‰ˆï¼‰**
```python
# åœ¨è¨“ç·´è…³æœ¬ä¸­å‹•æ…‹èª¿æ•´ç›®æ¨™ç¯„åœ
class CurriculumManager:
    def __init__(self):
        self.goal_range = 5.0  # åˆå§‹ 5m
    
    def update(self, iteration):
        if iteration % 1000 == 0 and self.goal_range < 10.0:
            self.goal_range += 0.05
            env_cfg.commands.goal_command.ranges.pos_x = (
                -self.goal_range, self.goal_range
            )
```

**éšæ®µ 3ï¼šç’°å¢ƒéš¨æ©ŸåŒ–ï¼ˆé€²éšï¼‰**
```python
# æ¯æ¬¡ reset æ™‚ç§»å‹•éšœç¤™ç‰©
def reset_obstacles(env):
    for i in range(4):
        x = torch.rand(1) * 12 - 6  # [-6, 6]
        y = torch.rand(1) * 12 - 6
        env.scene[f"obstacle_{i}"].set_world_poses(
            positions=torch.tensor([[x, y, 0.25]]),
        )
```

---

## ğŸ’¡ å¯¦ä½œå»ºè­°

### ğŸš€ å¿«é€Ÿé©—è­‰è·¯å¾‘ï¼ˆv5 å»ºè­°ï¼‰

**v5 é…ç½®ï¼šTD3 æ¥µç°¡çå‹µ + PPO è¨“ç·´**

```python
# 1. çå‹µå‡½æ•¸ï¼šå¾ 8 é … â†’ 4 é …
@configclass
class RewardsCfg:
    """v5ï¼šTD3 å•Ÿç™¼çš„æ¥µç°¡è¨­è¨ˆ"""
    
    # å¯†é›†çå‹µï¼ˆTD3 styleï¼‰
    forward_velocity = RewTerm(
        func=lambda env: env.action_manager.action[:, 0] / 2,
        weight=1.0,
    )
    angular_penalty = RewTerm(
        func=lambda env: -torch.abs(env.action_manager.action[:, 1]) / 2,
        weight=1.0,
    )
    obstacle_proximity = RewTerm(
        func=mdp.obstacle_proximity_penalty,  # å¯¦ä½œ TD3 çš„ r3(min_laser)
        weight=1.0,
    )
    
    # ç¨€ç–çå‹µ
    reached_goal = RewTerm(
        func=mdp.reached_goal_reward,
        weight=200.0,
    )
    
    # åˆªé™¤ï¼šprogress, standstill, anti_idle, spin, time, heading, near_goal

# 2. å¯¦ä½œ obstacle_proximity_penalty
def obstacle_proximity_penalty(env) -> torch.Tensor:
    """TD3 å¼é¿éšœæ‡²ç½°ï¼šr3(x) = -(1-x) if x<1 else 0"""
    min_lidar = torch.min(env.scene["lidar"].data.ray_hits_w, dim=-1).values
    penalty = torch.where(
        min_lidar < 1.0,
        -(1.0 - min_lidar) / 2,  # ç¯„åœ [-0.5, 0]
        torch.zeros_like(min_lidar),
    )
    return penalty

# 3. PPO åƒæ•¸ï¼ˆä¿æŒ v4ï¼‰
LocalPlannerPPORunnerCfg(
    learning_rate=3e-4,
    entropy_coef=0.001,
    max_iterations=10000,
    logger="wandb",
    ...
)
```

**ç†è«–é æœŸ**ï¼š
```
æ­£å¸¸ç§»å‹•ï¼ˆå‡è¨­ v=0.5, Ï‰=0.2, min_lidar=2.0ï¼‰ï¼š
  forward = 0.5 / 2 = 0.25
  angular = -0.2 / 2 = -0.1
  obstacle = 0ï¼ˆè·é›¢ > 1mï¼‰
  Total = 0.15 âœ…ï¼ˆé¼“å‹µå‰é€²ï¼‰

é è¿‘éšœç¤™ï¼ˆv=0.3, Ï‰=0.1, min_lidar=0.5ï¼‰ï¼š
  forward = 0.15
  angular = -0.05
  obstacle = -(1 - 0.5) / 2 = -0.25
  Total = -0.15 âŒï¼ˆæ‡²ç½°é è¿‘ï¼‰

åˆ°é”ç›®æ¨™ï¼š
  + 200 âœ…ï¼ˆå¼·çƒˆæ­£å‘ï¼‰
```

### ğŸ“Š v5 é©—æ”¶æ¨™æº–

| æŒ‡æ¨™ | v4 çµæœ | v5 ç›®æ¨™ | ç†ç”± |
|------|---------|---------|------|
| **è¨“ç·´ç©©å®šæ€§** | - | æ›²ç·šå¹³æ»‘ | æ¥µç°¡çå‹µæ¸›å°‘è¡çª |
| **Forward Reward** | - | > 0.2 | é¼“å‹µå‰é€²ç”Ÿæ•ˆ |
| **Obstacle Penalty** | - | < -0.1 | é¿éšœæ©Ÿåˆ¶ç”Ÿæ•ˆ |
| **Position Error** | 3.84m | < 2.5m | TD3 è«–æ–‡é”åˆ° <1m |
| **Success Rate** | 0% | > 10% | è«–æ–‡æœ€çµ‚ >80% |

### ğŸ› ï¸ é•·æœŸæ•´åˆè¨ˆç•«

**Phase 1ï¼šæ¥µç°¡çå‹µé©—è­‰**ï¼ˆv5 - ç•¶å‰ï¼‰
- âœ… å¯¦ä½œ TD3 å¼çå‹µå‡½æ•¸
- âœ… ç§»é™¤æ‰€æœ‰æ‡²ç½°é …ï¼ˆstandstill, anti_idle, time ç­‰ï¼‰
- ğŸ“Š è¨“ç·´ 10000 iter + WandB è¨˜éŒ„
- ğŸ¯ ç›®æ¨™ï¼šSuccess Rate > 10%

**Phase 2ï¼šèª²ç¨‹å­¸ç¿’**ï¼ˆv6ï¼‰
```python
# ç›®æ¨™è·é›¢å¾ 5m é€æ¼¸å¢åŠ åˆ° 10m
curriculum = CurriculumScheduler(
    initial_range=5.0,
    final_range=10.0,
    update_freq=1000,  # æ¯ 1000 iter å¢åŠ 
    increment=0.05,
)
```

**Phase 3ï¼šç’°å¢ƒéš¨æ©ŸåŒ–**ï¼ˆv7ï¼‰
```python
# å‹•æ…‹éšœç¤™ç‰© + æ©Ÿå™¨äººéš¨æ©Ÿåˆå§‹ä½ç½®
@configclass
class LocalPlannerSceneCfg:
    robot = AssetBaseCfg(
        init_state=AssetInitialStateCfg(
            pos=RandomUniformDistribution((-5, -5, 0), (5, 5, 0)),
            rot=RandomUniformDistribution((0, 0, -Ï€), (0, 0, Ï€)),
        ),
    )
```

**Phase 4ï¼šè¿‘éšœç¤™ç‰©ç­–ç•¥**ï¼ˆv8ï¼‰
```python
# 15% æ©Ÿç‡å¼·åˆ¶å¾Œé€€ï¼ˆç•¶ min_lidar < 0.6mï¼‰
def sample_actions_with_obstacle_strategy(policy_actions, lidar_data):
    ...
```

**Phase 5ï¼šTD3 å®Œæ•´ç§»æ¤**ï¼ˆv9 - å¯é¸ï¼‰
- å¯¦ä½œå®Œæ•´ TD3 ç®—æ³•
- Replay Bufferï¼ˆ1Mï¼‰
- é›™ Critic ç¶²è·¯
- Off-Policy è¨“ç·´

---

## ğŸ“š é—œéµå·®ç•°ç¸½çµ

### å“²å­¸å·®ç•°

| é …ç›® | DRL-robot-navigationï¼ˆTD3ï¼‰ | ç•¶å‰å°ˆæ¡ˆï¼ˆPPOï¼‰ |
|------|---------------------------|----------------|
| **çå‹µå“²å­¸** | æ¥µç°¡ï¼ˆ3 é …ï¼‰ | å¤šå…ƒï¼ˆ8 é …ï¼‰ |
| **å­¸ç¿’ç­–ç•¥** | Off-Policyï¼ˆReplayï¼‰ | On-Policyï¼ˆå³æ™‚ï¼‰ |
| **æ¢ç´¢ç­–ç•¥** | å™ªéŸ³è¡°æ¸› + ç‰¹æ®Šç­–ç•¥ | å›ºå®šå™ªéŸ³ |
| **èª²ç¨‹è¨­è¨ˆ** | ç›®æ¨™è·é›¢é€æ¼¸å¢åŠ  | å›ºå®šç¯„åœ |
| **ç¶²è·¯è¦æ¨¡** | å¤§ï¼ˆ1.5Mï¼‰ | å°ï¼ˆ200kï¼‰ |
| **è¨“ç·´æ­¥æ•¸** | é•·ï¼ˆ5Mï¼‰ | çŸ­ï¼ˆ240kï¼‰ |

### æˆåŠŸè¦ç´ åˆ†æ

**TD3 è«–æ–‡æˆåŠŸçš„é—œéµ**ï¼š
1. âœ… **æ¥µç°¡çå‹µ**ï¼šåªæœ‰ 3 é …ï¼Œé¿å…çå‹µè¡çª
2. âœ… **èª²ç¨‹å­¸ç¿’**ï¼šç›®æ¨™è·é›¢å¾ç°¡åˆ°é›£
3. âœ… **ç’°å¢ƒéš¨æ©ŸåŒ–**ï¼šå‹•æ…‹éšœç¤™ç‰©æ‰“ç ´éæ“¬åˆ
4. âœ… **è¿‘éšœç¤™ç­–ç•¥**ï¼šå¼·åˆ¶æ¢ç´¢å›°é›£å€åŸŸ
5. âœ… **é•·æ™‚é–“è¨“ç·´**ï¼š5M æ­¥ï¼ˆvs ç•¶å‰ 240kï¼‰
6. âœ… **å¤§ç¶²è·¯**ï¼š800-600 éš±è—å±¤

**ç•¶å‰å°ˆæ¡ˆçš„æŒ‘æˆ°**ï¼š
1. âŒ çå‹µéæ–¼è¤‡é›œï¼ˆ8 é …äº’ç›¸è¡çªï¼‰
2. âŒ ç¼ºä¹èª²ç¨‹å­¸ç¿’ï¼ˆç›®æ¨™ç¯„åœå›ºå®šï¼‰
3. âŒ ç’°å¢ƒå›ºå®šï¼ˆç„¡éš¨æ©ŸåŒ–ï¼‰
4. âŒ è¨“ç·´æ™‚é–“è¼ƒçŸ­
5. âš ï¸ ç¶²è·¯è¼ƒå°ï¼ˆå¯èƒ½è¡¨é”èƒ½åŠ›ä¸è¶³ï¼‰

---

## âœ… è¡Œå‹•å»ºè­°

### ç«‹å³å¯åšï¼ˆv5ï¼‰

```bash
# 1. å‰µå»º v5 é…ç½®æ–‡ä»¶
cp local_planner_env_cfg_min.py local_planner_env_cfg_td3_style.py

# 2. å¯¦ä½œ TD3 å¼çå‹µ
# - åˆªé™¤ progress, standstill, anti_idle, spin, time, heading, near_goal
# - æ–°å¢ forward_velocity, angular_penalty, obstacle_proximity
# - ä¿ç•™ reached_goal

# 3. å¯¦ä½œ obstacle_proximity_penalty å‡½æ•¸
# source/isaaclab_tasks/.../mdp/rewards.py

# 4. è¨“ç·´ v5
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-TD3Style-v0 \
    --num_envs 24 \
    --max_iterations 10000 \
    --headless

# 5. å°æ¯” v4 vs v5
# WandB ç›£æ§ï¼šforward, angular, obstacle, success_rate
```

### å¾ŒçºŒè¿­ä»£ï¼ˆv6-v9ï¼‰

1. **v6**ï¼šèª²ç¨‹å­¸ç¿’ï¼ˆç›®æ¨™è·é›¢å‹•æ…‹å¢åŠ ï¼‰
2. **v7**ï¼šç’°å¢ƒéš¨æ©ŸåŒ–ï¼ˆå‹•æ…‹éšœç¤™ç‰©ï¼‰
3. **v8**ï¼šè¿‘éšœç¤™ç­–ç•¥ï¼ˆå¼·åˆ¶æ¢ç´¢ï¼‰
4. **v9**ï¼šTD3 å®Œæ•´ç§»æ¤ï¼ˆå¯é¸ï¼‰

---

## ğŸ“– åƒè€ƒè³‡æ–™

1. **è«–æ–‡**ï¼š[Goal-Driven Autonomous Exploration Through Deep Reinforcement Learning](https://ieeexplore.ieee.org/document/9645287)
2. **GitHub**ï¼š[reiniscimurs/DRL-robot-navigation](https://github.com/reiniscimurs/DRL-robot-navigation)
3. **TD3 åŸè«–æ–‡**ï¼š[Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)
4. **Medium æ•™å­¸**ï¼š[Deep Reinforcement Learning in Mobile Robot Navigation - Tutorial](https://medium.com/@reinis_86651/deep-reinforcement-learning-in-mobile-robot-navigation-tutorial-part1-installation-d62715722303)

---

**ç¸½çµ**ï¼š`DRL-robot-navigation` å°ˆæ¡ˆæä¾›äº†ä¸€å€‹ç¶“éè«–æ–‡é©—è­‰çš„æˆåŠŸæ¶æ§‹ã€‚å»ºè­°å…ˆå€Ÿé‘‘å…¶æ¥µç°¡çå‹µè¨­è¨ˆï¼ˆv5ï¼‰ï¼Œé©—è­‰æ•ˆæœå¾Œå†é€æ­¥æ•´åˆèª²ç¨‹å­¸ç¿’ã€ç’°å¢ƒéš¨æ©ŸåŒ–ç­‰é€²éšæŠ€è¡“ã€‚TD3 ç®—æ³•çš„å®Œæ•´ç§»æ¤å¯ä½œç‚ºé•·æœŸç›®æ¨™ï¼ˆè¨˜æ†¶é«”å’Œæ™‚é–“æˆæœ¬è¼ƒé«˜ï¼‰ã€‚


