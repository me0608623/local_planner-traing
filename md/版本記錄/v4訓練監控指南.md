# 🎮 v4 訓練監控指南

> **訓練版本**：v4（方案 A - 平衡正負獎勵）  
> **啟動時間**：2025-10-30  
> **預計完成**：啟動後 3.5 小時  
> **訓練狀態**：🟢 運行中

---

## 📊 訓練配置摘要

```yaml
任務: Isaac-Navigation-LocalPlanner-Min-v0
算法: PPO (RSL-RL)
環境數: 24 (並行)
最大迭代: 10000
設備: cuda:0
記錄工具: WandB (nova-carter-navigation)

v4 核心調整:
  - standstill: 4.0 → 1.0 (↓75%)
  - anti_idle: 2.0 → 0.5 (↓75%)
  - spin_penalty: 0.5 → 0.1 (↓80%)
  - time_penalty: 0.01 → 0.005 (↓50%)
  - progress: 60.0 (保持)
  - near_goal: 20.0 (保持)
```

---

## 🔍 即時監控方式

### 方法 1️⃣：WandB 雲端監控（最推薦）

**訪問連結**：https://wandb.ai/

**登入資訊**：
- 使用您的 WandB 帳號登入
- 專案名稱：`nova-carter-navigation`
- Run 名稱：`v3_progress60_neargol20_time001`（會自動更新為新 run）

**關鍵曲線**：
```
📈 核心指標（必看）：
  - Episode_Reward/progress_to_goal   ← 必須從負值回正！
  - Episode_Reward/standstill_penalty ← 應該降到 -0.1 ~ -0.3
  - Episode_Reward/anti_idle          ← 應該降到 -0.1 ~ -0.2
  - Metrics/goal_command/position_error ← 應該 < 3.0m
  - Episode_Termination/goal_reached  ← 應該 > 0%
  - Mean reward                       ← 應該 > -10

📊 輔助指標：
  - Episode_Reward/near_goal_shaping  ← 應該 > 0.1
  - Episode_Reward/heading_alignment
  - Episode_Reward/time_penalty
  - Episode_Termination/time_out
```

---

### 方法 2️⃣：終端機即時日誌

```bash
# 即時查看訓練輸出（推薦）
cd /home/aa/IsaacLab
tail -f training_v4.log

# 查看最新 50 行
tail -50 training_v4.log

# 搜尋關鍵資訊
grep "Learning iteration" training_v4.log | tail -5
grep "Mean reward" training_v4.log | tail -5
grep "progress_to_goal" training_v4.log | tail -5
```

**終端機輸出示例**：
```
Learning iteration 1000/10000
Mean reward: -15.32
Episode_Reward/progress_to_goal: 0.0523      ← 關鍵！應該為正
Episode_Reward/standstill_penalty: -0.2134   ← 關鍵！應該降低
Metrics/goal_command/position_error: 2.87    ← 關鍵！應該 < 3m
```

---

### 方法 3️⃣：訓練進度檢查

```bash
# 檢查訓練進程
ps aux | grep train.py | grep -v grep

# 檢查 GPU 使用率
nvidia-smi

# 檢查當前迭代進度
grep -o "Learning iteration [0-9]*/10000" training_v4.log | tail -1

# 檢查當前獎勵值
grep "Mean reward:" training_v4.log | tail -1

# 檢查最新檢查點
ls -lth logs/rsl_rl/local_planner_carter/*/model_*.pt | head -5
```

---

### 方法 4️⃣：本地 TensorBoard（備用）

```bash
# 啟動 TensorBoard（另開終端）
cd /home/aa/IsaacLab
tensorboard --logdir logs/rsl_rl/local_planner_carter

# 瀏覽器訪問
# http://localhost:6006
```

---

## ⏱️ 檢查點時間表

### 🎯 關鍵檢查時間點

| 時間 | 迭代數 | 檢查項目 | 成功標準 |
|------|--------|----------|----------|
| **T+30min** | ~1000 | Progress 是否回正 | > 0 ⭐⭐⭐ |
| **T+30min** | ~1000 | Standstill 是否降低 | > -0.5 |
| **T+1.5h** | ~3000 | Progress 持續為正 | > 0.05 |
| **T+1.5h** | ~3000 | Position Error 改善 | < 3.5m |
| **T+2.5h** | ~5000 | Success Rate 出現 | > 0% |
| **T+3.5h** | ~10000 | 完整評估 | 見下方 |

---

## 📋 1000 iter 快速檢查（T+30min）

**最關鍵的早期指標**：

```bash
# 執行快速檢查
cd /home/aa/IsaacLab
grep "Learning iteration 1000/10000" training_v4.log -A 20
```

**判斷邏輯**：

✅ **v4 成功**（繼續訓練）：
```
progress_to_goal: > 0        ← 必須回正！
standstill: > -0.5           ← 懲罰降低
position_error: < 4.0m
```

⚠️ **v4 部分成功**（觀察至 3000 iter）：
```
progress_to_goal: -0.01 ~ 0  ← 接近零但未回正
standstill: -0.5 ~ -1.0      ← 有改善但不夠
```

❌ **v4 失敗**（停止訓練，啟動 v5）：
```
progress_to_goal: < -0.05    ← 仍然大幅負值
standstill: < -1.0           ← 懲罰仍然過重
```

---

## 📈 完整評估標準（10000 iter）

### ✅ v4 成功標準

**核心指標**（必須同時滿足）：
1. ✅ **Progress > 0.05**（從 v3 的 -0.01 回正）
2. ✅ **Standstill + Anti-idle < -0.5**（v3 是 -1.7）
3. ✅ **Position Error < 3.0m**（從 3.84m 改善）

**次要指標**（達成 2/3）：
4. ⭐ **Success Rate > 0%**（首次成功）
5. ⭐ **Near Goal > 0.1**（開始生效）
6. ⭐ **Mean Reward > -10**（從 -26 改善）

---

### 📊 v4 結果分類

**🎉 完全成功**：
- 核心指標全部達成
- 次要指標至少 2/3
- **下一步**：v5（TD3 極簡獎勵）或課程學習

**⚠️ 部分成功**：
- 核心指標 2/3 達成
- Progress 回正但不夠大（0.01-0.04）
- **下一步**：微調權重或嘗試 v5

**❌ 失敗**：
- Progress 仍為負值
- 懲罰仍然過重
- **下一步**：立即啟動 v5（TD3 極簡獎勵）

---

## 🚨 異常處理

### 訓練中斷

```bash
# 檢查進程是否存在
ps aux | grep train.py

# 如果中斷，查看最後日誌
tail -100 training_v4.log

# 從檢查點恢復（如果有）
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Min-v0 \
    --num_envs 24 \
    --resume \
    --load_run CHECKPOINT_PATH
```

### 訓練過慢

```bash
# 檢查 GPU 使用率
nvidia-smi

# 如果 GPU 使用率低 (<50%)，可能是數據傳輸瓶頸
# 檢查是否有其他進程占用 GPU
```

### WandB 未記錄

```bash
# 檢查 WandB 是否初始化
grep "wandb" training_v4.log | head -5

# 確認 API Key
echo $WANDB_API_KEY

# 手動重新登入（如果需要）
wandb login 015e82155dc7cecb79368e0415eaf09362c260ef
```

---

## 📝 訓練日誌範例

### 正常訓練輸出

```
[INFO]: Launching Isaac Sim Simulator first
[INFO]: Base environment: Environment device : cuda:0
[INFO]: Time taken for scene creation : 0.758 seconds

Learning iteration 0/10000
  Mean reward: -25.34
  Episode_Reward/progress_to_goal: 0.0124      ← 回正了！✅
  Episode_Reward/standstill_penalty: -0.2341   ← 降低了！✅
  Episode_Reward/anti_idle: -0.1523
  Episode_Reward/near_goal_shaping: 0.0532
  Episode_Reward/time_penalty: -0.0100
  Metrics/goal_command/position_error: 3.65    ← 改善中！✅
  Episode_Termination/time_out: 0.9583
  Episode_Termination/goal_reached: 0.0417     ← 出現成功！✅

Learning iteration 100/10000
  Mean reward: -18.27                          ← 持續改善！✅
  ...
```

---

## 🎯 v4 vs v3 對比表

| 指標 | v3（失敗） | v4 目標 | 實際（更新中） |
|------|-----------|---------|--------------|
| Progress | -0.0104 | > 0.05 | ? |
| Standstill | -1.16 | -0.1~-0.3 | ? |
| Anti-idle | -0.53 | -0.1~-0.2 | ? |
| Position Error | 3.84m | < 3.0m | ? |
| Success Rate | 0% | > 0% | ? |
| Mean Reward | -26.49 | > -10 | ? |

**更新方式**：
```bash
# 提取最新結果（10000 iter 完成後）
grep "Learning iteration 9999/10000" training_v4.log -A 20
```

---

## 📞 快速指令參考

```bash
# 1. 查看訓練狀態
tail -f /home/aa/IsaacLab/training_v4.log

# 2. 檢查當前進度
grep "Learning iteration" /home/aa/IsaacLab/training_v4.log | tail -1

# 3. 查看最新獎勵
grep "Mean reward" /home/aa/IsaacLab/training_v4.log | tail -1

# 4. 查看 Progress（關鍵！）
grep "progress_to_goal" /home/aa/IsaacLab/training_v4.log | tail -5

# 5. 查看 Standstill（關鍵！）
grep "standstill" /home/aa/IsaacLab/training_v4.log | tail -5

# 6. 查看 Position Error（關鍵！）
grep "position_error" /home/aa/IsaacLab/training_v4.log | tail -5

# 7. 查看 Success Rate
grep "goal_reached" /home/aa/IsaacLab/training_v4.log | tail -5

# 8. 停止訓練（如果需要）
pkill -f "train.py"
```

---

## 🎬 下一步行動（10000 iter 完成後）

### ✅ 如果 v4 成功
1. 更新 `md/AI交接手冊.md` 的實驗日誌
2. 創建 `md/v4訓練結果報告.md`
3. 決定：
   - 選項 A：v5（TD3 極簡獎勵）
   - 選項 B：課程學習（目標距離逐漸增加）
   - 選項 C：環境隨機化（動態障礙物）

### ⚠️ 如果 v4 部分成功
1. 分析哪些指標達標、哪些未達標
2. 微調權重（例如：再降低 standstill）
3. 重新訓練 v4.1

### ❌ 如果 v4 失敗
1. **立即啟動 v5（TD3 極簡獎勵）**
2. 實作 `obstacle_proximity_penalty` 函數
3. 簡化獎勵從 8 項 → 4 項
4. 參考 `md/DRL專案架構分析.md`

---

**v4 訓練正在後台運行！預計 3.5 小時完成。** 🚀

**監控連結**：https://wandb.ai/

祝訓練順利！ 🎉

