# 🎯 v4 訓練配置 - 平衡正負獎勵

> **版本**：v4（方案 A - 降低懲罰壓制）  
> **日期**：2025-10-30  
> **基於**：v3 失敗分析

---

## ❌ v3 失敗診斷

### 訓練結果（10000 iterations）
- ❌ **Progress: -0.0104**（變負！比 v2 更糟）
- ⚠️ **Position Error: 3.84m**（微改善 0.35m）
- ❌ **Standstill: -1.16**（懲罰暴增，v2 是 -0.006）
- ❌ **Anti-idle: -0.53**（頻繁觸發）
- ❌ **Success: 0%**（無改善）
- ✅ **Heading: 0.17**（持續降低，好）

### 根本問題

**懲罰過重，壓制探索**：

v3 設定：
```
正向獎勵：progress × 60 + near_goal × 20 ≈ 最多 10-15
負向懲罰：standstill × 4 + anti_idle × 2 + time × 0.01 × 1500 ≈ -15 到 -20
```

**結果**：
- Agent 被懲罰「嚇住」
- 不敢大膽移動，只敢小心翼翼
- Progress 從正值（0.04）變成負值（-0.01）
- Standstill 懲罰從 -0.006 暴增到 -1.16

**理論分析**：
- v2：懲罰輕 → Agent 敢動但太慢（progress 0.04）
- v3：懲罰重 → Agent 被壓制不敢動（progress -0.01）
- **需要平衡點**：懲罰適中，既推動又不壓制

---

## ✅ v4 修正策略（方案 A）

### 核心理念

**正向引導為主，懲罰為輔**：
- 保持強大的正向驅動（progress 60、near_goal 20）
- 大幅降低懲罰項（避免壓制探索）
- 讓 Agent 在「獎勵引導」下學習，而非「懲罰威脅」

### 具體調整

```python
# v3 → v4 權重變化

正向獎勵（保持強力）：
  progress_to_goal: 60.0 → 60.0   ✅ 保持
  near_goal_shaping: 20.0 → 20.0  ✅ 保持（radius 3.0m）
  heading_alignment: 1.0 → 1.0    ✅ 保持（條件式）
  reached_goal: 200.0 → 200.0     ✅ 保持

負向懲罰（大幅降低）：
  standstill: 4.0 → 1.0           ↓↓ 降低 75%
  anti_idle: 2.0 → 0.5            ↓↓ 降低 75%
  spin_penalty: 0.5 → 0.1         ↓ 降低 80%
  time_penalty: 0.01 → 0.005      ↓ 降低 50%
```

### 理論預期

**假設 Agent 每步接近 0.1m（保守估計）**：

```
正向獎勵：
  progress × 60 = 0.1 × 60 = 6.0
  near_goal × 20 = 0.2 × 20 = 4.0（假設進入 3m）
  heading × 1 = 0.5 × 1 = 0.5
  
負向懲罰：
  standstill × 1 ≈ -0.02 × 1 = -0.02
  anti_idle × 0.5 ≈ 0（如果有在動）
  spin × 0.1 ≈ 0
  time × 0.005 × 1500 = -7.5

每 episode 總獎勵 ≈ 6 + 4 + 0.5 - 0.02 - 7.5 ≈ 3.0（正值！）
```

**對比 v3**：
```
v3: 正向 ≈ 10-15, 負向 ≈ -15 到 -20 → 相互抵消，甚至負主導
v4: 正向 ≈ 10-15, 負向 ≈ -7 到 -8 → 正向主導 ✅
```

---

## 📊 v4 完整獎勵權重

```python
progress_to_goal: 60.0          # 主要驅動力（保持）
near_goal_shaping: 20.0         # 中距離塑形（保持，radius 3.0m）
heading_alignment: 1.0          # 朝向輔助（保持，條件式）
reached_goal: 200.0             # 成功大獎（保持）

standstill: 1.0                 # ↓↓ v4: 從 4.0 降低
anti_idle: 0.5                  # ↓↓ v4: 從 2.0 降低
spin_penalty: 0.1               # ↓ v4: 從 0.5 降低
time_penalty: 0.005             # ↓ v4: 從 0.01 降低
```

---

## 🎯 v4 驗收標準

### 核心指標（必須改善）

| 指標 | v3 結果 | v4 目標 | 判斷 |
|------|---------|---------|------|
| **Progress** | -0.0104 | **> 0.05** | ⭐⭐⭐ 必須回正 |
| **Position Error** | 3.84m | **< 3.0m** | ⭐⭐⭐ 持續改善 |
| **Standstill** | -1.16 | **> -0.3** | ⭐⭐ 懲罰降低 |
| **Anti-idle** | -0.53 | **> -0.2** | ⭐⭐ 懲罰降低 |
| **Success Rate** | 0% | **> 0%** | ⭐⭐ 出現成功 |
| **Near Goal** | 0 | **> 0.1** | ⭐ 開始生效 |

### 成功判斷邏輯

**✅ v4 成功**（同時滿足）：
1. Progress > 0（回到正值）
2. Standstill + Anti-idle 懲罰總和 > -0.5（v3 是 -1.7）
3. Position Error < 3.0m

**⚠️ v4 部分成功**（滿足 1 或 2）：
- 繼續調整權重比例

**❌ v4 失敗**（都不滿足）：
- 需要方案 B（極簡獎勵）

---

## 📋 v4 訓練計畫

### 訓練指令

```bash
cd /home/aa/IsaacLab

# 確保 WandB API Key 已設定
export WANDB_API_KEY="015e82155dc7cecb79368e0415eaf09362c260ef"

# v4 訓練（10000 iterations）
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Min-v0 \
    --num_envs 24 \
    --max_iterations 10000 \
    --headless
```

### WandB 監控

**專案**：nova-carter-navigation  
**Run 名稱**：v3_progress60_neargol20_time001（會自動更新為新 run）  
**查看**：https://wandb.ai/

### 檢查點評估

**1000 iter**：
- Progress 是否 > 0？
- Standstill 是否 > -0.5？
- 如果否 → 考慮方案 B

**3000 iter**：
- Progress 是否 > 0.05？
- Position Error 是否 < 3.5m？

**5000 iter**：
- Success Rate 是否 > 0？
- Near Goal Shaping 是否 > 0？

**10000 iter**：
- 完整評估
- 決定是否需要 v5 或進入課程學習

---

## 🔬 理論分析

### 獎勵平衡哲學

**v1-v3 的教訓**：
- v1：過度依賴單一獎勵（heading）→ Reward Hacking
- v2：多項獎勵但 progress 太弱 → 前進太慢
- v3：提升 progress 但懲罰過重 → 壓制探索

**v4 設計原則**：
1. **強大的正向引導**：progress 60（主導）+ near_goal 20（輔助）
2. **溫和的負向約束**：懲罰總和 < 正向獎勵的 1/3
3. **條件式輔助獎勵**：heading 只在前進時生效
4. **最小時間成本**：time 0.005（每 episode 只扣 -7.5）

### 數學期望

**理想狀態（Agent 學會前進）**：
```
progress × 60 = 0.15 × 60 = 9.0       （主要來源）
near_goal × 20 = 0.3 × 20 = 6.0       （進入 3m）
heading × 1 = 0.5 × 1 = 0.5           （輔助）
standstill × 1 ≈ -0.02 × 1 = -0.02    （微小）
time × 0.005 × 1500 = -7.5            （可承受）

總獎勵 ≈ 9 + 6 + 0.5 - 0.02 - 7.5 ≈ 8.0 ✅（強正值）
```

**最壞情況（Agent 仍然慢）**：
```
progress × 60 = 0.02 × 60 = 1.2
near_goal = 0
heading = 0.2
standstill ≈ -0.1 × 1 = -0.1
time = -7.5

總獎勵 ≈ 1.2 - 0.1 - 7.5 ≈ -6.4（可接受的負值）
```

**v3 實際（過度懲罰）**：
```
progress × 60 = -0.01 × 60 = -0.6
standstill × 4 ≈ -1.16 × 4 = -4.64（太重！）
anti_idle × 2 ≈ -0.53 × 2 = -1.06（太重！）
time × 0.01 × 1500 = -15.0（太重！）

總獎勵 ≈ -0.6 - 4.64 - 1.06 - 15 ≈ -21 ❌
```

---

## 📈 預期改善（v3 → v4）

| 指標 | v3 結果 | v4 目標 | 理由 |
|------|---------|---------|------|
| **Progress** | -0.0104 | **> 0.05** | 懲罰減輕，敢動 |
| **Standstill** | -1.16 | **-0.1 ~ -0.3** | 權重降低 75% |
| **Anti-idle** | -0.53 | **-0.1 ~ -0.2** | 權重降低 75% |
| **Time Total** | -15.0 | **-7.5** | 權重降低 50% |
| **Position Error** | 3.84m | **< 3.0m** | 持續改善 |
| **Mean Reward** | -26.49 | **> -10** | 正負平衡 |

---

## 🚀 v4 訓練執行

### 立即啟動

```bash
cd /home/aa/IsaacLab

export WANDB_API_KEY="015e82155dc7cecb79368e0415eaf09362c260ef"

./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Min-v0 \
    --num_envs 24 \
    --max_iterations 10000 \
    --headless
```

**預計時間**：約 3.5 小時

### WandB 監控重點

**1000 iter 關鍵檢查**：
- ✅ Progress > 0？（最關鍵）
- ✅ Standstill < -0.5？（懲罰降低）

**如果 Progress 仍為負**：
- 停止訓練
- 套用方案 B（極簡獎勵）

---

## 📝 版本對照總結

| 版本 | Progress 權重 | 懲罰總和 | Progress 結果 | Position Error | 問題 |
|------|-------------|----------|-------------|----------------|------|
| v1 | 15 | 低 | 0 | 5.15m | Reward Hacking |
| v2 | 30 | 中低 | 0.04 | 4.19m | 太慢 |
| v3 | 60 | **高** | **-0.01** | 3.84m | **懲罰過重** |
| v4 | 60 | **低** | **目標 >0.05** | **<3m** | **平衡** |

---

**v4 配置已完成！準備啟動訓練。** 🚀

