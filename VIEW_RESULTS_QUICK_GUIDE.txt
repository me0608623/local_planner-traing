╔══════════════════════════════════════════════════════════════════╗
║          📊 查看訓練結果和架構快速指南                           ║
╚══════════════════════════════════════════════════════════════════╝

✅ 您已經有訓練結果！
═══════════════════════════════════════════════════════════════════

最新訓練:
  📁 logs/rsl_rl/local_planner_carter/2025-10-23_00-43-53/
  
  包含:
  ✅ 30個模型檢查點 (model_0.pt ~ model_2999.pt)
  ✅ TensorBoard 日誌 (events.out.tfevents.*)
  ✅ 訓練參數 (params/)
  
  訓練狀態:
  🎉 已完成 3000 次迭代！


查看訓練結果的3種方法:
═══════════════════════════════════════════════════════════════════

方法1: TensorBoard（最推薦）⭐
──────────────────────────────────────────────────────
cd /home/aa/IsaacLab
tensorboard --logdir logs/rsl_rl/local_planner_carter/2025-10-23_00-43-53/

然後瀏覽器打開: http://localhost:6006

您會看到:
  📈 Mean Reward 曲線（訓練進度）
  🎯 Success Rate (reached_goal)
  ❌ Collision Rate
  ⏱️ Time Out Rate
  📉 Loss 曲線


方法2: Play 模式測試（可視化）⭐
──────────────────────────────────────────────────────
cd /home/aa/IsaacLab

# 測試最終模型
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 1 \
    --checkpoint logs/rsl_rl/local_planner_carter/2025-10-23_00-43-53/model_2999.pt

觀察:
  🤖 機器人如何移動
  🎯 是否到達目標
  🚧 如何避障
  💡 策略是否學會


方法3: 查看模型文件
──────────────────────────────────────────────────────
ls -lh logs/rsl_rl/local_planner_carter/2025-10-23_00-43-53/

每個model_*.pt包含:
  - 神經網路權重
  - 優化器狀態
  - 訓練迭代信息


查看訓練架構的位置:
═══════════════════════════════════════════════════════════════════

🏗️ 核心架構文件:
──────────────────────────────────────────────────────

1. 神經網路架構 ⭐
   📁 agents/rsl_rl_ppo_cfg.py
   📍 第 34-55 行
   
   Actor Network:
     輸入[369] → FC[256] → FC[256] → FC[128] → 輸出[2]
   
   Critic Network:
     輸入[369] → FC[256] → FC[256] → FC[128] → 輸出[1]

2. 觀測空間架構 ⭐
   📁 local_planner_env_cfg.py
   📍 第 163-194 行
   
   觀測 = [
     LiDAR[360],
     velocity[6],
     goal_position[2],
     goal_distance[1]
   ] = 369 維

3. 獎勵架構 ⭐
   📁 local_planner_env_cfg.py
   📍 第 219-261 行
   
   總獎勵 = 
     + progress_to_goal × 10.0
     + reached_goal × 100.0
     - collision × 50.0
     - ...

4. 場景架構 ⭐
   📁 local_planner_env_cfg.py
   📍 第 37-135 行
   
   場景 = 
     + 地形（40×40m）
     + Nova Carter 機器人
     + 360° LiDAR
     + 障礙物
     + 目標


📚 完整架構文檔:
═══════════════════════════════════════════════════════════════════

詳細說明:
  📖 md/CODE_ARCHITECTURE_GUIDE.md       ← 完整架構
  📖 TRAINING_STRATEGY_SUMMARY.md        ← 策略總結
  📖 md/PROJECT_ARCHITECTURE_SUMMARY.md  ← 項目總覽

快速參考:
  📋 WHERE_TO_FIND_SCENE_CONFIG.txt      ← 場景配置位置
  📋 AGENT_GOAL_PERCEPTION.txt           ← Agent感知機制


快速操作命令:
═══════════════════════════════════════════════════════════════════

1. 查看訓練曲線（TensorBoard）
   tensorboard --logdir logs/rsl_rl/

2. 測試最新模型
   ./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
       --task Isaac-Navigation-LocalPlanner-Carter-v0 \
       --num_envs 1 \
       --checkpoint logs/rsl_rl/local_planner_carter/2025-10-23_00-43-53/model_2999.pt

3. 查看架構代碼
   vim source/.../agents/rsl_rl_ppo_cfg.py +34

4. 查看訓練配置
   cat logs/rsl_rl/local_planner_carter/2025-10-23_00-43-53/params/agent.json


訓練架構層級圖:
═══════════════════════════════════════════════════════════════════

train.py (訓練腳本)
  ├─ LocalPlannerEnvCfg (環境配置)
  │   ├─ LocalPlannerSceneCfg
  │   │   ├─ 地形
  │   │   ├─ 機器人（USD）
  │   │   ├─ LiDAR
  │   │   └─ 障礙物
  │   │
  │   ├─ ObservationsCfg
  │   │   └─ policy: [LiDAR + velocity + goal]
  │   │
  │   ├─ ActionsCfg
  │   │   └─ DifferentialDrive: [v_linear, v_angular]
  │   │
  │   ├─ RewardsCfg
  │   │   ├─ progress_to_goal
  │   │   ├─ reached_goal
  │   │   └─ collision/obstacle penalties
  │   │
  │   └─ TerminationsCfg
  │       ├─ goal_reached
  │       ├─ collision
  │       └─ time_out
  │
  └─ LocalPlannerPPORunnerCfg (演算法配置)
      ├─ Actor Network [256, 256, 128]
      ├─ Critic Network [256, 256, 128]
      └─ PPO 超參數
          ├─ learning_rate: 1e-3
          ├─ clip_param: 0.2
          └─ num_mini_batches: 4


═══════════════════════════════════════════════════════════════════
📖 詳細文檔: HOW_TO_VIEW_TRAINING_RESULTS.md
═══════════════════════════════════════════════════════════════════
