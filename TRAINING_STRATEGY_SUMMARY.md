# è¨“ç·´ç­–ç•¥å¿«é€Ÿåƒè€ƒ

## ğŸ¯ æ ¸å¿ƒè¨“ç·´ç­–ç•¥ï¼šPPO + ç¨ å¯†çå‹µ

### ä¸»è¦æ–‡ä»¶

| æ–‡ä»¶ | ç”¨é€” | é‡è¦æ€§ |
|------|------|--------|
| **`mdp/rewards.py`** | çå‹µå‡½æ•¸å¯¦ç¾ | â­â­â­ **æœ€é‡è¦** |
| **`rsl_rl_ppo_cfg.py`** | PPOæ¼”ç®—æ³•é…ç½® | â­â­ |
| **`local_planner_env_cfg.py`** | ç’°å¢ƒé…ç½®ï¼ˆçå‹µæ¬Šé‡ï¼‰ | â­â­ |
| **`train.py`** | è¨“ç·´è…³æœ¬ | â­ |

---

## ğŸ“Š ç•¶å‰è¨“ç·´ç­–ç•¥

### çå‹µè¨­è¨ˆï¼ˆç¨ å¯†çå‹µï¼‰

```python
ç¸½çå‹µ = (
    + æ¥è¿‘ç›®æ¨™çå‹µ     Ã— 10.0     # æŒçºŒå¼•å°
    + åˆ°é”ç›®æ¨™çå‹µ     Ã— 100.0    # æœ€çµ‚ç›®æ¨™  
    - éšœç¤™ç‰©æ¥è¿‘æ‡²ç½°   Ã— 5.0      # å®‰å…¨è­¦å‘Š
    - ç¢°æ’æ‡²ç½°         Ã— 50.0     # åš´é‡éŒ¯èª¤
    - è§’é€Ÿåº¦æ‡²ç½°       Ã— 0.01     # å¹³æ»‘é‹å‹•
    - éœæ­¢æ‡²ç½°         Ã— 0.1      # é¼“å‹µè¡Œå‹•
)
```

### PPO è¶…åƒæ•¸

```python
å­¸ç¿’ç‡ (learning_rate) = 1e-3
PPOè£å‰ªåƒæ•¸ (clip_param) = 0.2
æŠ˜æ‰£å› å­ (gamma) = 0.99
ç†µä¿‚æ•¸ (entropy_coef) = 0.01
ç¶²è·¯æ¶æ§‹ = [256, 256, 128]  # 3å±¤MLP
```

---

## ğŸ”§ å¿«é€Ÿèª¿æ•´æŒ‡å—

### å•é¡Œè¨ºæ–·

| å•é¡Œ | ç—‡ç‹€ | è§£æ±ºæ–¹æ¡ˆ |
|------|------|---------|
| **ä¸ç§»å‹•** | æ©Ÿå™¨äººéœæ­¢ | å¢åŠ  `standstill_penalty` åˆ° -1.0 |
| **ç¢°æ’å¤ªå¤š** | collision rate > 50% | å¢åŠ  `collision_penalty` åˆ° -100.0 |
| **å¾ˆé›£åˆ°é”ç›®æ¨™** | success rate < 5% | å¢åŠ  `progress_to_goal` åˆ° 20.0 |
| **è¨“ç·´ä¸ç©©å®š** | reward åŠ‡çƒˆæ³¢å‹• | é™ä½ `learning_rate` åˆ° 3e-4 |
| **æ”¶æ–‚å¤ªæ…¢** | 500æ¬¡å¾Œç„¡é€²æ­¥ | å¢åŠ  `num_steps_per_env` åˆ° 48 |

### ä¿®æ”¹ä½ç½®

```bash
# ä¿®æ”¹çå‹µæ¬Šé‡
vim source/.../local_planner/local_planner_env_cfg.py
# æ‰¾åˆ° RewardsCfg é¡ï¼Œä¿®æ”¹ weight åƒæ•¸

# ä¿®æ”¹æ¼”ç®—æ³•åƒæ•¸
vim source/.../local_planner/agents/rsl_rl_ppo_cfg.py
# æ‰¾åˆ° LocalPlannerPPORunnerCfg é¡

# ä¿®æ”¹çå‹µå‡½æ•¸é‚è¼¯ï¼ˆé«˜ç´šï¼‰
vim source/.../local_planner/mdp/rewards.py
```

---

## ğŸ“ˆ æœŸæœ›è¨“ç·´æ›²ç·š

### æ­£å¸¸è¨“ç·´é€²åº¦

```
è¿­ä»£ 0-500:
â”œâ”€ Mean Reward: -2000 â†’ -500
â”œâ”€ Success Rate: 0% â†’ 10%
â””â”€ å­¸ç¿’åŸºæœ¬å°èˆª

è¿­ä»£ 500-1500:
â”œâ”€ Mean Reward: -500 â†’ 0
â”œâ”€ Success Rate: 10% â†’ 40%
â””â”€ å­¸ç¿’é¿éšœ + ç›®æ¨™å°å‘

è¿­ä»£ 1500-3000:
â”œâ”€ Mean Reward: 0 â†’ 500+
â”œâ”€ Success Rate: 40% â†’ 70%+
â””â”€ ç­–ç•¥ç²¾ç´°åŒ–
```

---

## ğŸ“ è¨“ç·´ç­–ç•¥åŸç†

### ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆï¼Ÿ

#### 1. ç¨ å¯†çå‹µ vs ç¨€ç–çå‹µ

**ç¨ å¯†çå‹µï¼ˆç•¶å‰ä½¿ç”¨ï¼‰**:
- âœ… æ¯æ­¥éƒ½æœ‰åé¥‹
- âœ… å­¸ç¿’æ›´å¿«
- âœ… é©åˆè¤‡é›œä»»å‹™

**ç¨€ç–çå‹µ**:
- âŒ åªåœ¨å®Œæˆæ™‚çµ¦çå‹µ
- âŒ å­¸ç¿’å¾ˆæ…¢
- âŒ é›£ä»¥æ”¶æ–‚

#### 2. PPO vs å…¶ä»–æ¼”ç®—æ³•

**PPO å„ªå‹¢**:
- âœ… ç©©å®šæ€§å¥½ï¼ˆè£å‰ªæ©Ÿåˆ¶ï¼‰
- âœ… æ¨£æœ¬æ•ˆç‡é«˜
- âœ… å¯¦ç¾ç°¡å–®
- âœ… ç¶“éé©—è­‰

#### 3. çå‹µæ¬Šé‡å±¤ç´š

```
é”æˆç›®æ¨™ (100)     # æœ€é‡è¦
    â†“
é¿å…ç¢°æ’ (50)      # å®‰å…¨ç¬¬ä¸€
    â†“
æ¥è¿‘ç›®æ¨™ (10)      # æŒçºŒå¼•å°
    â†“
ä¿æŒè·é›¢ (5)       # é é˜²æ€§å®‰å…¨
    â†“
é‹å‹•å“è³ª (0.1)     # éŒ¦ä¸Šæ·»èŠ±
```

---

## ğŸ’¡ å¯¦ç”¨æŠ€å·§

### 1. é–‹å§‹è¨“ç·´å‰

```bash
# å…ˆç”¨å°‘é‡è¿­ä»£æ¸¬è©¦
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py \
    --task Isaac-Navigation-LocalPlanner-Carter-v0 \
    --num_envs 2 \
    --headless \
    --max_iterations 10

# ç¢ºèªæ²’æœ‰éŒ¯èª¤å¾Œå†é–‹å§‹å®Œæ•´è¨“ç·´
```

### 2. ç›£æ§é—œéµæŒ‡æ¨™

```python
å¿…çœ‹æŒ‡æ¨™:
âœ… Mean reward (æ‡‰è©²ä¸Šå‡)
âœ… Episode_Reward/reached_goal (æ‡‰è©²å¢åŠ )
âœ… Episode_Termination/time_out (æ‡‰è©²æ¸›å°‘)
âœ… Episode_Termination/goal_reached (æ‡‰è©²å¢åŠ )
```

### 3. è¨ºæ–·å·¥å…·

```bash
# åˆ†æè¨“ç·´çµæœ
./isaaclab.sh -p scripts/analyze_training_log.py --stdin

# æª¢æŸ¥ç’°å¢ƒ
./isaaclab.sh -p scripts/diagnose_tensor_device.py
```

### 4. ä¿å­˜å¯¦é©—è¨˜éŒ„

```bash
# ä¿®æ”¹åƒæ•¸å‰è¨˜éŒ„ç•¶å‰è¨­ç½®
git add -A
git commit -m "å¯¦é©—1: baseline, lr=1e-3, reward_weight=10"

# è¨“ç·´å¾Œè¨˜éŒ„çµæœ
# åœ¨ commit message ä¸­è¨˜éŒ„æœ€çµ‚ reward å’Œ success rate
```

---

## ğŸ”¬ é€²éšèª¿å„ª

### A. çå‹µå·¥ç¨‹ï¼ˆReward Engineeringï¼‰

#### ç•¶å‰çå‹µå‡½æ•¸å•é¡Œè­˜åˆ¥

```bash
# æŸ¥çœ‹çå‹µåˆ†è§£
# è¨“ç·´æ—¥èªŒä¸­æœƒé¡¯ç¤º:
Episode_Reward/progress_to_goal: -125.65  # âŒ è² å€¼ï¼å•é¡Œï¼
Episode_Reward/reached_goal: 0.0000       # âŒ å¾æœªåˆ°é”
Episode_Reward/collision_penalty: -1000   # âŒ ç¢°æ’å¤ªå¤š
```

#### æ ¹æ“šå•é¡Œèª¿æ•´

```python
# å¦‚æœ progress_to_goal æ˜¯è² å€¼
â†’ æ©Ÿå™¨äººæ²’æœ‰æ¥è¿‘ç›®æ¨™
â†’ å¢åŠ  progress_to_goal weight æˆ–ç°¡åŒ–ç’°å¢ƒ

# å¦‚æœ collision_penalty å¾ˆå¤§
â†’ æ©Ÿå™¨äººç¢°æ’å¤ªé »ç¹  
â†’ å¢åŠ  obstacle_proximity_penalty æˆ– collision_penalty

# å¦‚æœ reached_goal å§‹çµ‚ç‚º0
â†’ ä»»å‹™å¤ªé›£
â†’ ç¸®çŸ­ç›®æ¨™è·é›¢æˆ–å»¶é•·æ™‚é–“
```

### B. Curriculum Learningï¼ˆèª²ç¨‹å­¸ç¿’ï¼‰

#### éšæ®µå¼é›£åº¦æå‡

```python
# éšæ®µ1: ç°¡å–®ï¼ˆ0-500æ¬¡è¿­ä»£ï¼‰
ç›®æ¨™è·é›¢: 2-3ç±³
éšœç¤™ç‰©: 0å€‹
æ™‚é–“: 40ç§’

# éšæ®µ2: ä¸­ç­‰ï¼ˆ500-1500æ¬¡ï¼‰
ç›®æ¨™è·é›¢: 3-6ç±³
éšœç¤™ç‰©: 3å€‹
æ™‚é–“: 35ç§’

# éšæ®µ3: å®Œæ•´ï¼ˆ1500-3000æ¬¡ï¼‰
ç›®æ¨™è·é›¢: 5-10ç±³
éšœç¤™ç‰©: 5-10å€‹
æ™‚é–“: 30ç§’
```

### C. è¶…åƒæ•¸æœç´¢

#### æ¨è–¦ç¯„åœ

```python
learning_rate: [1e-4, 3e-4, 1e-3, 3e-3]
entropy_coef: [0.001, 0.01, 0.05]
clip_param: [0.1, 0.2, 0.3]
num_mini_batches: [2, 4, 8]
```

---

## ğŸ“š ç›¸é—œæ–‡æª”

- [å®Œæ•´ä»£ç¢¼æ¶æ§‹æŒ‡å—](md/CODE_ARCHITECTURE_GUIDE.md) - è©³ç´°çš„æŠ€è¡“èªªæ˜
- [è¨“ç·´è¨ºæ–·æŒ‡å—](md/TRAINING_DIAGNOSIS_GUIDE.md) - å•é¡Œæ’æŸ¥
- [å¿«é€Ÿé–‹å§‹æŒ‡å—](QUICK_START_GUIDE.md) - ç«‹å³é–‹å§‹è¨“ç·´

---

**è¨˜ä½**: å¼·åŒ–å­¸ç¿’éœ€è¦å¯¦é©—å’Œèª¿æ•´ï¼Œæ²’æœ‰ä¸€æˆä¸è®Šçš„æœ€ä½³åƒæ•¸ï¼ğŸ§ª
